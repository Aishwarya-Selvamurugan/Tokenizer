# From Bias to Balance: How Multilingual Dataset Composition Affects Tokenizer Performance Across Languages

This repository contains the implementation and experimental setup for the research paper investigating how **balanced multilingual datasets** impact tokenizer performance across typologically diverse languages.

---

## ğŸ“– Overview
This study systematically evaluates three subword tokenization algorithms (**BPE, WordPiece, and Unigram**) across **nine languages** using balanced training corpora.  
We assess both:
- **Intrinsic tokenization metrics**  
- **Downstream task performance** on POS tagging, NER, and machine translation.

---

## ğŸ“‚ Repository Structure

- `Machine_Translation/` â€“ Machine translation experiments using **BART-large**
- `NER/` â€“ Named Entity Recognition task implementation
- `POS/` â€“ Part-of-Speech tagging experiments
- `Test_data_NSL_Sub_Word_fertility/` â€“ Evaluation scripts for intrinsic metrics
- `tokenizer_dataset_creation/` â€“ Dataset preprocessing and creation scripts
- `Tokenizer.ipynb` â€“ Main tokenizer training notebook
- `Tokenizer_results.xlsx` â€“ Compiled experimental results
- `tokenizer_nsl_fertility_report.csv` â€“ Detailed intrinsic evaluation metrics


---

## ğŸŒ Languages Studied
- **African Languages**: Yoruba, Swahili  
- **Asian Languages**: Mandarin Chinese, Japanese, Hindi, Bengali  
- **European Languages**: Russian, Turkish  
- **Middle Eastern Languages**: Arabic  

These languages represent diverse **writing systems** (Latin, Arabic, Devanagari, Cyrillic, CJK), **morphological complexity levels**, and **resource availability**.

---

## ğŸ”¤ Tokenization Algorithms
- **Byte Pair Encoding (BPE)** â€“ Frequency-driven character pair merging  
- **WordPiece** â€“ Likelihood-based merging with morphological awareness  
- **Unigram Language Model** â€“ Probabilistic framework with comprehensive vocabularies  

Each algorithm is evaluated across **three vocabulary sizes**: `15k`, `30k`, and `50k`.

---

## âœ¨ Key Features

### ğŸ› ï¸ Dataset Construction
- Balanced multilingual corpora from **Wikipedia** and **OSCAR**  
- Equal per-language character allocations to reduce **high-resource bias**  
- Three dataset scales: **100M, 200M, 400M characters**  
- Comprehensive preprocessing pipeline with **Unicode normalization**  

### ğŸ“Š Evaluation Metrics
- **Intrinsic**: Normalized Sequence Length (NSL), Subword Fertility  
- **Extrinsic**: Performance on downstream NLP tasks  

### ğŸ“Œ Downstream Tasks
- **POS Tagging**: Token classification using BERT-based models  
- **Named Entity Recognition**: Sequence labeling with class weighting  
- **Machine Translation**: Multilingual BART-large with custom tokenizers  

---

## âš™ï¸ Installation and Setup

### ğŸ”‘ Prerequisites
- Python **3.10+**  
- CUDA **11.8+**  
- GPU with **â‰¥16GB VRAM** (Tesla T4 or better)  
- For machine translation: **24GB VRAM** recommended (RTX 4090 or equivalent)  

---

ğŸš€ With this setup, you can reproduce all experiments, evaluate tokenizers, and explore the role of **balanced multilingual dataset design** in improving tokenizer fairness and efficiency across languages.
