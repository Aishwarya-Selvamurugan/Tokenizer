{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbd8f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\aish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aish\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10.0.2)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\aish\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aish\\anaconda3\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.2)\n",
      "Installing collected packages: fsspec, huggingface-hub, tokenizers, sentencepiece\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.10.1\n",
      "    Uninstalling fsspec-2021.10.1:\n",
      "      Successfully uninstalled fsspec-2021.10.1\n",
      "Successfully installed fsspec-2025.5.1 huggingface-hub-0.33.2 sentencepiece-0.2.0 tokenizers-0.21.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5018679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Training tokenizers for: SMALL (Vocab size: 15000)\n",
      "🚀 Starting: train_hf_bpe\n",
      "✅ Completed in 23.17s, RAM used: 63.04 MB\n",
      "\n",
      "🚀 Starting: train_hf_wordpiece\n",
      "✅ Completed in 19.76s, RAM used: 13.04 MB\n",
      "\n",
      "🚀 Starting: train_sp_bpe\n",
      "✅ Completed in 94.74s, RAM used: 8.25 MB\n",
      "\n",
      "🚀 Starting: train_sp_unigram\n",
      "✅ Completed in 166.59s, RAM used: -3.27 MB\n",
      "\n",
      "\n",
      "📦 Training tokenizers for: MEDIUM (Vocab size: 30000)\n",
      "🚀 Starting: train_hf_bpe\n",
      "✅ Completed in 34.73s, RAM used: 12.43 MB\n",
      "\n",
      "🚀 Starting: train_hf_wordpiece\n",
      "✅ Completed in 33.75s, RAM used: 3.64 MB\n",
      "\n",
      "🚀 Starting: train_sp_bpe\n",
      "✅ Completed in 344.54s, RAM used: 3.60 MB\n",
      "\n",
      "🚀 Starting: train_sp_unigram\n",
      "✅ Completed in 160.53s, RAM used: -4.06 MB\n",
      "\n",
      "\n",
      "📦 Training tokenizers for: LARGE (Vocab size: 50000)\n",
      "🚀 Starting: train_hf_bpe\n",
      "✅ Completed in 43.07s, RAM used: 4.40 MB\n",
      "\n",
      "🚀 Starting: train_hf_wordpiece\n",
      "✅ Completed in 43.72s, RAM used: 4.79 MB\n",
      "\n",
      "🚀 Starting: train_sp_bpe\n",
      "✅ Completed in 820.05s, RAM used: -20.76 MB\n",
      "\n",
      "🚀 Starting: train_sp_unigram\n",
      "✅ Completed in 158.73s, RAM used: -18.26 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "import sentencepiece as spm\n",
    "\n",
    "def monitor_resource_usage(func):\n",
    "    import time, psutil\n",
    "    def wrapper(*args, **kwargs):\n",
    "        import os\n",
    "        process = psutil.Process(os.getpid())\n",
    "        start_time = time.time()\n",
    "        start_mem = process.memory_info().rss\n",
    "\n",
    "        print(f\"🚀 Starting: {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        end_time = time.time()\n",
    "        end_mem = process.memory_info().rss\n",
    "        print(f\"✅ Completed in {end_time - start_time:.2f}s, RAM used: {(end_mem - start_mem)/1e6:.2f} MB\\n\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Training functions\n",
    "@monitor_resource_usage\n",
    "def train_hf_bpe(file_path, folder_path, vocab_size):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.train([file_path], trainer)\n",
    "    tokenizer.save(os.path.join(folder_path, \"hf_bpe.json\"))\n",
    "\n",
    "@monitor_resource_usage\n",
    "def train_hf_wordpiece(file_path, folder_path, vocab_size):\n",
    "    tokenizer = Tokenizer(models.WordPiece())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.train([file_path], trainer)\n",
    "    tokenizer.save(os.path.join(folder_path, \"hf_wordpiece.json\"))\n",
    "\n",
    "@monitor_resource_usage\n",
    "def train_sp_bpe(file_path, folder_path, vocab_size):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=file_path,\n",
    "        model_prefix=os.path.join(folder_path, \"sp_bpe\"),\n",
    "        vocab_size=vocab_size,\n",
    "        model_type='bpe',\n",
    "        character_coverage=1.0\n",
    "    )\n",
    "\n",
    "@monitor_resource_usage\n",
    "def train_sp_unigram(file_path, folder_path, vocab_size):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=file_path,\n",
    "        model_prefix=os.path.join(folder_path, \"sp_unigram\"),\n",
    "        vocab_size=vocab_size,\n",
    "        model_type='unigram',\n",
    "        character_coverage=1.0\n",
    "    )\n",
    "\n",
    "# Master training function\n",
    "def train_all_tokenizers(file_path, folder_path, vocab_size):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    # train_hf_bpe(file_path, folder_path, vocab_size)\n",
    "    train_hf_wordpiece(file_path, folder_path, vocab_size)\n",
    "    # train_sp_bpe(file_path, folder_path, vocab_size)\n",
    "    # train_sp_unigram(file_path, folder_path, vocab_size)\n",
    "\n",
    "# Run for all 3 corpus sizes\n",
    "base_input = r\"C:\\Users\\User\\Desktop\\tokenizer\\balanced_normalized\\final_balanced_\"\n",
    "base_output = r\"C:\\Users\\Aish\\OneDrive\\Desktop\\tokenizer\\vocab_final\"\n",
    "\n",
    "settings = {\n",
    "    \"small\": 15000,\n",
    "    # \"medium\": 30000,\n",
    "    # \"large\": 50000\n",
    "}\n",
    "\n",
    "for size, vocab_size in settings.items():\n",
    "    print(f\"\\n📦 Training tokenizers for: {size.upper()} (Vocab size: {vocab_size})\")\n",
    "    file_path = base_input + f\"{size}.txt\"\n",
    "    folder_path = base_output + size\n",
    "    train_all_tokenizers(file_path, folder_path, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a647ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_path = r\"C:\\Users\\Aish\\OneDrive\\Desktop\\tokenizer\\final_multilang_sentences.json\"\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    language_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9024349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aish\\OneDrive\\Desktop\\tokenizer\\vocab_finalsmall\n",
      "C:\\Users\\Aish\\OneDrive\\Desktop\\tokenizer\\vocab_finalmedium\n",
      "C:\\Users\\Aish\\OneDrive\\Desktop\\tokenizer\\vocab_finallarge\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Define folder and tokenizer paths\n",
    "base_path = r\"C:\\Users\\Aish\\OneDrive\\Desktop\\tokenizer\\vocab_final\"\n",
    "sizes = {\"small\": 15000, \"medium\": 30000, \"large\": 50000}\n",
    "types = [\"hf_bpe\", \"hf_wordpiece\", \"sp_bpe\", \"sp_unigram\"]\n",
    "\n",
    "tokenizers = {}\n",
    "\n",
    "for size in sizes:\n",
    "    folder = base_path+size\n",
    "    print(folder)\n",
    "    for ttype in types:\n",
    "        name = f\"{ttype}_{size}\"\n",
    "        if \"hf\" in ttype:\n",
    "            tokenizer = Tokenizer.from_file(os.path.join(folder, f\"{ttype}.json\"))\n",
    "        else:\n",
    "            tokenizer = spm.SentencePieceProcessor(model_file=os.path.join(folder, f\"{ttype}.model\"))\n",
    "        tokenizers[name] = tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a08661b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e58eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nsl(sequence, tokenizer):\n",
    "    if hasattr(tokenizer, \"encode\"):  # HF or SentencePiece\n",
    "        tokens = tokenizer.encode(sequence)\n",
    "        if hasattr(tokens, \"tokens\"):  # HF\n",
    "            tokens = tokens.tokens\n",
    "    else:\n",
    "        tokens = []\n",
    "\n",
    "    return len(tokens) / len(sequence) if len(sequence) > 0 else 0.0\n",
    "\n",
    "\n",
    "def compute_fertility(sequence, tokenizer):\n",
    "    words = sequence.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    if hasattr(tokenizer, \"encode\"):\n",
    "        tokens = tokenizer.encode(sequence)\n",
    "        if hasattr(tokens, \"tokens\"):\n",
    "            tokens = tokens.tokens\n",
    "    else:\n",
    "        tokens = []\n",
    "\n",
    "    return len(tokens) / num_words if num_words > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e1e62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for lang, sentences in language_data.items():\n",
    "    for tok_name, tok_obj in tokenizers.items():\n",
    "        nsl_scores = []\n",
    "        fert_scores = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            try:\n",
    "                nsl = compute_nsl(sentence, tok_obj)\n",
    "                fert = compute_fertility(sentence, tok_obj)\n",
    "                nsl_scores.append(nsl)\n",
    "                fert_scores.append(fert)\n",
    "            except Exception as e:\n",
    "                print(f\" Error with {lang}, {tok_name}: {e}\")\n",
    "\n",
    "        if nsl_scores and fert_scores:\n",
    "            results.append({\n",
    "                \"Language\": lang,\n",
    "                \"Tokenizer\": tok_name,\n",
    "                \"Vocab Size\": tok_name.split(\"_\")[-1],\n",
    "                \"NSL\": round(sum(nsl_scores)/len(nsl_scores), 4),\n",
    "                \"Fertility\": round(sum(fert_scores)/len(fert_scores), 4)\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d2672d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Tokenizer</th>\n",
       "      <th>Vocab Size</th>\n",
       "      <th>NSL</th>\n",
       "      <th>Fertility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>hf_bpe_large</td>\n",
       "      <td>large</td>\n",
       "      <td>0.3316</td>\n",
       "      <td>1.9056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>hf_bpe_medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>2.1427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>hf_bpe_small</td>\n",
       "      <td>small</td>\n",
       "      <td>0.4908</td>\n",
       "      <td>2.8232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>hf_wordpiece_large</td>\n",
       "      <td>large</td>\n",
       "      <td>0.3567</td>\n",
       "      <td>2.0482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>hf_wordpiece_medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>2.5773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language            Tokenizer Vocab Size     NSL  Fertility\n",
       "0   Arabic         hf_bpe_large      large  0.3316     1.9056\n",
       "1   Arabic        hf_bpe_medium     medium  0.3726     2.1427\n",
       "2   Arabic         hf_bpe_small      small  0.4908     2.8232\n",
       "3   Arabic   hf_wordpiece_large      large  0.3567     2.0482\n",
       "4   Arabic  hf_wordpiece_medium     medium  0.4485     2.5773"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.sort_values([\"Language\", \"Tokenizer\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Show preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb1cf1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File saved: tokenizer_nsl_fertility_report.csv\n",
      "📁 File location: C:\\Users\\Aish\\OneDrive\\Desktop\\tokenizer\\tokenizer_nsl_fertility_report.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"tokenizer_nsl_fertility_report.csv\", index=False)\n",
    "print(\"✅ File saved: tokenizer_nsl_fertility_report.csv\")\n",
    "\n",
    "# Optional: Open file directly (if running in Jupyter or locally)\n",
    "import os\n",
    "print(f\"📁 File location: {os.path.abspath('tokenizer_nsl_fertility_report.csv')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e2dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
