{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bbbc20e894c486589b915a204e02e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53e6ccf6938d4aeb84f29a11f67e5cac",
              "IPY_MODEL_0e26003a68d746069d8848284d4cd3a3",
              "IPY_MODEL_b8ea5af0e057463c9f8b0a495e840c98"
            ],
            "layout": "IPY_MODEL_f2cbf79ecb4d4b8a86e34f1bd12aa0b6"
          }
        },
        "53e6ccf6938d4aeb84f29a11f67e5cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_966ba7b437964ef090d9d7b2ed044f5c",
            "placeholder": "​",
            "style": "IPY_MODEL_0472d42de90343f497dc162438b2f8b0",
            "value": "config.json: 100%"
          }
        },
        "0e26003a68d746069d8848284d4cd3a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_678737cee9a6424f8d85211e8d582f12",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d88b343ae70e41699418ed26c1c51d66",
            "value": 570
          }
        },
        "b8ea5af0e057463c9f8b0a495e840c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9349fb41b50b49c7bf0d871db9a73b3c",
            "placeholder": "​",
            "style": "IPY_MODEL_12199c2ebcc843f391e884ea8864a90e",
            "value": " 570/570 [00:00&lt;00:00, 23.0kB/s]"
          }
        },
        "f2cbf79ecb4d4b8a86e34f1bd12aa0b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "966ba7b437964ef090d9d7b2ed044f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0472d42de90343f497dc162438b2f8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "678737cee9a6424f8d85211e8d582f12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88b343ae70e41699418ed26c1c51d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9349fb41b50b49c7bf0d871db9a73b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12199c2ebcc843f391e884ea8864a90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d47f08c13014f238f78ffee2264d5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a51d7df101ef41f4a332d9466b048240",
              "IPY_MODEL_098deb7ec5bf4a87aacd8f4b74fbf6fb",
              "IPY_MODEL_871d77c802944460ad93091e17f60cf0"
            ],
            "layout": "IPY_MODEL_6bd0efadcf594538af9baeaa9a8711d2"
          }
        },
        "a51d7df101ef41f4a332d9466b048240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56a8aafa1c5b4d4ba8b9cde87179a54d",
            "placeholder": "​",
            "style": "IPY_MODEL_41249c9504ab4f8c8fa9abcc905ac5a7",
            "value": "model.safetensors: 100%"
          }
        },
        "098deb7ec5bf4a87aacd8f4b74fbf6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fb18fb561b84670b844719ef4172609",
            "max": 435755784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d812e9b531984b59bac1c2b10797547e",
            "value": 435755784
          }
        },
        "871d77c802944460ad93091e17f60cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af069cd6ef584e7ebc216c9714231a44",
            "placeholder": "​",
            "style": "IPY_MODEL_17dda4960d4e4a5ab4b9726130f0f553",
            "value": " 436M/436M [00:19&lt;00:00, 22.3MB/s]"
          }
        },
        "6bd0efadcf594538af9baeaa9a8711d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8aafa1c5b4d4ba8b9cde87179a54d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41249c9504ab4f8c8fa9abcc905ac5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fb18fb561b84670b844719ef4172609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d812e9b531984b59bac1c2b10797547e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af069cd6ef584e7ebc216c9714231a44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17dda4960d4e4a5ab4b9726130f0f553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAvkoRlmfCbZ",
        "outputId": "b65ed29a-2e49-4464-90a9-00408d189739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SP_BPE training"
      ],
      "metadata": {
        "id": "Sk42FRrvsp0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "import sentencepiece as spm\n",
        "\n",
        "def monitor_resource_usage(func):\n",
        "    import time, psutil\n",
        "    def wrapper(*args, **kwargs):\n",
        "        import os\n",
        "        process = psutil.Process(os.getpid())\n",
        "        start_time = time.time()\n",
        "        start_mem = process.memory_info().rss\n",
        "\n",
        "        print(f\"🚀 Starting: {func.__name__}\")\n",
        "        result = func(*args, **kwargs)\n",
        "\n",
        "        end_time = time.time()\n",
        "        end_mem = process.memory_info().rss\n",
        "        print(f\"✅ Completed in {end_time - start_time:.2f}s, RAM used: {(end_mem - start_mem)/1e6:.2f} MB\\n\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "# Training functions\n",
        "@monitor_resource_usage\n",
        "def train_hf_bpe(file_path, folder_path, vocab_size):\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "    tokenizer.train([file_path], trainer)\n",
        "    tokenizer.save(os.path.join(folder_path, \"hf_bpe.json\"))\n",
        "\n",
        "@monitor_resource_usage\n",
        "def train_hf_wordpiece(file_path, folder_path, vocab_size):\n",
        "    tokenizer = Tokenizer(models.WordPiece())\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "    trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "    tokenizer.train([file_path], trainer)\n",
        "    tokenizer.save(os.path.join(folder_path, \"hf_wordpiece.json\"))\n",
        "\n",
        "@monitor_resource_usage\n",
        "def train_sp_bpe(file_path, folder_path, vocab_size):\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=file_path,\n",
        "        model_prefix=os.path.join(folder_path, \"sp_bpe\"),\n",
        "        vocab_size=vocab_size,\n",
        "        model_type='bpe',\n",
        "        character_coverage=1.0,\n",
        "        unk_id=0,\n",
        "        pad_id=1,\n",
        "        bos_id=2,\n",
        "        eos_id=3,\n",
        "        unk_piece=\"<unk>\",\n",
        "        pad_piece=\"<pad>\",\n",
        "        bos_piece=\"<s>\",\n",
        "        eos_piece=\"</s>\",\n",
        "        user_defined_symbols=[\"<mask>\"]\n",
        "    )\n",
        "\n",
        "@monitor_resource_usage\n",
        "def train_sp_unigram(file_path, folder_path, vocab_size):\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=file_path,\n",
        "        model_prefix=os.path.join(folder_path, \"sp_unigram\"),\n",
        "        vocab_size=vocab_size,\n",
        "        model_type='unigram',\n",
        "        character_coverage=1.0\n",
        "    )\n",
        "\n",
        "# Master training function\n",
        "def train_all_tokenizers(file_path, folder_path, vocab_size):\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    # train_hf_bpe(file_path, folder_path, vocab_size)\n",
        "    # train_hf_wordpiece(file_path, folder_path, vocab_size)\n",
        "    train_sp_bpe(file_path, folder_path, vocab_size)\n",
        "    # train_sp_unigram(file_path, folder_path, vocab_size)\n",
        "\n",
        "# Run for all 3 corpus sizes\n",
        "base_input = r\"/content/drive/MyDrive/Tokenizer_New/balanced_normalized/final_balanced_\"\n",
        "base_output = r\"/content/drive/MyDrive/Tokenizer_New\"\n",
        "\n",
        "settings = {\n",
        "    \"small\": 15000,\n",
        "    \"medium\": 30000,\n",
        "    \"large\": 50000\n",
        "}\n",
        "\n",
        "for size, vocab_size in settings.items():\n",
        "    print(f\"\\n📦 Training tokenizers for: {size.upper()} (Vocab size: {vocab_size})\")\n",
        "    file_path = base_input + f\"{size}.txt\"\n",
        "    folder_path = base_output + size\n",
        "    train_all_tokenizers(file_path, folder_path, vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jvj5nEPspsR",
        "outputId": "27edf453-bb97-41a5-8475-3dfae68e0e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📦 Training tokenizers for: SMALL (Vocab size: 15000)\n",
            "🚀 Starting: train_sp_bpe\n",
            "✅ Completed in 135.99s, RAM used: 1960.63 MB\n",
            "\n",
            "\n",
            "📦 Training tokenizers for: MEDIUM (Vocab size: 30000)\n",
            "🚀 Starting: train_sp_bpe\n",
            "✅ Completed in 486.55s, RAM used: 473.91 MB\n",
            "\n",
            "\n",
            "📦 Training tokenizers for: LARGE (Vocab size: 50000)\n",
            "🚀 Starting: train_sp_bpe\n",
            "✅ Completed in 1209.71s, RAM used: 180.33 MB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCGK5xeGsppw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bg-9hMAvspnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ymnx6Leospk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wmdy0v_vspib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiUXppqru4p0",
        "outputId": "bd8a13dd-c0b6-4ab9-e792-dbddbc6b19f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Existing tokenizer paths\n",
        "parent_dir = r\"/content/drive/MyDrive/Tokeizers/vocab_final\"\n",
        "sizes = [\"small\", \"medium\", \"large\"]\n",
        "tokenizer_types = [\n",
        "    (\"hf_bpe\", \"hf_bpe.json\"),\n",
        "    (\"hf_wordpiece\", \"hf_wordpiece.json\")\n",
        "]\n",
        "\n",
        "for size in sizes:\n",
        "    for tok_dir, tok_file in tokenizer_types:\n",
        "        input_json_path = os.path.join(parent_dir, f\"vocab_final{size}\", tok_dir, tok_file)\n",
        "        output_tokenizer_dir = os.path.join(parent_dir, f\"vocab_final{size}\", f\"{tok_dir}_hf\")\n",
        "\n",
        "        if not os.path.exists(input_json_path):\n",
        "            print(f\"❌ Not found: {input_json_path} (Skipping)\")\n",
        "            continue\n",
        "\n",
        "        os.makedirs(output_tokenizer_dir, exist_ok=True)\n",
        "        print(f\"🔁 Rewrapping {input_json_path} -> {output_tokenizer_dir}\")\n",
        "\n",
        "        try:\n",
        "            # Load original tokenizer trained with Tokenizers library\n",
        "            tokenizer_obj = Tokenizer.from_file(input_json_path)\n",
        "\n",
        "            # Wrap with Hugging Face-compatible tokenizer\n",
        "            hf_tokenizer = PreTrainedTokenizerFast(\n",
        "                tokenizer_object=tokenizer_obj,\n",
        "                unk_token=\"[UNK]\",\n",
        "                pad_token=\"[PAD]\",\n",
        "                cls_token=\"[CLS]\",\n",
        "                sep_token=\"[SEP]\",\n",
        "                mask_token=\"[MASK]\"\n",
        "            )\n",
        "\n",
        "            # Save in HF format: outputs tokenizer.json, config, special_tokens map\n",
        "            hf_tokenizer.save_pretrained(output_tokenizer_dir)\n",
        "            print(f\"✅ Wrapped and saved to: {output_tokenizer_dir}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to convert {input_json_path}: {e}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiNVyhQpjjDz",
        "outputId": "cb6d9142-101f-4b78-af65-2437b52bed8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Rewrapping /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_bpe/hf_bpe.json -> /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_bpe_hf\n",
            "✅ Wrapped and saved to: /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_bpe_hf\n",
            "\n",
            "🔁 Rewrapping /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_wordpiece/hf_wordpiece.json -> /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_wordpiece_hf\n",
            "✅ Wrapped and saved to: /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_wordpiece_hf\n",
            "\n",
            "🔁 Rewrapping /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalmedium/hf_bpe/hf_bpe.json -> /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalmedium/hf_bpe_hf\n",
            "✅ Wrapped and saved to: /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalmedium/hf_bpe_hf\n",
            "\n",
            "🔁 Rewrapping /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalmedium/hf_wordpiece/hf_wordpiece.json -> /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalmedium/hf_wordpiece_hf\n",
            "✅ Wrapped and saved to: /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalmedium/hf_wordpiece_hf\n",
            "\n",
            "🔁 Rewrapping /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finallarge/hf_bpe/hf_bpe.json -> /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finallarge/hf_bpe_hf\n",
            "✅ Wrapped and saved to: /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finallarge/hf_bpe_hf\n",
            "\n",
            "🔁 Rewrapping /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finallarge/hf_wordpiece/hf_wordpiece.json -> /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finallarge/hf_wordpiece_hf\n",
            "✅ Wrapped and saved to: /content/drive/MyDrive/Tokeizers/vocab_final/vocab_finallarge/hf_wordpiece_hf\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    r\"/content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_bpe_hf\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "FKcwRsJwqXPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "path = r\"/content/drive/MyDrive/Tokeizers/vocab_final/vocab_finalsmall/hf_bpe_hf/tokenizer.json\"\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"✅ tokenizer.json loaded successfully. Keys: {list(data.keys())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C22VrYrAqtSm",
        "outputId": "63c54544-0dc1-4860-9f20-73ab5f6c3366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ tokenizer.json loaded successfully. Keys: ['version', 'truncation', 'padding', 'added_tokens', 'normalizer', 'pre_tokenizer', 'post_processor', 'decoder', 'model']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzpaarW3URhk",
        "outputId": "9eb0e664-ab23-41a1-bf23-e2864bb15c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVauQzECUgBV",
        "outputId": "513feff1-1c03-4581-d370-50405aa29eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-22 07:04:47--  https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6257 (6.1K) [text/plain]\n",
            "Saving to: ‘sentencepiece_model_pb2.py’\n",
            "\n",
            "\r          sentencep   0%[                    ]       0  --.-KB/s               \rsentencepiece_model 100%[===================>]   6.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-22 07:04:47 (64.8 MB/s) - ‘sentencepiece_model_pb2.py’ saved [6257/6257]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sentencepiece as spm\n",
        "from tokenizers.implementations import SentencePieceUnigramTokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def get_spm_special_tokens(sp_model_path):\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(sp_model_path)\n",
        "    vocab = {sp.id_to_piece(i) for i in range(sp.get_piece_size())}\n",
        "\n",
        "    special_tokens = {\n",
        "        \"unk_token\": \"<unk>\",   # Force-insert\n",
        "        \"pad_token\": \"<pad>\",\n",
        "        \"cls_token\": \"<s>\",\n",
        "        \"sep_token\": \"</s>\",\n",
        "        \"mask_token\": \"<mask>\"\n",
        "    }\n",
        "\n",
        "    for tok in special_tokens.values():\n",
        "        if tok not in vocab:\n",
        "            print(f\"⚠️ Missing special token in vocab: {tok}\")\n",
        "\n",
        "    return special_tokens\n",
        "\n",
        "\n",
        "def convert_sp_model_to_hf(sp_model_path, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    # Find which special tokens the model actually has\n",
        "    special_tokens = get_spm_special_tokens(sp_model_path)\n",
        "\n",
        "    required = ['unk_token', 'pad_token', 'cls_token', 'sep_token']\n",
        "    for req in required:\n",
        "        if req not in special_tokens:\n",
        "            print(f\"⚠️ WARNING: Required special token {req} missing in vocab of {sp_model_path}. \"\n",
        "                  f\"Downstream issues may occur.\")\n",
        "\n",
        "    # Use only existing tokens as special\n",
        "    tokenizer = SentencePieceUnigramTokenizer.from_spm(sp_model_path)\n",
        "    tokenizer.add_special_tokens(list(special_tokens.values()))\n",
        "    if \"cls_token\" in special_tokens and \"sep_token\" in special_tokens:\n",
        "        tokenizer.post_processor = BertProcessing(\n",
        "            (special_tokens[\"sep_token\"], tokenizer.token_to_id(special_tokens[\"sep_token\"])),\n",
        "            (special_tokens[\"cls_token\"], tokenizer.token_to_id(special_tokens[\"cls_token\"]))\n",
        "        )\n",
        "    tokenizer_json_path = os.path.join(save_dir, \"tokenizer.json\")\n",
        "    tokenizer.save(tokenizer_json_path)\n",
        "    hf_tokenizer = PreTrainedTokenizerFast(\n",
        "        tokenizer_file=tokenizer_json_path,\n",
        "        **special_tokens\n",
        "    )\n",
        "    hf_tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"✅ Saved Hugging Face-compatible tokenizer to {save_dir} with special tokens: {special_tokens}\")\n",
        "\n",
        "# Apply to all your unigram models\n",
        "parent_dir = \"/content/drive/MyDrive/Tokenizer_New/vocab_final\"\n",
        "sizes = [\"small\", \"medium\", \"large\"]\n",
        "sp_tokenizer_types = [\n",
        "    (\"sp_unigram\", \"sp_unigram.model\"),\n",
        "]\n",
        "\n",
        "for size in sizes:\n",
        "    for tok_dir, sp_model_file in sp_tokenizer_types:\n",
        "        sp_model_path = os.path.join(parent_dir, f\"vocab_final{size}\", tok_dir, sp_model_file)\n",
        "        save_dir = os.path.join(parent_dir, f\"vocab_final{size}\", tok_dir + \"_hf\")\n",
        "        if not os.path.exists(sp_model_path):\n",
        "            print(f\"❌ {sp_model_path} not found. Skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            convert_sp_model_to_hf(sp_model_path, save_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed for {sp_model_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "xOOyIcImqlkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d249d3-1dd8-44e4-a757-76dc5191e117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Missing special token in vocab: <pad>\n",
            "⚠️ Missing special token in vocab: <mask>\n",
            "✅ Saved Hugging Face-compatible tokenizer to /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalsmall/sp_unigram_hf with special tokens: {'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<s>', 'sep_token': '</s>', 'mask_token': '<mask>'}\n",
            "⚠️ Missing special token in vocab: <pad>\n",
            "⚠️ Missing special token in vocab: <mask>\n",
            "✅ Saved Hugging Face-compatible tokenizer to /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalmedium/sp_unigram_hf with special tokens: {'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<s>', 'sep_token': '</s>', 'mask_token': '<mask>'}\n",
            "⚠️ Missing special token in vocab: <pad>\n",
            "⚠️ Missing special token in vocab: <mask>\n",
            "✅ Saved Hugging Face-compatible tokenizer to /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finallarge/sp_unigram_hf with special tokens: {'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<s>', 'sep_token': '</s>', 'mask_token': '<mask>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import os\n",
        "# # from tokenizers.implementations import SentencePieceBPETokenizer\n",
        "# # from tokenizers.processors import BertProcessing\n",
        "# # from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# # def convert_sp_bpe_to_hf(sp_model_path, save_dir):\n",
        "# #     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# #     # Define the special tokens your SPM-BPE model was trained with\n",
        "# #     special_tokens = {\n",
        "# #         \"unk_token\": \"<unk>\",\n",
        "# #         \"pad_token\": \"<pad>\",\n",
        "# #         \"cls_token\": \"<s>\",\n",
        "# #         \"sep_token\": \"</s>\",\n",
        "# #         \"mask_token\": \"<mask>\"\n",
        "# #     }\n",
        "\n",
        "# #     # Load the SentencePiece BPE tokenizer\n",
        "# #     tokenizer = SentencePieceBPETokenizer(sp_model_path)\n",
        "\n",
        "# #     # Add the special tokens to tokenizer\n",
        "# #     tokenizer.add_special_tokens(list(special_tokens.values()))\n",
        "\n",
        "# #     # Set up BERT-style [CLS] and [SEP] post-processing\n",
        "# #     tokenizer.post_processor = BertProcessing(\n",
        "# #         (special_tokens[\"sep_token\"], tokenizer.token_to_id(special_tokens[\"sep_token\"])),\n",
        "# #         (special_tokens[\"cls_token\"], tokenizer.token_to_id(special_tokens[\"cls_token\"]))\n",
        "# #     )\n",
        "\n",
        "# #     # Save tokenizer to tokenizer.json\n",
        "# #     tokenizer_json_path = os.path.join(save_dir, \"tokenizer.json\")\n",
        "# #     tokenizer.save(tokenizer_json_path)\n",
        "\n",
        "# #     # Wrap it using PreTrainedTokenizerFast for HF compatibility\n",
        "# #     hf_tokenizer = PreTrainedTokenizerFast(\n",
        "# #         tokenizer_file=tokenizer_json_path,\n",
        "# #         **special_tokens\n",
        "# #     )\n",
        "\n",
        "# #     # Save to Hugging Face format\n",
        "# #     hf_tokenizer.save_pretrained(save_dir)\n",
        "# #     print(f\"✅ Saved Hugging Face tokenizer to {save_dir} with special tokens: {special_tokens}\")\n",
        "\n",
        "\n",
        "# # # === Use this for batch conversion for sp_bpe only ===\n",
        "# # parent_dir = \"/content/drive/MyDrive/Tokenizer_New\"\n",
        "# # sizes = [\"small\", \"medium\", \"large\"]\n",
        "# # sp_tokenizer_types = [\n",
        "# #     (\"sp_bpe\", \"sp_bpe.model\"),\n",
        "# # ]\n",
        "\n",
        "# # for size in sizes:\n",
        "# #     for tok_dir, sp_model_file in sp_tokenizer_types:\n",
        "# #         sp_model_path = os.path.join(parent_dir+f\"{size}\", sp_model_file)\n",
        "# #         save_dir = os.path.join(parent_dir+f\"{size}\", f\"vocab_final{size}\", tok_dir + \"_hf\")\n",
        "\n",
        "# #         if not os.path.exists(sp_model_path):\n",
        "# #             print(f\"❌ {sp_model_path} not found. Skipping.\")\n",
        "# #             continue\n",
        "\n",
        "# #         try:\n",
        "# #             convert_sp_bpe_to_hf(sp_model_path, save_dir)\n",
        "# #         except Exception as e:\n",
        "# #             print(f\"❌ Failed for {sp_model_path}: {e}\")\n",
        "\n",
        "# import os\n",
        "# from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# def convert_sp_bpe_to_hf(sp_model_path, save_dir):\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#     # Define special tokens consistent with SentencePiece training\n",
        "#     special_tokens = {\n",
        "#         \"unk_token\": \"<unk>\",\n",
        "#         \"pad_token\": \"<pad>\",\n",
        "#         \"cls_token\": \"<s>\",\n",
        "#         \"sep_token\": \"</s>\",\n",
        "#         \"mask_token\": \"<mask>\"\n",
        "#     }\n",
        "\n",
        "#     # Create Hugging Face-compatible tokenizer directly from the .model file\n",
        "#     tokenizer = PreTrainedTokenizerFast(\n",
        "#         sp_model_kwargs={\"model_file\": sp_model_path},\n",
        "#         **special_tokens\n",
        "#     )\n",
        "\n",
        "#     # Save tokenizer in Hugging Face format\n",
        "#     tokenizer.save_pretrained(save_dir)\n",
        "#     print(f\"✅ Saved HF-compatible tokenizer to: {save_dir}\")\n",
        "#     print(f\"🧠 Special tokens used: {special_tokens}\")\n",
        "\n",
        "\n",
        "# # === Batch convert SentencePiece .model files for each size ===\n",
        "# parent_dir = \"/content/drive/MyDrive/Tokenizer_New\"\n",
        "# sizes = [\"small\", \"medium\", \"large\"]\n",
        "# sp_tokenizer_types = [\n",
        "#     (\"sp_bpe\", \"sp_bpe.model\"),\n",
        "# ]\n",
        "\n",
        "# for size in sizes:\n",
        "#     for tok_dir, sp_model_file in sp_tokenizer_types:\n",
        "#         sp_model_path = os.path.join(parent_dir, f\"Tokenizer_New{size}\", sp_model_file)\n",
        "#         save_dir = os.path.join(parent_dir ,f\"Tokenizer_New{size}\", f\"vocab_final{size}\", tok_dir + \"_hf\")\n",
        "\n",
        "#         if not os.path.exists(sp_model_path):\n",
        "#             print(f\"❌ {sp_model_path} not found. Skipping.\")\n",
        "#             continue\n",
        "\n",
        "#         try:\n",
        "#             convert_sp_bpe_to_hf(sp_model_path, save_dir)\n",
        "#         except Exception as e:\n",
        "#             print(f\"❌ Failed for {sp_model_path}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvHw564-U0HN",
        "outputId": "2feff5c4-f087-4e6b-9312-4b32b39e913e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Failed for /content/drive/MyDrive/Tokenizer_New/Tokenizer_Newsmall/sp_bpe.model: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
            "❌ Failed for /content/drive/MyDrive/Tokenizer_New/Tokenizer_Newmedium/sp_bpe.model: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
            "❌ Failed for /content/drive/MyDrive/Tokenizer_New/Tokenizer_Newlarge/sp_bpe.model: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BertForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "-CbVLo0qy89x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load JSONL\n",
        "def load_jsonl(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "# Label dictionaries (shared across all tokenizers)\n",
        "# Label dictionaries (shared across all tokenizers)\n",
        "def build_label_maps(*datasets):\n",
        "    all_tags = set()\n",
        "    for dataset in datasets:\n",
        "        for item in dataset:\n",
        "            # Ensure tags are strings\n",
        "            clean_tags = [str(tag) for tag in item[\"tags\"]]\n",
        "            all_tags.update(clean_tags)\n",
        "            item[\"tags\"] = clean_tags  # Update reference in memory too\n",
        "\n",
        "    # Ensure 'O' is always in the tags set\n",
        "    if 'O' not in all_tags:\n",
        "        all_tags.add('O')\n",
        "\n",
        "    tags = sorted(list(all_tags))\n",
        "    label2id = {tag: i for i, tag in enumerate(tags)}\n",
        "    id2label = {i: tag for tag, i in label2id.items()}\n",
        "    return label2id, id2label\n",
        "\n",
        "# PyTorch Dataset for BERT Token Classification\n",
        "class PosDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, label2id, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        tokens, tags = item[\"tokens\"], item[\"tags\"]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        labels = [-100] * len(encoding[\"input_ids\"])\n",
        "        word_ids = encoding.word_ids()\n",
        "        prev_word = None\n",
        "        for i, word_id in enumerate(word_ids):\n",
        "            if word_id is None:\n",
        "                continue\n",
        "            if word_id != prev_word:\n",
        "                labels[i] = self.label2id.get(tags[word_id], self.label2id['O'])\n",
        "            prev_word = word_id\n",
        "\n",
        "        encoding[\"labels\"] = labels\n",
        "        return {k: torch.tensor(v) for k, v in encoding.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}"
      ],
      "metadata": {
        "id": "kUR8fV-py85x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred, id2label):\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    true_tags, pred_tags = [], []\n",
        "\n",
        "    for p_seq, l_seq in zip(preds, labels):\n",
        "        for p, l in zip(p_seq, l_seq):\n",
        "            if l != -100:\n",
        "                true_tags.append(id2label[l])\n",
        "                pred_tags.append(id2label[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, output_dict=True)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"f1_macro\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"f1_weighted\": report[\"weighted avg\"][\"f1-score\"],\n",
        "        \"report\": report\n",
        "    }\n"
      ],
      "metadata": {
        "id": "J3jDwLl2y8zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = r\"/content/drive/MyDrive/Tokenizer_New\"\n",
        "# tokenizer_base = os.path.join(base_path, 'vocab_final')\n",
        "\n",
        "dataset_dir = os.path.join(base_path, 'output')\n",
        "\n",
        "# Paths\n",
        "train_path = os.path.join(dataset_dir, 'train_pos.jsonl')\n",
        "test_path = os.path.join(dataset_dir, 'test_pos.jsonl')\n",
        "# accuracy_log_file = \"POS_accuracy.txt\"\n",
        "\n",
        "# Clear existing accuracy log\n",
        "# with open(accuracy_log_file, 'w'): pass\n",
        "\n",
        "# Tokenizer settings\n",
        "# tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
        "# tokenizer_types = [\"hf_bpe_hf\", \"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
        "tokenizer_sizes = [\"small\"]\n",
        "tokenizer_types = [\"hf_wordpiece_hf\"]\n",
        "# tokenizer_types = [ \"sp_bpe_hf\"]\n"
      ],
      "metadata": {
        "id": "DMBJla-zy8v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Only 1 model retraining"
      ],
      "metadata": {
        "id": "vT6Wl0H49Hlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BertForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        "    logging as hf_logging\n",
        ")\n",
        "\n",
        "# Suppress warnings and Hugging Face logs\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "# === 🧩 Custom Callback to print metrics after each epoch ===\n",
        "class PrintMetricsCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics:\n",
        "            print(\"\\n📊 Evaluation Results:\")\n",
        "            for key, value in metrics.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# === 🧠 Metric Computation Function (No detailed report) ===\n",
        "def compute_metrics(pred, id2label):\n",
        "    from sklearn.metrics import classification_report\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    labels = pred.label_ids\n",
        "    true_tags, pred_tags = [], []\n",
        "\n",
        "    for p_seq, l_seq in zip(preds, labels):\n",
        "        for p, l in zip(p_seq, l_seq):\n",
        "            if l != -100:\n",
        "                true_tags.append(id2label[l])\n",
        "                pred_tags.append(id2label[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, output_dict=True)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"f1_macro\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"f1_weighted\": report[\"weighted avg\"][\"f1-score\"]\n",
        "    }\n",
        "\n",
        "# === 🚀 Load Data (Assumes helper functions exist) ===\n",
        "train_data = load_jsonl(train_path)\n",
        "test_data = load_jsonl(test_path)\n",
        "label2id, id2label = build_label_maps(train_data, test_data)\n",
        "num_labels = len(label2id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === 📂 Logging Setup ===\n",
        "accuracy_log_file = \"/content/drive/MyDrive/Tokenizer_New/POS_models/accuracy_log.csv\"\n",
        "os.makedirs(os.path.dirname(accuracy_log_file), exist_ok=True)\n",
        "csv_header = [\n",
        "    \"Model Name\", \"Epoch\",\n",
        "    \"Train Accuracy\", \"Train F1 Macro\", \"Train F1 Weighted\",\n",
        "    \"Test Accuracy\", \"Test F1 Macro\", \"Test F1 Weighted\"\n",
        "]\n",
        "if not os.path.isfile(accuracy_log_file):\n",
        "    with open(accuracy_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        csv.writer(csvfile).writerow(csv_header)\n",
        "\n",
        "# === 🔄 Model Training Loop ===\n",
        "for size in tokenizer_sizes:\n",
        "    for tok_type in tokenizer_types:\n",
        "        model_name = f\"{size}_{tok_type}_POS\"\n",
        "        tokenizer_path = os.path.join(base_path, \"vocab_final\", f\"vocab_final{size}\", tok_type)\n",
        "        model_output_dir = f\"/content/drive/MyDrive/Tokenizer_New/POS_models/{model_name}\"\n",
        "\n",
        "        print(f\"\\n🔧 Running: {model_name}\")\n",
        "        print(\"🔍 Loading tokenizer from:\", tokenizer_path)\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
        "        train_dataset = PosDataset(train_data, tokenizer, label2id)\n",
        "        test_dataset = PosDataset(test_data, tokenizer, label2id)\n",
        "\n",
        "        model = BertForTokenClassification.from_pretrained(\n",
        "            \"bert-base-cased\",\n",
        "            num_labels=num_labels,\n",
        "            label2id=label2id,\n",
        "            id2label=id2label,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        model.to(device)\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            output_dir=model_output_dir,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=3,  # ⬅️ Adjust as needed\n",
        "            eval_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            learning_rate=5e-5,\n",
        "            logging_dir=os.path.join(model_output_dir, \"logs\"),\n",
        "            save_strategy=\"no\",\n",
        "            report_to=[],\n",
        "            disable_tqdm=False  # ⬅️ Show progress bar\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=lambda x: compute_metrics(x, id2label),\n",
        "            callbacks=[PrintMetricsCallback()]  # 👈 Add metrics logger\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        train_metrics = trainer.evaluate(train_dataset)\n",
        "        test_metrics = trainer.evaluate(test_dataset)\n",
        "\n",
        "        print(f\"✅ {model_name} training completed\")\n",
        "        print(\"📌 Logging results to CSV\")\n",
        "\n",
        "        epoch = 3  # Update if looping across epochs\n",
        "        with open(accuracy_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([\n",
        "                model_name, epoch,\n",
        "                round(train_metrics.get('eval_accuracy', 0.0), 4),\n",
        "                round(train_metrics.get('eval_f1_macro', 0.0), 4),\n",
        "                round(train_metrics.get('eval_f1_weighted', 0.0), 4),\n",
        "                round(test_metrics.get('eval_accuracy', 0.0), 4),\n",
        "                round(test_metrics.get('eval_f1_macro', 0.0), 4),\n",
        "                round(test_metrics.get('eval_f1_weighted', 0.0), 4),\n",
        "            ])\n",
        "\n",
        "        model.save_pretrained(model_output_dir)\n",
        "        tokenizer.save_pretrained(model_output_dir)\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "RFt5-E_iTUY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7bbbc20e894c486589b915a204e02e3a",
            "53e6ccf6938d4aeb84f29a11f67e5cac",
            "0e26003a68d746069d8848284d4cd3a3",
            "b8ea5af0e057463c9f8b0a495e840c98",
            "f2cbf79ecb4d4b8a86e34f1bd12aa0b6",
            "966ba7b437964ef090d9d7b2ed044f5c",
            "0472d42de90343f497dc162438b2f8b0",
            "678737cee9a6424f8d85211e8d582f12",
            "d88b343ae70e41699418ed26c1c51d66",
            "9349fb41b50b49c7bf0d871db9a73b3c",
            "12199c2ebcc843f391e884ea8864a90e",
            "8d47f08c13014f238f78ffee2264d5d5",
            "a51d7df101ef41f4a332d9466b048240",
            "098deb7ec5bf4a87aacd8f4b74fbf6fb",
            "871d77c802944460ad93091e17f60cf0",
            "6bd0efadcf594538af9baeaa9a8711d2",
            "56a8aafa1c5b4d4ba8b9cde87179a54d",
            "41249c9504ab4f8c8fa9abcc905ac5a7",
            "4fb18fb561b84670b844719ef4172609",
            "d812e9b531984b59bac1c2b10797547e",
            "af069cd6ef584e7ebc216c9714231a44",
            "17dda4960d4e4a5ab4b9726130f0f553"
          ]
        },
        "outputId": "5bac14c8-7644-40d8-87ac-81367149baeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Running: small_hf_wordpiece_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalsmall/hf_wordpiece_hf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bbbc20e894c486589b915a204e02e3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d47f08c13014f238f78ffee2264d5d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3104426721.py:105: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 06:10, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.684600</td>\n",
              "      <td>1.090986</td>\n",
              "      <td>0.668331</td>\n",
              "      <td>0.247713</td>\n",
              "      <td>0.640194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.930400</td>\n",
              "      <td>0.804323</td>\n",
              "      <td>0.752239</td>\n",
              "      <td>0.340665</td>\n",
              "      <td>0.737343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.707600</td>\n",
              "      <td>0.738841</td>\n",
              "      <td>0.773577</td>\n",
              "      <td>0.380381</td>\n",
              "      <td>0.760934</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0910\n",
            "  eval_accuracy: 0.6683\n",
            "  eval_f1_macro: 0.2477\n",
            "  eval_f1_weighted: 0.6402\n",
            "  eval_runtime: 9.8821\n",
            "  eval_samples_per_second: 126.2890\n",
            "  eval_steps_per_second: 7.8930\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8043\n",
            "  eval_accuracy: 0.7522\n",
            "  eval_f1_macro: 0.3407\n",
            "  eval_f1_weighted: 0.7373\n",
            "  eval_runtime: 9.8947\n",
            "  eval_samples_per_second: 126.1290\n",
            "  eval_steps_per_second: 7.8830\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7388\n",
            "  eval_accuracy: 0.7736\n",
            "  eval_f1_macro: 0.3804\n",
            "  eval_f1_weighted: 0.7609\n",
            "  eval_runtime: 9.8967\n",
            "  eval_samples_per_second: 126.1030\n",
            "  eval_steps_per_second: 7.8810\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:49]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.5422\n",
            "  eval_accuracy: 0.8309\n",
            "  eval_f1_macro: 0.3838\n",
            "  eval_f1_weighted: 0.8206\n",
            "  eval_runtime: 39.5613\n",
            "  eval_samples_per_second: 126.1080\n",
            "  eval_steps_per_second: 7.8860\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7388\n",
            "  eval_accuracy: 0.7736\n",
            "  eval_f1_macro: 0.3804\n",
            "  eval_f1_weighted: 0.7609\n",
            "  eval_runtime: 9.9642\n",
            "  eval_samples_per_second: 125.2490\n",
            "  eval_steps_per_second: 7.8280\n",
            "  epoch: 3.0000\n",
            "✅ small_hf_wordpiece_hf_POS training completed\n",
            "📌 Logging results to CSV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fINAL pos"
      ],
      "metadata": {
        "id": "4sd7fEfq9FUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BertForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        "    logging as hf_logging\n",
        ")\n",
        "\n",
        "# Suppress warnings and Hugging Face logs\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "hf_logging.set_verbosity_error()\n",
        "\n",
        "# === 🧩 Custom Callback to print metrics after each epoch ===\n",
        "class PrintMetricsCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics:\n",
        "            print(\"\\n📊 Evaluation Results:\")\n",
        "            for key, value in metrics.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# === 🧠 Metric Computation Function (No detailed report) ===\n",
        "def compute_metrics(pred, id2label):\n",
        "    from sklearn.metrics import classification_report\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    labels = pred.label_ids\n",
        "    true_tags, pred_tags = [], []\n",
        "\n",
        "    for p_seq, l_seq in zip(preds, labels):\n",
        "        for p, l in zip(p_seq, l_seq):\n",
        "            if l != -100:\n",
        "                true_tags.append(id2label[l])\n",
        "                pred_tags.append(id2label[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, output_dict=True)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"f1_macro\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"f1_weighted\": report[\"weighted avg\"][\"f1-score\"]\n",
        "    }\n",
        "\n",
        "# === 🚀 Load Data (Assumes helper functions exist) ===\n",
        "train_data = load_jsonl(train_path)\n",
        "test_data = load_jsonl(test_path)\n",
        "label2id, id2label = build_label_maps(train_data, test_data)\n",
        "num_labels = len(label2id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === 📂 Logging Setup ===\n",
        "accuracy_log_file = \"/content/drive/MyDrive/Tokenizer_New/POS_models/accuracy_log.csv\"\n",
        "os.makedirs(os.path.dirname(accuracy_log_file), exist_ok=True)\n",
        "csv_header = [\n",
        "    \"Model Name\", \"Epoch\",\n",
        "    \"Train Accuracy\", \"Train F1 Macro\", \"Train F1 Weighted\",\n",
        "    \"Test Accuracy\", \"Test F1 Macro\", \"Test F1 Weighted\"\n",
        "]\n",
        "if not os.path.isfile(accuracy_log_file):\n",
        "    with open(accuracy_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        csv.writer(csvfile).writerow(csv_header)\n",
        "\n",
        "# === 🔄 Model Training Loop ===\n",
        "for size in tokenizer_sizes:\n",
        "    for tok_type in tokenizer_types:\n",
        "        model_name = f\"{size}_{tok_type}_POS\"\n",
        "        tokenizer_path = os.path.join(base_path, \"vocab_final\", f\"vocab_final{size}\", tok_type)\n",
        "        model_output_dir = f\"/content/drive/MyDrive/Tokenizer_New/POS_models/{model_name}\"\n",
        "\n",
        "        print(f\"\\n🔧 Running: {model_name}\")\n",
        "        print(\"🔍 Loading tokenizer from:\", tokenizer_path)\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
        "        train_dataset = PosDataset(train_data, tokenizer, label2id)\n",
        "        test_dataset = PosDataset(test_data, tokenizer, label2id)\n",
        "\n",
        "        model = BertForTokenClassification.from_pretrained(\n",
        "            \"bert-base-cased\",\n",
        "            num_labels=num_labels,\n",
        "            label2id=label2id,\n",
        "            id2label=id2label,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "        model.to(device)\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            output_dir=model_output_dir,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=3,  # ⬅️ Adjust as needed\n",
        "            eval_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            learning_rate=5e-5,\n",
        "            logging_dir=os.path.join(model_output_dir, \"logs\"),\n",
        "            save_strategy=\"no\",\n",
        "            report_to=[],\n",
        "            disable_tqdm=False  # ⬅️ Show progress bar\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=lambda x: compute_metrics(x, id2label),\n",
        "            callbacks=[PrintMetricsCallback()]  # 👈 Add metrics logger\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        train_metrics = trainer.evaluate(train_dataset)\n",
        "        test_metrics = trainer.evaluate(test_dataset)\n",
        "\n",
        "        print(f\"✅ {model_name} training completed\")\n",
        "        print(\"📌 Logging results to CSV\")\n",
        "\n",
        "        epoch = 3  # Update if looping across epochs\n",
        "        with open(accuracy_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([\n",
        "                model_name, epoch,\n",
        "                round(train_metrics.get('eval_accuracy', 0.0), 4),\n",
        "                round(train_metrics.get('eval_f1_macro', 0.0), 4),\n",
        "                round(train_metrics.get('eval_f1_weighted', 0.0), 4),\n",
        "                round(test_metrics.get('eval_accuracy', 0.0), 4),\n",
        "                round(test_metrics.get('eval_f1_macro', 0.0), 4),\n",
        "                round(test_metrics.get('eval_f1_weighted', 0.0), 4),\n",
        "            ])\n",
        "\n",
        "        model.save_pretrained(model_output_dir)\n",
        "        tokenizer.save_pretrained(model_output_dir)\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ct56PkUCy8sg",
        "outputId": "9e749b56-152e-406e-a989-fa2d2b62751a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Running: small_hf_bpe_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalsmall/hf_bpe_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.820600</td>\n",
              "      <td>1.253701</td>\n",
              "      <td>0.622557</td>\n",
              "      <td>0.216492</td>\n",
              "      <td>0.596295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.098100</td>\n",
              "      <td>0.980833</td>\n",
              "      <td>0.695459</td>\n",
              "      <td>0.278272</td>\n",
              "      <td>0.678960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.868400</td>\n",
              "      <td>0.913823</td>\n",
              "      <td>0.720705</td>\n",
              "      <td>0.297295</td>\n",
              "      <td>0.705706</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.2537\n",
            "  eval_accuracy: 0.6226\n",
            "  eval_f1_macro: 0.2165\n",
            "  eval_f1_weighted: 0.5963\n",
            "  eval_runtime: 8.8563\n",
            "  eval_samples_per_second: 140.9170\n",
            "  eval_steps_per_second: 8.8070\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9808\n",
            "  eval_accuracy: 0.6955\n",
            "  eval_f1_macro: 0.2783\n",
            "  eval_f1_weighted: 0.6790\n",
            "  eval_runtime: 8.7022\n",
            "  eval_samples_per_second: 143.4120\n",
            "  eval_steps_per_second: 8.9630\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9138\n",
            "  eval_accuracy: 0.7207\n",
            "  eval_f1_macro: 0.2973\n",
            "  eval_f1_weighted: 0.7057\n",
            "  eval_runtime: 8.7907\n",
            "  eval_samples_per_second: 141.9690\n",
            "  eval_steps_per_second: 8.8730\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7000\n",
            "  eval_accuracy: 0.7847\n",
            "  eval_f1_macro: 0.2957\n",
            "  eval_f1_weighted: 0.7722\n",
            "  eval_runtime: 35.2379\n",
            "  eval_samples_per_second: 141.5800\n",
            "  eval_steps_per_second: 8.8540\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9138\n",
            "  eval_accuracy: 0.7207\n",
            "  eval_f1_macro: 0.2973\n",
            "  eval_f1_weighted: 0.7057\n",
            "  eval_runtime: 8.6757\n",
            "  eval_samples_per_second: 143.8490\n",
            "  eval_steps_per_second: 8.9910\n",
            "  epoch: 3.0000\n",
            "✅ small_hf_bpe_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: small_hf_wordpiece_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalsmall/hf_wordpiece_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.678800</td>\n",
              "      <td>1.058406</td>\n",
              "      <td>0.677550</td>\n",
              "      <td>0.255755</td>\n",
              "      <td>0.659275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.900700</td>\n",
              "      <td>0.766397</td>\n",
              "      <td>0.763625</td>\n",
              "      <td>0.357712</td>\n",
              "      <td>0.751308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.682100</td>\n",
              "      <td>0.709198</td>\n",
              "      <td>0.783033</td>\n",
              "      <td>0.395188</td>\n",
              "      <td>0.772219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0584\n",
            "  eval_accuracy: 0.6776\n",
            "  eval_f1_macro: 0.2558\n",
            "  eval_f1_weighted: 0.6593\n",
            "  eval_runtime: 8.7115\n",
            "  eval_samples_per_second: 143.2590\n",
            "  eval_steps_per_second: 8.9540\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7664\n",
            "  eval_accuracy: 0.7636\n",
            "  eval_f1_macro: 0.3577\n",
            "  eval_f1_weighted: 0.7513\n",
            "  eval_runtime: 8.8034\n",
            "  eval_samples_per_second: 141.7630\n",
            "  eval_steps_per_second: 8.8600\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7092\n",
            "  eval_accuracy: 0.7830\n",
            "  eval_f1_macro: 0.3952\n",
            "  eval_f1_weighted: 0.7722\n",
            "  eval_runtime: 8.8583\n",
            "  eval_samples_per_second: 140.8850\n",
            "  eval_steps_per_second: 8.8050\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.5149\n",
            "  eval_accuracy: 0.8413\n",
            "  eval_f1_macro: 0.3889\n",
            "  eval_f1_weighted: 0.8325\n",
            "  eval_runtime: 35.4375\n",
            "  eval_samples_per_second: 140.7830\n",
            "  eval_steps_per_second: 8.8040\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7092\n",
            "  eval_accuracy: 0.7830\n",
            "  eval_f1_macro: 0.3952\n",
            "  eval_f1_weighted: 0.7722\n",
            "  eval_runtime: 8.9310\n",
            "  eval_samples_per_second: 139.7380\n",
            "  eval_steps_per_second: 8.7340\n",
            "  epoch: 3.0000\n",
            "✅ small_hf_wordpiece_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: small_sp_unigram_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalsmall/sp_unigram_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.866400</td>\n",
              "      <td>1.249190</td>\n",
              "      <td>0.616414</td>\n",
              "      <td>0.214627</td>\n",
              "      <td>0.585714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.075700</td>\n",
              "      <td>0.926874</td>\n",
              "      <td>0.711538</td>\n",
              "      <td>0.288657</td>\n",
              "      <td>0.697370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.806400</td>\n",
              "      <td>0.824974</td>\n",
              "      <td>0.748409</td>\n",
              "      <td>0.329877</td>\n",
              "      <td>0.734043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.2492\n",
            "  eval_accuracy: 0.6164\n",
            "  eval_f1_macro: 0.2146\n",
            "  eval_f1_weighted: 0.5857\n",
            "  eval_runtime: 8.8669\n",
            "  eval_samples_per_second: 140.7480\n",
            "  eval_steps_per_second: 8.7970\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9269\n",
            "  eval_accuracy: 0.7115\n",
            "  eval_f1_macro: 0.2887\n",
            "  eval_f1_weighted: 0.6974\n",
            "  eval_runtime: 8.8955\n",
            "  eval_samples_per_second: 140.2960\n",
            "  eval_steps_per_second: 8.7680\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8250\n",
            "  eval_accuracy: 0.7484\n",
            "  eval_f1_macro: 0.3299\n",
            "  eval_f1_weighted: 0.7340\n",
            "  eval_runtime: 8.9152\n",
            "  eval_samples_per_second: 139.9850\n",
            "  eval_steps_per_second: 8.7490\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:44]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.6272\n",
            "  eval_accuracy: 0.8067\n",
            "  eval_f1_macro: 0.3209\n",
            "  eval_f1_weighted: 0.7947\n",
            "  eval_runtime: 35.5970\n",
            "  eval_samples_per_second: 140.1520\n",
            "  eval_steps_per_second: 8.7650\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8250\n",
            "  eval_accuracy: 0.7484\n",
            "  eval_f1_macro: 0.3299\n",
            "  eval_f1_weighted: 0.7340\n",
            "  eval_runtime: 9.0279\n",
            "  eval_samples_per_second: 138.2380\n",
            "  eval_steps_per_second: 8.6400\n",
            "  epoch: 3.0000\n",
            "✅ small_sp_unigram_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: medium_hf_bpe_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalmedium/hf_bpe_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.877600</td>\n",
              "      <td>1.355067</td>\n",
              "      <td>0.586857</td>\n",
              "      <td>0.201274</td>\n",
              "      <td>0.558501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.195100</td>\n",
              "      <td>1.079542</td>\n",
              "      <td>0.668813</td>\n",
              "      <td>0.256299</td>\n",
              "      <td>0.644675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.965200</td>\n",
              "      <td>1.007952</td>\n",
              "      <td>0.693160</td>\n",
              "      <td>0.272376</td>\n",
              "      <td>0.673517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.3551\n",
            "  eval_accuracy: 0.5869\n",
            "  eval_f1_macro: 0.2013\n",
            "  eval_f1_weighted: 0.5585\n",
            "  eval_runtime: 8.8558\n",
            "  eval_samples_per_second: 140.9240\n",
            "  eval_steps_per_second: 8.8080\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0795\n",
            "  eval_accuracy: 0.6688\n",
            "  eval_f1_macro: 0.2563\n",
            "  eval_f1_weighted: 0.6447\n",
            "  eval_runtime: 8.8317\n",
            "  eval_samples_per_second: 141.3100\n",
            "  eval_steps_per_second: 8.8320\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0080\n",
            "  eval_accuracy: 0.6932\n",
            "  eval_f1_macro: 0.2724\n",
            "  eval_f1_weighted: 0.6735\n",
            "  eval_runtime: 8.9631\n",
            "  eval_samples_per_second: 139.2380\n",
            "  eval_steps_per_second: 8.7020\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8039\n",
            "  eval_accuracy: 0.7526\n",
            "  eval_f1_macro: 0.2686\n",
            "  eval_f1_weighted: 0.7366\n",
            "  eval_runtime: 35.3578\n",
            "  eval_samples_per_second: 141.1000\n",
            "  eval_steps_per_second: 8.8240\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0080\n",
            "  eval_accuracy: 0.6932\n",
            "  eval_f1_macro: 0.2724\n",
            "  eval_f1_weighted: 0.6735\n",
            "  eval_runtime: 8.8234\n",
            "  eval_samples_per_second: 141.4420\n",
            "  eval_steps_per_second: 8.8400\n",
            "  epoch: 3.0000\n",
            "✅ medium_hf_bpe_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: medium_hf_wordpiece_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalmedium/hf_wordpiece_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:44, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.836100</td>\n",
              "      <td>1.266284</td>\n",
              "      <td>0.617625</td>\n",
              "      <td>0.209012</td>\n",
              "      <td>0.584500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.084200</td>\n",
              "      <td>0.973385</td>\n",
              "      <td>0.706892</td>\n",
              "      <td>0.278520</td>\n",
              "      <td>0.691141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.842300</td>\n",
              "      <td>0.893388</td>\n",
              "      <td>0.732484</td>\n",
              "      <td>0.291479</td>\n",
              "      <td>0.717426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.2663\n",
            "  eval_accuracy: 0.6176\n",
            "  eval_f1_macro: 0.2090\n",
            "  eval_f1_weighted: 0.5845\n",
            "  eval_runtime: 8.8677\n",
            "  eval_samples_per_second: 140.7350\n",
            "  eval_steps_per_second: 8.7960\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9734\n",
            "  eval_accuracy: 0.7069\n",
            "  eval_f1_macro: 0.2785\n",
            "  eval_f1_weighted: 0.6911\n",
            "  eval_runtime: 8.9436\n",
            "  eval_samples_per_second: 139.5410\n",
            "  eval_steps_per_second: 8.7210\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8934\n",
            "  eval_accuracy: 0.7325\n",
            "  eval_f1_macro: 0.2915\n",
            "  eval_f1_weighted: 0.7174\n",
            "  eval_runtime: 8.7906\n",
            "  eval_samples_per_second: 141.9690\n",
            "  eval_steps_per_second: 8.8730\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.6659\n",
            "  eval_accuracy: 0.7964\n",
            "  eval_f1_macro: 0.2877\n",
            "  eval_f1_weighted: 0.7839\n",
            "  eval_runtime: 35.3469\n",
            "  eval_samples_per_second: 141.1440\n",
            "  eval_steps_per_second: 8.8270\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8934\n",
            "  eval_accuracy: 0.7325\n",
            "  eval_f1_macro: 0.2915\n",
            "  eval_f1_weighted: 0.7174\n",
            "  eval_runtime: 8.9692\n",
            "  eval_samples_per_second: 139.1430\n",
            "  eval_steps_per_second: 8.6960\n",
            "  epoch: 3.0000\n",
            "✅ medium_hf_wordpiece_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: medium_sp_unigram_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finalmedium/sp_unigram_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.917100</td>\n",
              "      <td>1.345953</td>\n",
              "      <td>0.595759</td>\n",
              "      <td>0.193717</td>\n",
              "      <td>0.553467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.174400</td>\n",
              "      <td>1.015500</td>\n",
              "      <td>0.692879</td>\n",
              "      <td>0.295584</td>\n",
              "      <td>0.672473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.909300</td>\n",
              "      <td>0.927165</td>\n",
              "      <td>0.721081</td>\n",
              "      <td>0.321789</td>\n",
              "      <td>0.706057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.3460\n",
            "  eval_accuracy: 0.5958\n",
            "  eval_f1_macro: 0.1937\n",
            "  eval_f1_weighted: 0.5535\n",
            "  eval_runtime: 8.8679\n",
            "  eval_samples_per_second: 140.7320\n",
            "  eval_steps_per_second: 8.7960\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0155\n",
            "  eval_accuracy: 0.6929\n",
            "  eval_f1_macro: 0.2956\n",
            "  eval_f1_weighted: 0.6725\n",
            "  eval_runtime: 8.9353\n",
            "  eval_samples_per_second: 139.6700\n",
            "  eval_steps_per_second: 8.7290\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9272\n",
            "  eval_accuracy: 0.7211\n",
            "  eval_f1_macro: 0.3218\n",
            "  eval_f1_weighted: 0.7061\n",
            "  eval_runtime: 8.9698\n",
            "  eval_samples_per_second: 139.1330\n",
            "  eval_steps_per_second: 8.6960\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:44]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7375\n",
            "  eval_accuracy: 0.7752\n",
            "  eval_f1_macro: 0.3141\n",
            "  eval_f1_weighted: 0.7625\n",
            "  eval_runtime: 35.7799\n",
            "  eval_samples_per_second: 139.4360\n",
            "  eval_steps_per_second: 8.7200\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9272\n",
            "  eval_accuracy: 0.7211\n",
            "  eval_f1_macro: 0.3218\n",
            "  eval_f1_weighted: 0.7061\n",
            "  eval_runtime: 9.0352\n",
            "  eval_samples_per_second: 138.1260\n",
            "  eval_steps_per_second: 8.6330\n",
            "  epoch: 3.0000\n",
            "✅ medium_sp_unigram_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: large_hf_bpe_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finallarge/hf_bpe_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.863800</td>\n",
              "      <td>1.317910</td>\n",
              "      <td>0.600021</td>\n",
              "      <td>0.214637</td>\n",
              "      <td>0.567274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.163100</td>\n",
              "      <td>1.060182</td>\n",
              "      <td>0.675103</td>\n",
              "      <td>0.257960</td>\n",
              "      <td>0.648381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.938500</td>\n",
              "      <td>0.989401</td>\n",
              "      <td>0.701466</td>\n",
              "      <td>0.277514</td>\n",
              "      <td>0.681194</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.3179\n",
            "  eval_accuracy: 0.6000\n",
            "  eval_f1_macro: 0.2146\n",
            "  eval_f1_weighted: 0.5673\n",
            "  eval_runtime: 8.8814\n",
            "  eval_samples_per_second: 140.5180\n",
            "  eval_steps_per_second: 8.7820\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0602\n",
            "  eval_accuracy: 0.6751\n",
            "  eval_f1_macro: 0.2580\n",
            "  eval_f1_weighted: 0.6484\n",
            "  eval_runtime: 8.8460\n",
            "  eval_samples_per_second: 141.0800\n",
            "  eval_steps_per_second: 8.8180\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9894\n",
            "  eval_accuracy: 0.7015\n",
            "  eval_f1_macro: 0.2775\n",
            "  eval_f1_weighted: 0.6812\n",
            "  eval_runtime: 8.8327\n",
            "  eval_samples_per_second: 141.2920\n",
            "  eval_steps_per_second: 8.8310\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7718\n",
            "  eval_accuracy: 0.7643\n",
            "  eval_f1_macro: 0.2719\n",
            "  eval_f1_weighted: 0.7484\n",
            "  eval_runtime: 35.0883\n",
            "  eval_samples_per_second: 142.1840\n",
            "  eval_steps_per_second: 8.8920\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9894\n",
            "  eval_accuracy: 0.7015\n",
            "  eval_f1_macro: 0.2775\n",
            "  eval_f1_weighted: 0.6812\n",
            "  eval_runtime: 8.8094\n",
            "  eval_samples_per_second: 141.6670\n",
            "  eval_steps_per_second: 8.8540\n",
            "  epoch: 3.0000\n",
            "✅ large_hf_bpe_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: large_hf_wordpiece_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finallarge/hf_wordpiece_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.846300</td>\n",
              "      <td>1.303553</td>\n",
              "      <td>0.605035</td>\n",
              "      <td>0.210884</td>\n",
              "      <td>0.569007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.125600</td>\n",
              "      <td>0.986396</td>\n",
              "      <td>0.703427</td>\n",
              "      <td>0.271906</td>\n",
              "      <td>0.683168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.854200</td>\n",
              "      <td>0.890353</td>\n",
              "      <td>0.733462</td>\n",
              "      <td>0.288317</td>\n",
              "      <td>0.718666</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.3036\n",
            "  eval_accuracy: 0.6050\n",
            "  eval_f1_macro: 0.2109\n",
            "  eval_f1_weighted: 0.5690\n",
            "  eval_runtime: 8.8610\n",
            "  eval_samples_per_second: 140.8420\n",
            "  eval_steps_per_second: 8.8030\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9864\n",
            "  eval_accuracy: 0.7034\n",
            "  eval_f1_macro: 0.2719\n",
            "  eval_f1_weighted: 0.6832\n",
            "  eval_runtime: 8.7458\n",
            "  eval_samples_per_second: 142.6970\n",
            "  eval_steps_per_second: 8.9190\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8904\n",
            "  eval_accuracy: 0.7335\n",
            "  eval_f1_macro: 0.2883\n",
            "  eval_f1_weighted: 0.7187\n",
            "  eval_runtime: 8.7755\n",
            "  eval_samples_per_second: 142.2140\n",
            "  eval_steps_per_second: 8.8880\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:43]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.6639\n",
            "  eval_accuracy: 0.8001\n",
            "  eval_f1_macro: 0.2848\n",
            "  eval_f1_weighted: 0.7880\n",
            "  eval_runtime: 35.3254\n",
            "  eval_samples_per_second: 141.2300\n",
            "  eval_steps_per_second: 8.8320\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.8904\n",
            "  eval_accuracy: 0.7335\n",
            "  eval_f1_macro: 0.2883\n",
            "  eval_f1_weighted: 0.7187\n",
            "  eval_runtime: 8.8154\n",
            "  eval_samples_per_second: 141.5700\n",
            "  eval_steps_per_second: 8.8480\n",
            "  epoch: 3.0000\n",
            "✅ large_hf_wordpiece_hf_POS training completed\n",
            "📌 Logging results to CSV\n",
            "\n",
            "🔧 Running: large_sp_unigram_hf_POS\n",
            "🔍 Loading tokenizer from: /content/drive/MyDrive/Tokenizer_New/vocab_final/vocab_finallarge/sp_unigram_hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2567049121.py:227: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 05:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.921200</td>\n",
              "      <td>1.372876</td>\n",
              "      <td>0.585726</td>\n",
              "      <td>0.192165</td>\n",
              "      <td>0.545082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.187700</td>\n",
              "      <td>1.029719</td>\n",
              "      <td>0.686197</td>\n",
              "      <td>0.276444</td>\n",
              "      <td>0.661232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.932600</td>\n",
              "      <td>0.949980</td>\n",
              "      <td>0.710700</td>\n",
              "      <td>0.314127</td>\n",
              "      <td>0.694399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.3729\n",
            "  eval_accuracy: 0.5857\n",
            "  eval_f1_macro: 0.1922\n",
            "  eval_f1_weighted: 0.5451\n",
            "  eval_runtime: 8.8756\n",
            "  eval_samples_per_second: 140.6100\n",
            "  eval_steps_per_second: 8.7880\n",
            "  epoch: 1.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 1.0297\n",
            "  eval_accuracy: 0.6862\n",
            "  eval_f1_macro: 0.2764\n",
            "  eval_f1_weighted: 0.6612\n",
            "  eval_runtime: 8.9084\n",
            "  eval_samples_per_second: 140.0920\n",
            "  eval_steps_per_second: 8.7560\n",
            "  epoch: 2.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9500\n",
            "  eval_accuracy: 0.7107\n",
            "  eval_f1_macro: 0.3141\n",
            "  eval_f1_weighted: 0.6944\n",
            "  eval_runtime: 8.8935\n",
            "  eval_samples_per_second: 140.3260\n",
            "  eval_steps_per_second: 8.7700\n",
            "  epoch: 3.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [312/312 00:44]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.7654\n",
            "  eval_accuracy: 0.7662\n",
            "  eval_f1_macro: 0.3085\n",
            "  eval_f1_weighted: 0.7528\n",
            "  eval_runtime: 35.6033\n",
            "  eval_samples_per_second: 140.1270\n",
            "  eval_steps_per_second: 8.7630\n",
            "  epoch: 3.0000\n",
            "\n",
            "📊 Evaluation Results:\n",
            "  eval_loss: 0.9500\n",
            "  eval_accuracy: 0.7107\n",
            "  eval_f1_macro: 0.3141\n",
            "  eval_f1_weighted: 0.6944\n",
            "  eval_runtime: 9.0682\n",
            "  eval_samples_per_second: 137.6240\n",
            "  eval_steps_per_second: 8.6020\n",
            "  epoch: 3.0000\n",
            "✅ large_sp_unigram_hf_POS training completed\n",
            "📌 Logging results to CSV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UtKU3qa8y8pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oyi1-vAdy8iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4M36Nl_Wy8fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jN0xGzfZy8cx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}