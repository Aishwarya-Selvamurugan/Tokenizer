{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9b5a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded and extracted to 'ted2020_en_sw'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "url = \"https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-sw.txt.zip\"\n",
    "output_path = \"en-sw.txt.zip\"\n",
    "\n",
    "# Download the zip file\n",
    "urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "# Extract it\n",
    "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"ted2020_en_sw\")\n",
    "\n",
    "print(\"✅ Downloaded and extracted to 'ted2020_en_sw'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32565860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total sentence pairs: 9745\n"
     ]
    }
   ],
   "source": [
    "en_file_path = \"ted2020_en_sw/TED2020.en-sw.en\"\n",
    "sw_file_path = \"ted2020_en_sw/TED2020.en-sw.sw\"\n",
    "\n",
    "# Count lines (each line = 1 sentence)\n",
    "with open(en_file_path, \"r\", encoding=\"utf-8\") as en_file:\n",
    "    en_lines = en_file.readlines()\n",
    "\n",
    "with open(sw_file_path, \"r\", encoding=\"utf-8\") as sw_file:\n",
    "    sw_lines = sw_file.readlines()\n",
    "\n",
    "# Safety check (in case of mismatch)\n",
    "assert len(en_lines) == len(sw_lines), \"Mismatch between EN and SW sentence count\"\n",
    "\n",
    "print(f\"✅ Total sentence pairs: {len(en_lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad7ae49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# ✅ Language map\n",
    "languages = {\n",
    "    \"yo\": \"Yoruba\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"zh\": \"Chinese\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"sw\": \"Swahili\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"tr\": \"Turkish\"\n",
    "}\n",
    "\n",
    "# ✅ Data quality filter function\n",
    "def is_valid_translation_pair(en_text, target_text):\n",
    "    \"\"\"Filter out low-quality translation pairs\"\"\"\n",
    "    if not en_text or not target_text:\n",
    "        return False\n",
    "    \n",
    "    # Remove whitespace for comparison\n",
    "    en_clean = en_text.strip()\n",
    "    target_clean = target_text.strip()\n",
    "    \n",
    "    # Filter out identical pairs (likely technical strings)\n",
    "    if en_clean == target_clean:\n",
    "        return False\n",
    "    \n",
    "    # Filter out very short pairs (less than 3 words)\n",
    "    if len(en_clean.split()) < 3 or len(target_clean.split()) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Filter out technical/UI strings\n",
    "    technical_keywords = [\n",
    "        'html', 'xml', 'mozilla', 'utf-8', 'iso-8859', 'ascii',\n",
    "        'preferences', 'workspace', 'desktop file', 'invalid',\n",
    "        'error', 'warning', 'debug', 'log', 'config'\n",
    "    ]\n",
    "    \n",
    "    if any(keyword in en_clean.lower() for keyword in technical_keywords):\n",
    "        return False\n",
    "    \n",
    "    # Filter out very long sentences (likely corrupted)\n",
    "    if len(en_clean) > 200 or len(target_clean) > 200:\n",
    "        return False\n",
    "    \n",
    "    # Filter out sentences that are mostly punctuation or numbers\n",
    "    if len(''.join(c for c in en_clean if c.isalpha())) < 10:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "loaded_datasets = {}  # lang_code: Dataset\n",
    "min_count = float(\"inf\")\n",
    "\n",
    "# ✅ Load Swahili manually\n",
    "def load_ted2020_en_sw(path=\"ted2020_en_sw\"):\n",
    "    \"\"\"Load TED2020 English-Swahili dataset\"\"\"\n",
    "    try:\n",
    "        with open(os.path.join(path, \"TED2020.en-sw.en\"), encoding=\"utf-8\") as f_en, \\\n",
    "             open(os.path.join(path, \"TED2020.en-sw.sw\"), encoding=\"utf-8\") as f_sw:\n",
    "            en_lines = f_en.readlines()\n",
    "            sw_lines = f_sw.readlines()\n",
    "        \n",
    "        # Filter valid pairs during loading\n",
    "        valid_pairs = []\n",
    "        for en, sw in zip(en_lines, sw_lines):\n",
    "            en_clean = en.strip()\n",
    "            sw_clean = sw.strip()\n",
    "            if is_valid_translation_pair(en_clean, sw_clean):\n",
    "                valid_pairs.append({\"en\": en_clean, \"sw\": sw_clean})\n",
    "        \n",
    "        data = {\"translation\": valid_pairs}\n",
    "        print(f\"📁 Loaded {len(valid_pairs)} valid Swahili pairs (filtered from {len(en_lines)} total)\")\n",
    "        return Dataset.from_dict(data)\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️  Swahili TED2020 files not found, skipping Swahili\")\n",
    "        return None\n",
    "\n",
    "# ✅ Load datasets and find minimum size for balancing\n",
    "print(\"📦 Loading datasets...\")\n",
    "for code in languages:\n",
    "    print(f\"Loading {languages[code]} ({code})...\")\n",
    "    \n",
    "    if code == \"sw\":\n",
    "        ds = load_ted2020_en_sw()\n",
    "        if ds is None:\n",
    "            continue\n",
    "    else:\n",
    "        try:\n",
    "            # Try both directions for OPUS100\n",
    "            try:\n",
    "                ds = load_dataset(\"opus100\", f\"en-{code}\")[\"train\"]\n",
    "            except:\n",
    "                ds = load_dataset(\"opus100\", f\"{code}-en\")[\"train\"]\n",
    "            \n",
    "            # Filter the dataset for quality\n",
    "            def filter_dataset(example):\n",
    "                en_text = example[\"translation\"][\"en\"]\n",
    "                target_text = example[\"translation\"][code]\n",
    "                return is_valid_translation_pair(en_text, target_text)\n",
    "            \n",
    "            print(f\"  Original size: {len(ds)}\")\n",
    "            ds = ds.filter(filter_dataset)\n",
    "            print(f\"  After filtering: {len(ds)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed to load {code}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    loaded_datasets[code] = ds\n",
    "    min_count = min(min_count, len(ds))\n",
    "    print(f\"  ✅ Loaded {len(ds)} valid pairs\")\n",
    "\n",
    "print(f\"\\n📊 Balancing all datasets to {min_count} sentence pairs\")\n",
    "\n",
    "# ✅ Combine and clean datasets\n",
    "combined_data = {\"translation\": [], \"language\": []}\n",
    "total_added = 0\n",
    "\n",
    "for code, ds in loaded_datasets.items():\n",
    "    print(f\"Processing {languages[code]}...\")\n",
    "    \n",
    "    # Shuffle and sample\n",
    "    sampled = ds.shuffle(seed=42).select(range(min_count))\n",
    "    added_count = 0\n",
    "    \n",
    "    for row in sampled:\n",
    "        en_text = row[\"translation\"][\"en\"].strip()\n",
    "        tgt_text = row[\"translation\"][code].strip()\n",
    "        \n",
    "        # Double-check quality (in case some slipped through)\n",
    "        if is_valid_translation_pair(en_text, tgt_text):\n",
    "            combined_data[\"translation\"].append({\n",
    "                \"en\": en_text,\n",
    "                code: tgt_text\n",
    "            })\n",
    "            combined_data[\"language\"].append(code)\n",
    "            added_count += 1\n",
    "    \n",
    "    print(f\"  ✅ Added {added_count} pairs\")\n",
    "    total_added += added_count\n",
    "\n",
    "print(f\"\\n📈 Total dataset size: {total_added} translation pairs\")\n",
    "\n",
    "# ✅ Create Hugging Face Dataset and split\n",
    "full_dataset = Dataset.from_dict(combined_data)\n",
    "split = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"test\": split[\"test\"]\n",
    "})\n",
    "\n",
    "# ✅ Save to disk\n",
    "final_dataset.save_to_disk(\"balanced_mt_dataset\")\n",
    "print(\"✅ Final dataset saved to 'balanced_mt_dataset'\")\n",
    "\n",
    "# ✅ Quality check - show first 10 examples\n",
    "print(\"\\n🔍 Quality Check - First 10 examples:\")\n",
    "for i in range(min(10, len(final_dataset[\"train\"]))):\n",
    "    example = final_dataset[\"train\"][i]\n",
    "    lang = example[\"language\"]\n",
    "    en_text = example[\"translation\"][\"en\"]\n",
    "    target_text = example[\"translation\"][lang]\n",
    "    \n",
    "    # Truncate long texts for display\n",
    "    en_display = en_text[:60] + \"...\" if len(en_text) > 60 else en_text\n",
    "    target_display = target_text[:60] + \"...\" if len(target_text) > 60 else target_text\n",
    "    \n",
    "    print(f\"{i+1:2d}. {lang.upper():3s} | EN: {en_display}\")\n",
    "    print(f\"    {' '*3} | {lang.upper():2s}: {target_display}\")\n",
    "    print()\n",
    "\n",
    "# ✅ Dataset statistics\n",
    "print(\"📊 Dataset Statistics:\")\n",
    "print(f\"Train size: {len(final_dataset['train'])}\")\n",
    "print(f\"Test size: {len(final_dataset['test'])}\")\n",
    "\n",
    "lang_counts = collections.Counter(combined_data[\"language\"])\n",
    "for code, count in lang_counts.items():\n",
    "    print(f\"{languages[code]:10s}: {count:,} pairs\")\n",
    "\n",
    "# ✅ Visualize balanced language distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "lang_names = [languages[k] for k in lang_counts.keys()]\n",
    "counts = list(lang_counts.values())\n",
    "\n",
    "sns.barplot(x=lang_names, y=counts, palette=\"viridis\")\n",
    "plt.title(\"Balanced Sentence Distribution Across Languages\\n(After Quality Filtering)\")\n",
    "plt.xlabel(\"Language\")\n",
    "plt.ylabel(\"Translation Pairs\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count + max(counts)*0.01, f'{count:,}', \n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ✅ Additional analysis: Average sentence lengths\n",
    "print(\"\\n📏 Average Sentence Lengths:\")\n",
    "lang_lengths = {code: {'en': [], 'target': []} for code in languages.keys()}\n",
    "\n",
    "for example in final_dataset[\"train\"]:\n",
    "    lang = example[\"language\"]\n",
    "    if lang in lang_lengths:\n",
    "        en_len = len(example[\"translation\"][\"en\"].split())\n",
    "        target_len = len(example[\"translation\"][lang].split())\n",
    "        lang_lengths[lang]['en'].append(en_len)\n",
    "        lang_lengths[lang]['target'].append(target_len)\n",
    "\n",
    "for code, lengths in lang_lengths.items():\n",
    "    if lengths['en']:  # Only if we have data for this language\n",
    "        avg_en = sum(lengths['en']) / len(lengths['en'])\n",
    "        avg_target = sum(lengths['target']) / len(lengths['target'])\n",
    "        print(f\"{languages[code]:10s}: EN={avg_en:.1f} words, {code.upper()}={avg_target:.1f} words\")\n",
    "\n",
    "print(\"\\n✅ Dataset creation complete!\")\n",
    "print(\"💡 Usage example:\")\n",
    "print(\"from datasets import load_from_disk\")\n",
    "print(\"dataset = load_from_disk('balanced_mt_dataset')\")\n",
    "print(\"print(dataset['train'][0])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ef6019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 34376/34376 [00:00<00:00, 144270.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3820/3820 [00:00<00:00, 177533.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save train set\n",
    "final_dataset[\"train\"].save_to_disk(\"balanced_mt_dataset/train\")\n",
    "\n",
    "# Save test set\n",
    "final_dataset[\"test\"].save_to_disk(\"balanced_mt_dataset/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865de871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": load_from_disk(\"balanced_mt_dataset/train\"),\n",
    "    \"test\": load_from_disk(\"balanced_mt_dataset/test\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713890c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59aeb5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check what's actually in your Yoruba dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m yo_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_datasets\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 5 Yoruba examples:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Check what's actually in your Yoruba dataset\n",
    "yo_dataset = loaded_datasets[\"yo\"]\n",
    "print(\"First 5 Yoruba examples:\")\n",
    "for i in range(5):\n",
    "    print(f\"EN: {yo_dataset[i]['translation']['en']}\")\n",
    "    print(f\"YO: {yo_dataset[i]['translation']['yo']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Check other languages too\n",
    "for code in ['ar', 'zh', 'hi']:\n",
    "    ds = loaded_datasets[code]\n",
    "    print(f\"\\nFirst example from {code}:\")\n",
    "    print(f\"EN: {ds[0]['translation']['en']}\")\n",
    "    print(f\"{code.upper()}: {ds[0]['translation'][code]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1c4b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ZH | EN: In paragraph 1.50:... | ZH: 3. 在第1.50段:...\n",
      "1: BN | EN: Would you like to send meeting invitations to part... | BN: আপনি কি অংশগ্রহণকারীদেরকে সভার আমন্ত্রনপত্র প্রেরণ...\n",
      "2: RU | EN: As commanding officer, it's my job to interpret th... | RU: Капитан приказал нам вернуться за ним....\n",
      "3: AR | EN: Would you turn back?... | AR: -هل لنا أن نعود؟...\n",
      "4: JA | EN: I promise them to you if you will do me the favor ... | JA: 願いを聞いてもらえるなら 約束する...\n",
      "5: SW | EN: I was no different.... | SW: Sikuwa tofauti...\n",
      "6: ZH | EN: Girl, you could die at 40 from ajax poisoning.... | ZH: 姑娘 你可能40岁就死于清洁剂中毒...\n",
      "7: SW | EN: In two generations, those had produced 3,800 grand... | SW: Ndani ya vizazi viwili, hao walizalisha wajukuu 3,...\n",
      "8: RU | EN: UNIFEM has not devised an adequate tracking system... | RU: Необходимые системы контроля для этого ЮНИФЕМ пока...\n",
      "9: AR | EN: I'm still analyzing this, but for the most part, i... | AR: مازلت أحلل هذا لكن الجزء الأكبر يبدو لي كمعلومات ح...\n"
     ]
    }
   ],
   "source": [
    "# Check first 10 examples to see the variety\n",
    "for i in range(10):\n",
    "    example = final_dataset[\"train\"][i]\n",
    "    lang = example[\"language\"]\n",
    "    en_text = example[\"translation\"][\"en\"]\n",
    "    target_text = example[\"translation\"][lang]\n",
    "    print(f\"{i}: {lang.upper()} | EN: {en_text[:50]}... | {lang.upper()}: {target_text[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97edfc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 34376\n",
      "Test size: 3820\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(final_dataset[\"train\"]))\n",
    "print(\"Test size:\", len(final_dataset[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fa4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'ar': None,\n",
       "  'bn': 'আপনি কি অংশগ্রহণকারীদেরকে সভার আমন্ত্রনপত্র প্রেরণ করতে চান?',\n",
       "  'en': 'Would you like to send meeting invitations to participants?',\n",
       "  'hi': None,\n",
       "  'ja': None,\n",
       "  'ru': None,\n",
       "  'sw': None,\n",
       "  'tr': None,\n",
       "  'yo': None,\n",
       "  'zh': None},\n",
       " 'language': 'bn'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading complete multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "🌍 All languages included: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr']\n",
      "🚀 Starting multilingual training with 9 CUSTOM TOKENIZERS...\n",
      "📊 Training approach: ONE model per tokenizer handling ALL 9 languages\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 1/9\n",
      "🔧 Custom Tokenizer: small_hf_bpe_hf\n",
      "📦 Base Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 15002\n",
      "🔑 Special tokens - BOS: 15000, EOS: 15001, PAD: 0\n",
      "🤖 Loading base multilingual mBART model...\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "🏋️  Starting multilingual training with custom tokenizer...\n",
      "{'loss': 10.3007, 'grad_norm': 5.423027038574219, 'learning_rate': 5.82e-06, 'epoch': 0.05234231876472128}\n",
      "{'loss': 7.8621, 'grad_norm': 7.870815753936768, 'learning_rate': 1.182e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 6.7606, 'grad_norm': 8.400216102600098, 'learning_rate': 1.782e-05, 'epoch': 0.15702695629416383}\n",
      "{'loss': 6.1106, 'grad_norm': 6.761409759521484, 'learning_rate': 2.3820000000000002e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 5.7008, 'grad_norm': 6.1130452156066895, 'learning_rate': 2.982e-05, 'epoch': 0.26171159382360637}\n",
      "{'loss': 5.4413, 'grad_norm': 5.033845901489258, 'learning_rate': 2.9443913625071663e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 5.3028, 'grad_norm': 5.455345153808594, 'learning_rate': 2.8870628702465126e-05, 'epoch': 0.36639623135304894}\n",
      "{'loss': 5.1233, 'grad_norm': 4.724295139312744, 'learning_rate': 2.8297343779858588e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 4.9179, 'grad_norm': 5.004588603973389, 'learning_rate': 2.7724058857252054e-05, 'epoch': 0.4710808688824915}\n",
      "{'loss': 4.9121, 'grad_norm': 4.671501636505127, 'learning_rate': 2.715077393464552e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 4.7404, 'grad_norm': 4.359947204589844, 'learning_rate': 2.6577489012038985e-05, 'epoch': 0.575765506411934}\n",
      "{'loss': 4.7646, 'grad_norm': 5.410670757293701, 'learning_rate': 2.6004204089432447e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 4.6828, 'grad_norm': 4.8040266036987305, 'learning_rate': 2.5430919166825913e-05, 'epoch': 0.6804501439413766}\n",
      "{'loss': 4.5528, 'grad_norm': 5.157586097717285, 'learning_rate': 2.485763424421938e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 4.6245, 'grad_norm': 4.585670471191406, 'learning_rate': 2.4284349321612845e-05, 'epoch': 0.7851347814708192}\n",
      "{'loss': 4.5572, 'grad_norm': 4.698575019836426, 'learning_rate': 2.3711064399006307e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 4.4949, 'grad_norm': 5.703697681427002, 'learning_rate': 2.313777947639977e-05, 'epoch': 0.8898194190002617}\n",
      "{'loss': 4.4167, 'grad_norm': 5.356400012969971, 'learning_rate': 2.2564494553793235e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 4.3514, 'grad_norm': 4.925133228302002, 'learning_rate': 2.19912096311867e-05, 'epoch': 0.9945040565297043}\n",
      "{'eval_loss': 4.2966837882995605, 'eval_bleu': 0.00012803489343441582, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 12.23515625, 'eval_avg_label_length': 23.015652687536917, 'eval_empty_predictions': 0.6219728292971057, 'eval_runtime': 284.6663, 'eval_samples_per_second': 11.895, 'eval_steps_per_second': 2.975, 'epoch': 1.0}\n",
      "{'loss': 4.2525, 'grad_norm': 4.547128200531006, 'learning_rate': 2.1417924708580163e-05, 'epoch': 1.046584663700602}\n",
      "{'loss': 4.2162, 'grad_norm': 5.355544567108154, 'learning_rate': 2.084463978597363e-05, 'epoch': 1.0989269824653232}\n",
      "{'loss': 4.1167, 'grad_norm': 5.805546283721924, 'learning_rate': 2.0271354863367095e-05, 'epoch': 1.1512693012300446}\n",
      "{'loss': 4.1979, 'grad_norm': 5.222780227661133, 'learning_rate': 1.969806994076056e-05, 'epoch': 1.2036116199947657}\n",
      "{'loss': 4.1231, 'grad_norm': 4.820038795471191, 'learning_rate': 1.9124785018154023e-05, 'epoch': 1.255953938759487}\n",
      "{'loss': 4.1349, 'grad_norm': 5.643107891082764, 'learning_rate': 1.855150009554749e-05, 'epoch': 1.3082962575242083}\n",
      "{'loss': 4.123, 'grad_norm': 4.817971706390381, 'learning_rate': 1.797821517294095e-05, 'epoch': 1.3606385762889297}\n",
      "{'loss': 3.9832, 'grad_norm': 5.000853061676025, 'learning_rate': 1.7404930250334416e-05, 'epoch': 1.4129808950536509}\n",
      "{'loss': 4.0027, 'grad_norm': 5.804320335388184, 'learning_rate': 1.683164532772788e-05, 'epoch': 1.465323213818372}\n",
      "{'loss': 3.9565, 'grad_norm': 5.052690505981445, 'learning_rate': 1.6258360405121345e-05, 'epoch': 1.5176655325830934}\n",
      "{'loss': 3.9459, 'grad_norm': 5.840681076049805, 'learning_rate': 1.568507548251481e-05, 'epoch': 1.5700078513478148}\n",
      "{'loss': 3.946, 'grad_norm': 4.861568927764893, 'learning_rate': 1.5111790559908274e-05, 'epoch': 1.622350170112536}\n",
      "{'loss': 4.0032, 'grad_norm': 4.631413459777832, 'learning_rate': 1.453850563730174e-05, 'epoch': 1.6746924888772572}\n",
      "{'loss': 3.8673, 'grad_norm': 4.24465274810791, 'learning_rate': 1.3965220714695202e-05, 'epoch': 1.7270348076419786}\n",
      "{'loss': 3.9441, 'grad_norm': 5.199272155761719, 'learning_rate': 1.3391935792088668e-05, 'epoch': 1.7793771264067}\n",
      "{'loss': 3.9344, 'grad_norm': 5.6135358810424805, 'learning_rate': 1.2818650869482134e-05, 'epoch': 1.8317194451714212}\n",
      "{'loss': 3.9372, 'grad_norm': 6.282603740692139, 'learning_rate': 1.2245365946875598e-05, 'epoch': 1.8840617639361423}\n",
      "{'loss': 3.8717, 'grad_norm': 5.3967061042785645, 'learning_rate': 1.1672081024269062e-05, 'epoch': 1.9364040827008635}\n",
      "{'loss': 3.8682, 'grad_norm': 5.971645832061768, 'learning_rate': 1.1098796101662526e-05, 'epoch': 1.988746401465585}\n",
      "{'eval_loss': 3.845402956008911, 'eval_bleu': 0.00010863251485219205, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 10.555217831813577, 'eval_avg_label_length': 23.015652687536917, 'eval_empty_predictions': 0.7085056113408151, 'eval_runtime': 191.7896, 'eval_samples_per_second': 17.655, 'eval_steps_per_second': 4.416, 'epoch': 2.0}\n",
      "{'loss': 3.6819, 'grad_norm': 5.266168594360352, 'learning_rate': 1.0525511179055992e-05, 'epoch': 2.040827008636483}\n",
      "{'loss': 3.6708, 'grad_norm': 4.751755237579346, 'learning_rate': 9.952226256449456e-06, 'epoch': 2.093169327401204}\n",
      "{'loss': 3.7353, 'grad_norm': 5.011890888214111, 'learning_rate': 9.378941333842921e-06, 'epoch': 2.145511646165925}\n",
      "{'loss': 3.7778, 'grad_norm': 6.022607803344727, 'learning_rate': 8.805656411236384e-06, 'epoch': 2.1978539649306463}\n",
      "{'loss': 3.6071, 'grad_norm': 5.359704494476318, 'learning_rate': 8.23237148862985e-06, 'epoch': 2.2501962836953675}\n",
      "{'loss': 3.6679, 'grad_norm': 4.823181629180908, 'learning_rate': 7.659086566023314e-06, 'epoch': 2.302538602460089}\n",
      "{'loss': 3.6312, 'grad_norm': 5.662222862243652, 'learning_rate': 7.085801643416778e-06, 'epoch': 2.3548809212248103}\n",
      "{'loss': 3.6208, 'grad_norm': 5.735134601593018, 'learning_rate': 6.5125167208102425e-06, 'epoch': 2.4072232399895315}\n",
      "{'loss': 3.6809, 'grad_norm': 4.9918999671936035, 'learning_rate': 5.939231798203707e-06, 'epoch': 2.4595655587542526}\n",
      "{'loss': 3.6037, 'grad_norm': 5.402812957763672, 'learning_rate': 5.365946875597172e-06, 'epoch': 2.511907877518974}\n",
      "{'loss': 3.6437, 'grad_norm': 5.543225288391113, 'learning_rate': 4.792661952990637e-06, 'epoch': 2.5642501962836954}\n",
      "{'loss': 3.5789, 'grad_norm': 6.154842853546143, 'learning_rate': 4.219377030384101e-06, 'epoch': 2.6165925150484166}\n",
      "{'loss': 3.6336, 'grad_norm': 7.417726039886475, 'learning_rate': 3.646092107777565e-06, 'epoch': 2.668934833813138}\n",
      "{'loss': 3.5684, 'grad_norm': 6.220751762390137, 'learning_rate': 3.0728071851710297e-06, 'epoch': 2.7212771525778594}\n",
      "{'loss': 3.5995, 'grad_norm': 5.783827304840088, 'learning_rate': 2.499522262564495e-06, 'epoch': 2.7736194713425806}\n",
      "{'loss': 3.6458, 'grad_norm': 5.721475124359131, 'learning_rate': 1.9262373399579594e-06, 'epoch': 2.8259617901073018}\n",
      "{'loss': 3.6302, 'grad_norm': 6.731168746948242, 'learning_rate': 1.3529524173514237e-06, 'epoch': 2.878304108872023}\n",
      "{'loss': 3.6819, 'grad_norm': 5.290224075317383, 'learning_rate': 7.796674947448882e-07, 'epoch': 2.930646427636744}\n",
      "{'loss': 3.6158, 'grad_norm': 5.0746917724609375, 'learning_rate': 2.0638257213835276e-07, 'epoch': 2.9829887464014657}\n",
      "{'eval_loss': 3.708686590194702, 'eval_bleu': 0.00011270316607821121, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 22.17810945273632, 'eval_avg_label_length': 23.015652687536917, 'eval_empty_predictions': 0.7031896042528056, 'eval_runtime': 351.9597, 'eval_samples_per_second': 9.62, 'eval_steps_per_second': 2.407, 'epoch': 3.0}\n",
      "{'train_runtime': 2556.6551, 'train_samples_per_second': 35.866, 'train_steps_per_second': 2.242, 'train_loss': 4.367945000531888, 'epoch': 3.0}\n",
      "📊 Final multilingual evaluation...\n",
      "{'eval_loss': 4.2966837882995605, 'eval_bleu': 0.00012803489343441582, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 12.23515625, 'eval_avg_label_length': 23.015652687536917, 'eval_empty_predictions': 0.6219728292971057, 'eval_runtime': 277.9507, 'eval_samples_per_second': 12.182, 'eval_steps_per_second': 3.047, 'epoch': 3.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_small_hf_bpe_hf\n",
      "📈 Overall BLEU Score: 0.0001\n",
      "🎯 Overall Exact Match: 0.0000\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): \n",
      "  zh (Chinese): \n",
      "  hi (Hindi): \n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 2/9\n",
      "🔧 Custom Tokenizer: small_hf_wordpiece_hf\n",
      "📦 Base Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/hf_wordpiece_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 23634\n",
      "🔑 Special tokens - BOS: 23632, EOS: 23633, PAD: 0\n",
      "🤖 Loading base multilingual mBART model...\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "🏋️  Starting multilingual training with custom tokenizer...\n",
      "{'loss': 10.3476, 'grad_norm': 11.022256851196289, 'learning_rate': 5.8800000000000005e-06, 'epoch': 0.05234231876472128}\n",
      "{'loss': 6.4525, 'grad_norm': 10.119338035583496, 'learning_rate': 1.1880000000000001e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 5.0065, 'grad_norm': 8.592623710632324, 'learning_rate': 1.7879999999999998e-05, 'epoch': 0.15702695629416383}\n",
      "{'loss': 4.527, 'grad_norm': 7.473294734954834, 'learning_rate': 2.3880000000000002e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 4.1418, 'grad_norm': 7.966894626617432, 'learning_rate': 2.9880000000000002e-05, 'epoch': 0.26171159382360637}\n",
      "{'loss': 3.882, 'grad_norm': 5.52272367477417, 'learning_rate': 2.9438180775845595e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 3.7626, 'grad_norm': 6.3380866050720215, 'learning_rate': 2.886489585323906e-05, 'epoch': 0.36639623135304894}\n",
      "{'loss': 3.573, 'grad_norm': 4.707721710205078, 'learning_rate': 2.8291610930632527e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 3.4365, 'grad_norm': 4.932058811187744, 'learning_rate': 2.771832600802599e-05, 'epoch': 0.4710808688824915}\n",
      "{'loss': 3.3746, 'grad_norm': 4.441863059997559, 'learning_rate': 2.7145041085419455e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 3.2335, 'grad_norm': 3.978405475616455, 'learning_rate': 2.657175616281292e-05, 'epoch': 0.575765506411934}\n",
      "{'loss': 3.2441, 'grad_norm': 4.030572891235352, 'learning_rate': 2.5998471240206383e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 3.1674, 'grad_norm': 5.014392852783203, 'learning_rate': 2.5425186317599845e-05, 'epoch': 0.6804501439413766}\n",
      "{'loss': 3.0729, 'grad_norm': 4.023387432098389, 'learning_rate': 2.485190139499331e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 3.0813, 'grad_norm': 4.092008590698242, 'learning_rate': 2.4278616472386777e-05, 'epoch': 0.7851347814708192}\n",
      "{'loss': 3.0896, 'grad_norm': 5.011559963226318, 'learning_rate': 2.3705331549780243e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 3.04, 'grad_norm': 4.357403755187988, 'learning_rate': 2.3132046627173705e-05, 'epoch': 0.8898194190002617}\n",
      "{'loss': 2.9564, 'grad_norm': 4.174289226531982, 'learning_rate': 2.255876170456717e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 2.9472, 'grad_norm': 4.1465301513671875, 'learning_rate': 2.1985476781960636e-05, 'epoch': 0.9945040565297043}\n",
      "{'eval_loss': 2.8741586208343506, 'eval_bleu': 0.04741015352446313, 'eval_exact_match': 0.00029533372711163615, 'eval_avg_pred_length': 43.916692284045496, 'eval_avg_label_length': 35.69453471196455, 'eval_empty_predictions': 0.03927938570584761, 'eval_runtime': 1092.5978, 'eval_samples_per_second': 3.099, 'eval_steps_per_second': 0.775, 'epoch': 1.0}\n",
      "{'loss': 2.854, 'grad_norm': 3.6011250019073486, 'learning_rate': 2.1412191859354102e-05, 'epoch': 1.046584663700602}\n",
      "{'loss': 2.8143, 'grad_norm': 5.230954647064209, 'learning_rate': 2.083890693674756e-05, 'epoch': 1.0989269824653232}\n",
      "{'loss': 2.7357, 'grad_norm': 4.8607177734375, 'learning_rate': 2.0265622014141027e-05, 'epoch': 1.1512693012300446}\n",
      "{'loss': 2.8201, 'grad_norm': 3.9420244693756104, 'learning_rate': 1.9692337091534492e-05, 'epoch': 1.2036116199947657}\n",
      "{'loss': 2.8412, 'grad_norm': 4.38055944442749, 'learning_rate': 1.914198356583222e-05, 'epoch': 1.255953938759487}\n",
      "{'loss': 2.7795, 'grad_norm': 5.282721519470215, 'learning_rate': 1.8568698643225682e-05, 'epoch': 1.3082962575242083}\n",
      "{'loss': 2.7426, 'grad_norm': 3.6589958667755127, 'learning_rate': 1.7995413720619148e-05, 'epoch': 1.3606385762889297}\n",
      "{'loss': 2.6509, 'grad_norm': 3.9300129413604736, 'learning_rate': 1.7422128798012613e-05, 'epoch': 1.4129808950536509}\n",
      "{'loss': 2.6799, 'grad_norm': 4.576044082641602, 'learning_rate': 1.6848843875406076e-05, 'epoch': 1.465323213818372}\n",
      "{'loss': 2.6626, 'grad_norm': 3.7480568885803223, 'learning_rate': 1.627555895279954e-05, 'epoch': 1.5176655325830934}\n",
      "{'loss': 2.6495, 'grad_norm': 4.463987350463867, 'learning_rate': 1.5702274030193007e-05, 'epoch': 1.5700078513478148}\n",
      "{'loss': 2.6044, 'grad_norm': 3.962099313735962, 'learning_rate': 1.5128989107586471e-05, 'epoch': 1.622350170112536}\n",
      "{'loss': 2.6647, 'grad_norm': 3.7463085651397705, 'learning_rate': 1.4555704184979935e-05, 'epoch': 1.6746924888772572}\n",
      "{'loss': 2.5189, 'grad_norm': 3.7053892612457275, 'learning_rate': 1.39824192623734e-05, 'epoch': 1.7270348076419786}\n",
      "{'loss': 2.5931, 'grad_norm': 4.154898166656494, 'learning_rate': 1.3409134339766865e-05, 'epoch': 1.7793771264067}\n",
      "{'loss': 2.6073, 'grad_norm': 4.758492946624756, 'learning_rate': 1.2835849417160329e-05, 'epoch': 1.8317194451714212}\n",
      "{'loss': 2.6098, 'grad_norm': 4.534704208374023, 'learning_rate': 1.2262564494553793e-05, 'epoch': 1.8840617639361423}\n",
      "{'loss': 2.5588, 'grad_norm': 4.728023529052734, 'learning_rate': 1.1689279571947257e-05, 'epoch': 1.9364040827008635}\n",
      "{'loss': 2.547, 'grad_norm': 4.238070964813232, 'learning_rate': 1.1115994649340723e-05, 'epoch': 1.988746401465585}\n",
      "{'eval_loss': 2.530738353729248, 'eval_bleu': 0.10524775165009073, 'eval_exact_match': 0.03573538098050797, 'eval_avg_pred_length': 36.02991706161137, 'eval_avg_label_length': 35.69453471196455, 'eval_empty_predictions': 0.0029533372711163615, 'eval_runtime': 916.8962, 'eval_samples_per_second': 3.693, 'eval_steps_per_second': 0.924, 'epoch': 2.0}\n",
      "{'loss': 2.4262, 'grad_norm': 4.271914958953857, 'learning_rate': 1.0542709726734188e-05, 'epoch': 2.040827008636483}\n",
      "{'loss': 2.4389, 'grad_norm': 3.758467674255371, 'learning_rate': 9.96942480412765e-06, 'epoch': 2.093169327401204}\n",
      "{'loss': 2.4634, 'grad_norm': 3.8336684703826904, 'learning_rate': 9.396139881521117e-06, 'epoch': 2.145511646165925}\n",
      "{'loss': 2.4997, 'grad_norm': 4.989747047424316, 'learning_rate': 8.82285495891458e-06, 'epoch': 2.1978539649306463}\n",
      "{'loss': 2.3752, 'grad_norm': 4.039297103881836, 'learning_rate': 8.249570036308046e-06, 'epoch': 2.2501962836953675}\n",
      "{'loss': 2.4165, 'grad_norm': 3.4642865657806396, 'learning_rate': 7.67628511370151e-06, 'epoch': 2.302538602460089}\n",
      "{'loss': 2.4234, 'grad_norm': 4.094995498657227, 'learning_rate': 7.103000191094974e-06, 'epoch': 2.3548809212248103}\n",
      "{'loss': 2.3926, 'grad_norm': 4.112281799316406, 'learning_rate': 6.529715268488438e-06, 'epoch': 2.4072232399895315}\n",
      "{'loss': 2.4337, 'grad_norm': 3.914252758026123, 'learning_rate': 5.956430345881903e-06, 'epoch': 2.4595655587542526}\n",
      "{'loss': 2.4106, 'grad_norm': 4.2762532234191895, 'learning_rate': 5.383145423275367e-06, 'epoch': 2.511907877518974}\n",
      "{'loss': 2.4085, 'grad_norm': 4.062292098999023, 'learning_rate': 4.809860500668832e-06, 'epoch': 2.5642501962836954}\n",
      "{'loss': 2.3878, 'grad_norm': 5.214261054992676, 'learning_rate': 4.236575578062296e-06, 'epoch': 2.6165925150484166}\n",
      "{'loss': 2.3742, 'grad_norm': 4.94301176071167, 'learning_rate': 3.6632906554557616e-06, 'epoch': 2.668934833813138}\n",
      "{'loss': 2.3539, 'grad_norm': 4.627719879150391, 'learning_rate': 3.090005732849226e-06, 'epoch': 2.7212771525778594}\n",
      "{'loss': 2.3697, 'grad_norm': 4.158807754516602, 'learning_rate': 2.5167208102426905e-06, 'epoch': 2.7736194713425806}\n",
      "{'loss': 2.4179, 'grad_norm': 4.526008605957031, 'learning_rate': 1.943435887636155e-06, 'epoch': 2.8259617901073018}\n",
      "{'loss': 2.3588, 'grad_norm': 4.595973491668701, 'learning_rate': 1.3701509650296198e-06, 'epoch': 2.878304108872023}\n",
      "{'loss': 2.4765, 'grad_norm': 4.1766743659973145, 'learning_rate': 7.968660424230843e-07, 'epoch': 2.930646427636744}\n",
      "{'loss': 2.4195, 'grad_norm': 4.110343933105469, 'learning_rate': 2.2358111981654884e-07, 'epoch': 2.9829887464014657}\n",
      "{'eval_loss': 2.4207489490509033, 'eval_bleu': 0.12608148997697943, 'eval_exact_match': 0.0490253987005316, 'eval_avg_pred_length': 36.73592175459395, 'eval_avg_label_length': 35.69453471196455, 'eval_empty_predictions': 0.0035440047253396337, 'eval_runtime': 945.8669, 'eval_samples_per_second': 3.58, 'eval_steps_per_second': 0.895, 'epoch': 3.0}\n",
      "{'train_runtime': 4643.3796, 'train_samples_per_second': 19.748, 'train_steps_per_second': 1.235, 'train_loss': 3.0322416403022046, 'epoch': 3.0}\n",
      "📊 Final multilingual evaluation...\n",
      "{'eval_loss': 2.4207489490509033, 'eval_bleu': 0.12608148997697943, 'eval_exact_match': 0.0490253987005316, 'eval_avg_pred_length': 36.73592175459395, 'eval_avg_label_length': 35.69453471196455, 'eval_empty_predictions': 0.0035440047253396337, 'eval_runtime': 945.4278, 'eval_samples_per_second': 3.581, 'eval_steps_per_second': 0.896, 'epoch': 3.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_small_hf_wordpiece_hf\n",
      "📈 Overall BLEU Score: 0.1261\n",
      "🎯 Overall Exact Match: 0.0490\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): ه ##ل أ ##ن ##ا أ ##ن ##ا م ##ا ##ذ ##ا ؟\n",
      "  zh (Chinese): 你 ##知 ##道 ##吗 ?\n",
      "  hi (Hindi): क ##् ##य ##ा , त ##ु ##म ##् ##ह ##ा ##र ##े क ##् ##य ##ा क ##र र ##ह ##े ह ##ो ?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 3/9\n",
      "🔧 Custom Tokenizer: small_sp_unigram_hf\n",
      "📦 Base Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/sp_unigram_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 15002\n",
      "🔑 Special tokens - BOS: 1, EOS: 2, PAD: 15000\n",
      "🤖 Loading base multilingual mBART model...\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "🏋️  Starting multilingual training with custom tokenizer...\n",
      "{'loss': 8.6782, 'grad_norm': 5.561738014221191, 'learning_rate': 5.8800000000000005e-06, 'epoch': 0.05234231876472128}\n",
      "{'loss': 7.238, 'grad_norm': 5.764930725097656, 'learning_rate': 1.1880000000000001e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 6.5943, 'grad_norm': 4.941795349121094, 'learning_rate': 1.7879999999999998e-05, 'epoch': 0.15702695629416383}\n",
      "{'loss': 6.0114, 'grad_norm': 5.194766044616699, 'learning_rate': 2.3880000000000002e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 5.5585, 'grad_norm': 5.28207540512085, 'learning_rate': 2.9880000000000002e-05, 'epoch': 0.26171159382360637}\n",
      "{'loss': 5.183, 'grad_norm': 4.232866287231445, 'learning_rate': 2.9438180775845595e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 5.0675, 'grad_norm': 5.403024196624756, 'learning_rate': 2.886489585323906e-05, 'epoch': 0.36639623135304894}\n",
      "{'loss': 4.8244, 'grad_norm': 4.903186321258545, 'learning_rate': 2.8291610930632527e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 4.633, 'grad_norm': 5.349541664123535, 'learning_rate': 2.771832600802599e-05, 'epoch': 0.4710808688824915}\n",
      "{'loss': 4.5906, 'grad_norm': 4.692155361175537, 'learning_rate': 2.7145041085419455e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 4.4319, 'grad_norm': 4.362256050109863, 'learning_rate': 2.657175616281292e-05, 'epoch': 0.575765506411934}\n",
      "{'loss': 4.4528, 'grad_norm': 5.766404151916504, 'learning_rate': 2.5998471240206383e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 4.3384, 'grad_norm': 5.011628150939941, 'learning_rate': 2.5425186317599845e-05, 'epoch': 0.6804501439413766}\n",
      "{'loss': 4.2427, 'grad_norm': 5.088468551635742, 'learning_rate': 2.485190139499331e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 4.2923, 'grad_norm': 5.506152153015137, 'learning_rate': 2.4278616472386777e-05, 'epoch': 0.7851347814708192}\n",
      "{'loss': 4.1914, 'grad_norm': 4.701361656188965, 'learning_rate': 2.3705331549780243e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 4.1442, 'grad_norm': 5.90431547164917, 'learning_rate': 2.3132046627173705e-05, 'epoch': 0.8898194190002617}\n",
      "{'loss': 4.0626, 'grad_norm': 5.276830196380615, 'learning_rate': 2.255876170456717e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 3.9966, 'grad_norm': 4.427673816680908, 'learning_rate': 2.1985476781960636e-05, 'epoch': 0.9945040565297043}\n",
      "{'eval_loss': 3.929722547531128, 'eval_bleu': 1.881276675920001e-06, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 14.333333333333334, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.9982279976373302, 'eval_runtime': 55.2542, 'eval_samples_per_second': 61.28, 'eval_steps_per_second': 15.329, 'epoch': 1.0}\n",
      "{'loss': 3.8666, 'grad_norm': 4.840915679931641, 'learning_rate': 2.1412191859354102e-05, 'epoch': 1.046584663700602}\n",
      "{'loss': 3.8349, 'grad_norm': 5.685991287231445, 'learning_rate': 2.083890693674756e-05, 'epoch': 1.0989269824653232}\n",
      "{'loss': 3.7428, 'grad_norm': 5.748795032501221, 'learning_rate': 2.0265622014141027e-05, 'epoch': 1.1512693012300446}\n",
      "{'loss': 3.8075, 'grad_norm': 5.287862777709961, 'learning_rate': 1.9692337091534492e-05, 'epoch': 1.2036116199947657}\n",
      "{'loss': 3.7493, 'grad_norm': 5.698851108551025, 'learning_rate': 1.9119052168927958e-05, 'epoch': 1.255953938759487}\n",
      "{'loss': 3.7504, 'grad_norm': 5.432952880859375, 'learning_rate': 1.8545767246321424e-05, 'epoch': 1.3082962575242083}\n",
      "{'loss': 3.7503, 'grad_norm': 5.191469192504883, 'learning_rate': 1.7972482323714886e-05, 'epoch': 1.3606385762889297}\n",
      "{'loss': 3.6232, 'grad_norm': 5.239195346832275, 'learning_rate': 1.7399197401108352e-05, 'epoch': 1.4129808950536509}\n",
      "{'loss': 3.6451, 'grad_norm': 5.9778971672058105, 'learning_rate': 1.6825912478501818e-05, 'epoch': 1.465323213818372}\n",
      "{'loss': 3.6071, 'grad_norm': 6.048638820648193, 'learning_rate': 1.6252627555895283e-05, 'epoch': 1.5176655325830934}\n",
      "{'loss': 3.6051, 'grad_norm': 5.705920219421387, 'learning_rate': 1.5679342633288742e-05, 'epoch': 1.5700078513478148}\n",
      "{'loss': 3.6088, 'grad_norm': 4.93747615814209, 'learning_rate': 1.5106057710682208e-05, 'epoch': 1.622350170112536}\n",
      "{'loss': 3.6489, 'grad_norm': 4.771890163421631, 'learning_rate': 1.4532772788075674e-05, 'epoch': 1.6746924888772572}\n",
      "{'loss': 3.5205, 'grad_norm': 4.647773742675781, 'learning_rate': 1.3959487865469138e-05, 'epoch': 1.7270348076419786}\n",
      "{'loss': 3.5911, 'grad_norm': 5.082062244415283, 'learning_rate': 1.3386202942862604e-05, 'epoch': 1.7793771264067}\n",
      "{'loss': 3.5788, 'grad_norm': 5.606661319732666, 'learning_rate': 1.2812918020256068e-05, 'epoch': 1.8317194451714212}\n",
      "{'loss': 3.5617, 'grad_norm': 5.7446370124816895, 'learning_rate': 1.2239633097649532e-05, 'epoch': 1.8840617639361423}\n",
      "{'loss': 3.5282, 'grad_norm': 5.105860233306885, 'learning_rate': 1.1666348175042996e-05, 'epoch': 1.9364040827008635}\n",
      "{'loss': 3.5116, 'grad_norm': 5.657111644744873, 'learning_rate': 1.1093063252436462e-05, 'epoch': 1.988746401465585}\n",
      "{'eval_loss': 3.5010273456573486, 'eval_bleu': 0.0, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 1.0, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.9997046662728883, 'eval_runtime': 55.8408, 'eval_samples_per_second': 60.637, 'eval_steps_per_second': 15.168, 'epoch': 2.0}\n",
      "{'loss': 3.3537, 'grad_norm': 5.237386226654053, 'learning_rate': 1.0519778329829926e-05, 'epoch': 2.040827008636483}\n",
      "{'loss': 3.3147, 'grad_norm': 4.965818405151367, 'learning_rate': 9.946493407223391e-06, 'epoch': 2.093169327401204}\n",
      "{'loss': 3.3685, 'grad_norm': 5.0875678062438965, 'learning_rate': 9.373208484616854e-06, 'epoch': 2.145511646165925}\n",
      "{'loss': 3.4081, 'grad_norm': 5.900079250335693, 'learning_rate': 8.79992356201032e-06, 'epoch': 2.1978539649306463}\n",
      "{'loss': 3.2792, 'grad_norm': 5.447059154510498, 'learning_rate': 8.226638639403783e-06, 'epoch': 2.2501962836953675}\n",
      "{'loss': 3.309, 'grad_norm': 5.048880100250244, 'learning_rate': 7.653353716797249e-06, 'epoch': 2.302538602460089}\n",
      "{'loss': 3.2789, 'grad_norm': 5.822857856750488, 'learning_rate': 7.080068794190713e-06, 'epoch': 2.3548809212248103}\n",
      "{'loss': 3.2922, 'grad_norm': 5.535046577453613, 'learning_rate': 6.506783871584177e-06, 'epoch': 2.4072232399895315}\n",
      "{'loss': 3.3099, 'grad_norm': 4.713966369628906, 'learning_rate': 5.939231798203707e-06, 'epoch': 2.4595655587542526}\n",
      "{'loss': 3.2546, 'grad_norm': 5.277884006500244, 'learning_rate': 5.365946875597172e-06, 'epoch': 2.511907877518974}\n",
      "{'loss': 3.2903, 'grad_norm': 5.2463531494140625, 'learning_rate': 4.792661952990637e-06, 'epoch': 2.5642501962836954}\n",
      "{'loss': 3.2371, 'grad_norm': 6.324517726898193, 'learning_rate': 4.219377030384101e-06, 'epoch': 2.6165925150484166}\n",
      "{'loss': 3.276, 'grad_norm': 6.5360846519470215, 'learning_rate': 3.646092107777565e-06, 'epoch': 2.668934833813138}\n",
      "{'loss': 3.2365, 'grad_norm': 5.4790358543396, 'learning_rate': 3.0728071851710297e-06, 'epoch': 2.7212771525778594}\n",
      "{'loss': 3.283, 'grad_norm': 5.677194595336914, 'learning_rate': 2.499522262564495e-06, 'epoch': 2.7736194713425806}\n",
      "{'loss': 3.2711, 'grad_norm': 5.824473857879639, 'learning_rate': 1.9262373399579594e-06, 'epoch': 2.8259617901073018}\n",
      "{'loss': 3.2628, 'grad_norm': 5.592801094055176, 'learning_rate': 1.3529524173514237e-06, 'epoch': 2.878304108872023}\n",
      "{'loss': 3.3164, 'grad_norm': 5.345499515533447, 'learning_rate': 7.796674947448882e-07, 'epoch': 2.930646427636744}\n",
      "{'loss': 3.2712, 'grad_norm': 5.057862758636475, 'learning_rate': 2.0638257213835276e-07, 'epoch': 2.9829887464014657}\n",
      "{'eval_loss': 3.3801627159118652, 'eval_bleu': 0.0, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 9.857142857142858, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.9793266391021854, 'eval_runtime': 175.9644, 'eval_samples_per_second': 19.243, 'eval_steps_per_second': 4.813, 'epoch': 3.0}\n",
      "{'train_runtime': 1965.1396, 'train_samples_per_second': 46.662, 'train_steps_per_second': 2.917, 'train_loss': 4.006351341195658, 'epoch': 3.0}\n",
      "📊 Final multilingual evaluation...\n",
      "{'eval_loss': 3.929722547531128, 'eval_bleu': 1.881276675920001e-06, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 14.333333333333334, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.9982279976373302, 'eval_runtime': 54.9673, 'eval_samples_per_second': 61.6, 'eval_steps_per_second': 15.409, 'epoch': 3.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_small_sp_unigram_hf\n",
      "📈 Overall BLEU Score: 0.0000\n",
      "🎯 Overall Exact Match: 0.0000\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): \n",
      "  zh (Chinese): \n",
      "  hi (Hindi): \n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 4/9\n",
      "🔧 Custom Tokenizer: medium_hf_bpe_hf\n",
      "📦 Base Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 30002\n",
      "🔑 Special tokens - BOS: 30000, EOS: 30001, PAD: 0\n",
      "🤖 Loading base multilingual mBART model...\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "🏋️  Starting multilingual training with custom tokenizer...\n",
      "{'loss': 11.0148, 'grad_norm': 5.467469692230225, 'learning_rate': 5.8800000000000005e-06, 'epoch': 0.05234231876472128}\n",
      "{'loss': 8.7865, 'grad_norm': 9.097526550292969, 'learning_rate': 1.1880000000000001e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 7.8063, 'grad_norm': 6.029853820800781, 'learning_rate': 1.7879999999999998e-05, 'epoch': 0.15702695629416383}\n",
      "{'loss': 7.324, 'grad_norm': 6.6974568367004395, 'learning_rate': 2.3880000000000002e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 6.9265, 'grad_norm': 6.591342926025391, 'learning_rate': 2.9880000000000002e-05, 'epoch': 0.26171159382360637}\n",
      "{'loss': 6.5524, 'grad_norm': 5.206162929534912, 'learning_rate': 2.9438180775845595e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 6.3942, 'grad_norm': 5.940801620483398, 'learning_rate': 2.886489585323906e-05, 'epoch': 0.36639623135304894}\n",
      "{'loss': 6.148, 'grad_norm': 5.178916931152344, 'learning_rate': 2.8291610930632527e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 5.8328, 'grad_norm': 5.171658515930176, 'learning_rate': 2.771832600802599e-05, 'epoch': 0.4710808688824915}\n",
      "{'loss': 5.8742, 'grad_norm': 5.052799224853516, 'learning_rate': 2.7145041085419455e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 5.6764, 'grad_norm': 4.1873602867126465, 'learning_rate': 2.657175616281292e-05, 'epoch': 0.575765506411934}\n",
      "{'loss': 5.6741, 'grad_norm': 5.738368511199951, 'learning_rate': 2.5998471240206383e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 5.5554, 'grad_norm': 5.386896133422852, 'learning_rate': 2.5425186317599845e-05, 'epoch': 0.6804501439413766}\n",
      "{'loss': 5.4283, 'grad_norm': 5.559201240539551, 'learning_rate': 2.485190139499331e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 5.5131, 'grad_norm': 4.933656692504883, 'learning_rate': 2.4278616472386777e-05, 'epoch': 0.7851347814708192}\n",
      "{'loss': 5.4455, 'grad_norm': 4.673617362976074, 'learning_rate': 2.3705331549780243e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 5.3527, 'grad_norm': 5.9293293952941895, 'learning_rate': 2.3132046627173705e-05, 'epoch': 0.8898194190002617}\n",
      "{'loss': 5.296, 'grad_norm': 5.5979790687561035, 'learning_rate': 2.255876170456717e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 5.1877, 'grad_norm': 5.1220574378967285, 'learning_rate': 2.1985476781960636e-05, 'epoch': 0.9945040565297043}\n",
      "{'eval_loss': 5.196696758270264, 'eval_bleu': 0.0018840141865606739, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 8.499762695775985, 'eval_avg_label_length': 18.499704666272887, 'eval_empty_predictions': 0.3777318369757826, 'eval_runtime': 287.2372, 'eval_samples_per_second': 11.788, 'eval_steps_per_second': 2.949, 'epoch': 1.0}\n",
      "{'loss': 5.0852, 'grad_norm': 4.929686069488525, 'learning_rate': 2.1412191859354102e-05, 'epoch': 1.046584663700602}\n",
      "{'loss': 5.0513, 'grad_norm': 6.173146724700928, 'learning_rate': 2.083890693674756e-05, 'epoch': 1.0989269824653232}\n",
      "{'loss': 4.9333, 'grad_norm': 5.9203267097473145, 'learning_rate': 2.0265622014141027e-05, 'epoch': 1.1512693012300446}\n",
      "{'loss': 5.0374, 'grad_norm': 5.6419525146484375, 'learning_rate': 1.9692337091534492e-05, 'epoch': 1.2036116199947657}\n",
      "{'loss': 4.9613, 'grad_norm': 5.550527095794678, 'learning_rate': 1.9119052168927958e-05, 'epoch': 1.255953938759487}\n",
      "{'loss': 4.9554, 'grad_norm': 6.1974945068359375, 'learning_rate': 1.8545767246321424e-05, 'epoch': 1.3082962575242083}\n",
      "{'loss': 4.9765, 'grad_norm': 4.902836322784424, 'learning_rate': 1.7972482323714886e-05, 'epoch': 1.3606385762889297}\n",
      "{'loss': 4.8171, 'grad_norm': 5.074525356292725, 'learning_rate': 1.7399197401108352e-05, 'epoch': 1.4129808950536509}\n",
      "{'loss': 4.8567, 'grad_norm': 6.376983165740967, 'learning_rate': 1.6825912478501818e-05, 'epoch': 1.465323213818372}\n",
      "{'loss': 4.7662, 'grad_norm': 5.519491672515869, 'learning_rate': 1.6252627555895283e-05, 'epoch': 1.5176655325830934}\n",
      "{'loss': 4.8066, 'grad_norm': 6.1509528160095215, 'learning_rate': 1.5679342633288742e-05, 'epoch': 1.5700078513478148}\n",
      "{'loss': 4.7943, 'grad_norm': 5.0787248611450195, 'learning_rate': 1.5106057710682208e-05, 'epoch': 1.622350170112536}\n",
      "{'loss': 4.8428, 'grad_norm': 4.863827228546143, 'learning_rate': 1.4532772788075674e-05, 'epoch': 1.6746924888772572}\n",
      "{'loss': 4.7175, 'grad_norm': 4.4008612632751465, 'learning_rate': 1.3959487865469138e-05, 'epoch': 1.7270348076419786}\n",
      "{'loss': 4.7806, 'grad_norm': 5.497349262237549, 'learning_rate': 1.3386202942862604e-05, 'epoch': 1.7793771264067}\n",
      "{'loss': 4.786, 'grad_norm': 5.524230480194092, 'learning_rate': 1.2812918020256068e-05, 'epoch': 1.8317194451714212}\n",
      "{'loss': 4.8206, 'grad_norm': 5.739416599273682, 'learning_rate': 1.2239633097649532e-05, 'epoch': 1.8840617639361423}\n",
      "{'loss': 4.7527, 'grad_norm': 5.5431671142578125, 'learning_rate': 1.1666348175042996e-05, 'epoch': 1.9364040827008635}\n",
      "{'loss': 4.7178, 'grad_norm': 6.148876667022705, 'learning_rate': 1.1093063252436462e-05, 'epoch': 1.988746401465585}\n",
      "{'eval_loss': 4.7485175132751465, 'eval_bleu': 0.0005049133721103387, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 4.418260038240918, 'eval_avg_label_length': 18.499704666272887, 'eval_empty_predictions': 0.3821618428824572, 'eval_runtime': 167.7951, 'eval_samples_per_second': 20.179, 'eval_steps_per_second': 5.048, 'epoch': 2.0}\n",
      "{'loss': 4.4708, 'grad_norm': 5.2793121337890625, 'learning_rate': 1.0519778329829926e-05, 'epoch': 2.040827008636483}\n",
      "{'loss': 4.4741, 'grad_norm': 4.940310478210449, 'learning_rate': 9.946493407223391e-06, 'epoch': 2.093169327401204}\n",
      "{'loss': 4.5619, 'grad_norm': 5.258909225463867, 'learning_rate': 9.378941333842921e-06, 'epoch': 2.145511646165925}\n",
      "{'loss': 4.6119, 'grad_norm': 6.229960918426514, 'learning_rate': 8.805656411236384e-06, 'epoch': 2.1978539649306463}\n",
      "{'loss': 4.389, 'grad_norm': 5.813996315002441, 'learning_rate': 8.23237148862985e-06, 'epoch': 2.2501962836953675}\n",
      "{'loss': 4.4857, 'grad_norm': 5.582672595977783, 'learning_rate': 7.659086566023314e-06, 'epoch': 2.302538602460089}\n",
      "{'loss': 4.4555, 'grad_norm': 6.858811378479004, 'learning_rate': 7.085801643416778e-06, 'epoch': 2.3548809212248103}\n",
      "{'loss': 4.4402, 'grad_norm': 6.172133445739746, 'learning_rate': 6.5125167208102425e-06, 'epoch': 2.4072232399895315}\n",
      "{'loss': 4.5276, 'grad_norm': 5.524244785308838, 'learning_rate': 5.939231798203707e-06, 'epoch': 2.4595655587542526}\n",
      "{'loss': 4.4112, 'grad_norm': 5.875540733337402, 'learning_rate': 5.365946875597172e-06, 'epoch': 2.511907877518974}\n",
      "{'loss': 4.4549, 'grad_norm': 5.796280860900879, 'learning_rate': 4.792661952990637e-06, 'epoch': 2.5642501962836954}\n",
      "{'loss': 4.3627, 'grad_norm': 6.888566017150879, 'learning_rate': 4.219377030384101e-06, 'epoch': 2.6165925150484166}\n",
      "{'loss': 4.479, 'grad_norm': 8.091780662536621, 'learning_rate': 3.646092107777565e-06, 'epoch': 2.668934833813138}\n",
      "{'loss': 4.3887, 'grad_norm': 6.232186794281006, 'learning_rate': 3.0728071851710297e-06, 'epoch': 2.7212771525778594}\n",
      "{'loss': 4.4256, 'grad_norm': 6.417212009429932, 'learning_rate': 2.499522262564495e-06, 'epoch': 2.7736194713425806}\n",
      "{'loss': 4.4576, 'grad_norm': 6.102798938751221, 'learning_rate': 1.9262373399579594e-06, 'epoch': 2.8259617901073018}\n",
      "{'loss': 4.4496, 'grad_norm': 6.9173479080200195, 'learning_rate': 1.3529524173514237e-06, 'epoch': 2.878304108872023}\n",
      "{'loss': 4.5248, 'grad_norm': 5.143466472625732, 'learning_rate': 7.796674947448882e-07, 'epoch': 2.930646427636744}\n",
      "{'loss': 4.42, 'grad_norm': 5.54390811920166, 'learning_rate': 2.0638257213835276e-07, 'epoch': 2.9829887464014657}\n",
      "{'eval_loss': 4.608800888061523, 'eval_bleu': 0.0007242128942672903, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 4.822447992259313, 'eval_avg_label_length': 18.499704666272887, 'eval_empty_predictions': 0.38954518606024807, 'eval_runtime': 180.542, 'eval_samples_per_second': 18.755, 'eval_steps_per_second': 4.691, 'epoch': 3.0}\n",
      "{'train_runtime': 2309.8142, 'train_samples_per_second': 39.699, 'train_steps_per_second': 2.482, 'train_loss': 5.239545996809181, 'epoch': 3.0}\n",
      "📊 Final multilingual evaluation...\n",
      "{'eval_loss': 5.196696758270264, 'eval_bleu': 0.0018840141865606739, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 8.499762695775985, 'eval_avg_label_length': 18.499704666272887, 'eval_empty_predictions': 0.3777318369757826, 'eval_runtime': 287.9351, 'eval_samples_per_second': 11.76, 'eval_steps_per_second': 2.942, 'epoch': 3.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_medium_hf_bpe_hf\n",
      "📈 Overall BLEU Score: 0.0019\n",
      "🎯 Overall Exact Match: 0.0000\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): \n",
      "  zh (Chinese): \n",
      "  hi (Hindi): हैं ?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 5/9\n",
      "🔧 Custom Tokenizer: medium_hf_wordpiece_hf\n",
      "📦 Base Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/hf_wordpiece_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 30002\n",
      "🔑 Special tokens - BOS: 30000, EOS: 30001, PAD: 0\n",
      "🤖 Loading base multilingual mBART model...\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "🏋️  Starting multilingual training with custom tokenizer...\n",
      "{'loss': 11.1376, 'grad_norm': 5.200934410095215, 'learning_rate': 5.8800000000000005e-06, 'epoch': 0.05234231876472128}\n",
      "{'loss': 8.4933, 'grad_norm': 7.781042575836182, 'learning_rate': 1.1880000000000001e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 7.4095, 'grad_norm': 6.730116844177246, 'learning_rate': 1.7879999999999998e-05, 'epoch': 0.15702695629416383}\n",
      "{'loss': 6.8762, 'grad_norm': 8.255982398986816, 'learning_rate': 2.3880000000000002e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 6.4249, 'grad_norm': 6.454185962677002, 'learning_rate': 2.9880000000000002e-05, 'epoch': 0.26171159382360637}\n",
      "{'loss': 6.0396, 'grad_norm': 5.469091892242432, 'learning_rate': 2.9438180775845595e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 5.8739, 'grad_norm': 5.635328769683838, 'learning_rate': 2.886489585323906e-05, 'epoch': 0.36639623135304894}\n",
      "{'loss': 5.6596, 'grad_norm': 4.893613815307617, 'learning_rate': 2.8291610930632527e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 5.3776, 'grad_norm': 5.278167247772217, 'learning_rate': 2.7724058857252054e-05, 'epoch': 0.4710808688824915}\n",
      "{'loss': 5.4115, 'grad_norm': 5.182387351989746, 'learning_rate': 2.715077393464552e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 5.2092, 'grad_norm': 5.049655914306641, 'learning_rate': 2.6577489012038985e-05, 'epoch': 0.575765506411934}\n",
      "{'loss': 5.2278, 'grad_norm': 5.983367919921875, 'learning_rate': 2.6004204089432447e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 5.1389, 'grad_norm': 5.940249919891357, 'learning_rate': 2.5430919166825913e-05, 'epoch': 0.6804501439413766}\n",
      "{'loss': 4.9966, 'grad_norm': 5.678930282592773, 'learning_rate': 2.485763424421938e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 5.0505, 'grad_norm': 5.4056525230407715, 'learning_rate': 2.4284349321612845e-05, 'epoch': 0.7851347814708192}\n",
      "{'loss': 4.9898, 'grad_norm': 5.141661643981934, 'learning_rate': 2.3711064399006307e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 4.9066, 'grad_norm': 5.868112564086914, 'learning_rate': 2.313777947639977e-05, 'epoch': 0.8898194190002617}\n",
      "{'loss': 4.8093, 'grad_norm': 5.357609748840332, 'learning_rate': 2.2564494553793235e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 4.7066, 'grad_norm': 5.249181270599365, 'learning_rate': 2.19912096311867e-05, 'epoch': 0.9945040565297043}\n",
      "{'eval_loss': 4.6730499267578125, 'eval_bleu': 0.0012218624112152248, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 15.34607564050427, 'eval_avg_label_length': 21.59615952732644, 'eval_empty_predictions': 0.2737743650324867, 'eval_runtime': 473.2252, 'eval_samples_per_second': 7.155, 'eval_steps_per_second': 1.79, 'epoch': 1.0}\n",
      "{'loss': 4.6162, 'grad_norm': 5.017311096191406, 'learning_rate': 2.1417924708580163e-05, 'epoch': 1.046584663700602}\n",
      "{'loss': 4.5727, 'grad_norm': 6.215651035308838, 'learning_rate': 2.084463978597363e-05, 'epoch': 1.0989269824653232}\n",
      "{'loss': 4.4516, 'grad_norm': 5.833532333374023, 'learning_rate': 2.0271354863367095e-05, 'epoch': 1.1512693012300446}\n",
      "{'loss': 4.5556, 'grad_norm': 5.667123794555664, 'learning_rate': 1.969806994076056e-05, 'epoch': 1.2036116199947657}\n",
      "{'loss': 4.4681, 'grad_norm': 5.065176963806152, 'learning_rate': 1.9124785018154023e-05, 'epoch': 1.255953938759487}\n",
      "{'loss': 4.4524, 'grad_norm': 5.989053726196289, 'learning_rate': 1.855150009554749e-05, 'epoch': 1.3082962575242083}\n",
      "{'loss': 4.4408, 'grad_norm': 5.763132095336914, 'learning_rate': 1.797821517294095e-05, 'epoch': 1.3606385762889297}\n",
      "{'loss': 4.286, 'grad_norm': 5.582121849060059, 'learning_rate': 1.7404930250334416e-05, 'epoch': 1.4129808950536509}\n",
      "{'loss': 4.3301, 'grad_norm': 6.536685943603516, 'learning_rate': 1.683164532772788e-05, 'epoch': 1.465323213818372}\n",
      "{'loss': 4.2823, 'grad_norm': 6.431714057922363, 'learning_rate': 1.6258360405121345e-05, 'epoch': 1.5176655325830934}\n",
      "{'loss': 4.2726, 'grad_norm': 6.292059898376465, 'learning_rate': 1.568507548251481e-05, 'epoch': 1.5700078513478148}\n",
      "{'loss': 4.2501, 'grad_norm': 5.488346099853516, 'learning_rate': 1.5111790559908274e-05, 'epoch': 1.622350170112536}\n",
      "{'loss': 4.3121, 'grad_norm': 4.912924289703369, 'learning_rate': 1.453850563730174e-05, 'epoch': 1.6746924888772572}\n",
      "{'loss': 4.1707, 'grad_norm': 4.68790340423584, 'learning_rate': 1.3965220714695202e-05, 'epoch': 1.7270348076419786}\n",
      "{'loss': 4.2297, 'grad_norm': 5.489402770996094, 'learning_rate': 1.3391935792088668e-05, 'epoch': 1.7793771264067}\n",
      "{'loss': 4.2237, 'grad_norm': 5.442539215087891, 'learning_rate': 1.2818650869482134e-05, 'epoch': 1.8317194451714212}\n",
      "{'loss': 4.232, 'grad_norm': 6.572483062744141, 'learning_rate': 1.2245365946875598e-05, 'epoch': 1.8840617639361423}\n",
      "{'loss': 4.1812, 'grad_norm': 6.136053562164307, 'learning_rate': 1.1672081024269062e-05, 'epoch': 1.9364040827008635}\n",
      "{'loss': 4.1446, 'grad_norm': 6.585842609405518, 'learning_rate': 1.1098796101662526e-05, 'epoch': 1.988746401465585}\n",
      "{'eval_loss': 4.140580177307129, 'eval_bleu': 0.001447428365853842, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 7.197128851540616, 'eval_avg_label_length': 21.59615952732644, 'eval_empty_predictions': 0.15652687536916715, 'eval_runtime': 262.9845, 'eval_samples_per_second': 12.875, 'eval_steps_per_second': 3.221, 'epoch': 2.0}\n",
      "{'loss': 3.9291, 'grad_norm': 5.609498023986816, 'learning_rate': 1.0525511179055992e-05, 'epoch': 2.040827008636483}\n",
      "{'loss': 3.9205, 'grad_norm': 5.4231181144714355, 'learning_rate': 9.952226256449456e-06, 'epoch': 2.093169327401204}\n",
      "{'loss': 4.005, 'grad_norm': 5.5605573654174805, 'learning_rate': 9.378941333842921e-06, 'epoch': 2.145511646165925}\n",
      "{'loss': 4.0511, 'grad_norm': 6.467677593231201, 'learning_rate': 8.805656411236384e-06, 'epoch': 2.1978539649306463}\n",
      "{'loss': 3.8437, 'grad_norm': 6.310855865478516, 'learning_rate': 8.23237148862985e-06, 'epoch': 2.2501962836953675}\n",
      "{'loss': 3.939, 'grad_norm': 5.3875627517700195, 'learning_rate': 7.659086566023314e-06, 'epoch': 2.302538602460089}\n",
      "{'loss': 3.9012, 'grad_norm': 6.542329788208008, 'learning_rate': 7.085801643416778e-06, 'epoch': 2.3548809212248103}\n",
      "{'loss': 3.8877, 'grad_norm': 6.257019519805908, 'learning_rate': 6.5125167208102425e-06, 'epoch': 2.4072232399895315}\n",
      "{'loss': 3.9369, 'grad_norm': 5.166493892669678, 'learning_rate': 5.939231798203707e-06, 'epoch': 2.4595655587542526}\n",
      "{'loss': 3.8559, 'grad_norm': 6.025891304016113, 'learning_rate': 5.365946875597172e-06, 'epoch': 2.511907877518974}\n",
      "{'loss': 3.8824, 'grad_norm': 6.528195381164551, 'learning_rate': 4.792661952990637e-06, 'epoch': 2.5642501962836954}\n",
      "{'loss': 3.8109, 'grad_norm': 7.602482318878174, 'learning_rate': 4.219377030384101e-06, 'epoch': 2.6165925150484166}\n",
      "{'loss': 3.8832, 'grad_norm': 8.038132667541504, 'learning_rate': 3.646092107777565e-06, 'epoch': 2.668934833813138}\n",
      "{'loss': 3.8251, 'grad_norm': 6.3327226638793945, 'learning_rate': 3.0728071851710297e-06, 'epoch': 2.7212771525778594}\n",
      "{'loss': 3.8588, 'grad_norm': 6.747406482696533, 'learning_rate': 2.499522262564495e-06, 'epoch': 2.7736194713425806}\n",
      "{'loss': 3.8749, 'grad_norm': 6.554255962371826, 'learning_rate': 1.9262373399579594e-06, 'epoch': 2.8259617901073018}\n",
      "{'loss': 3.8711, 'grad_norm': 6.9693379402160645, 'learning_rate': 1.3529524173514237e-06, 'epoch': 2.878304108872023}\n",
      "{'loss': 3.9519, 'grad_norm': 5.62723970413208, 'learning_rate': 7.796674947448882e-07, 'epoch': 2.930646427636744}\n",
      "{'loss': 3.8687, 'grad_norm': 6.112924098968506, 'learning_rate': 2.0638257213835276e-07, 'epoch': 2.9829887464014657}\n",
      "{'eval_loss': 3.9762558937072754, 'eval_bleu': 0.0016470835337751215, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 10.200801871032409, 'eval_avg_label_length': 21.59615952732644, 'eval_empty_predictions': 0.116066154754873, 'eval_runtime': 381.875, 'eval_samples_per_second': 8.867, 'eval_steps_per_second': 2.218, 'epoch': 3.0}\n",
      "{'train_runtime': 2807.3603, 'train_samples_per_second': 32.663, 'train_steps_per_second': 2.042, 'train_loss': 4.735205854771546, 'epoch': 3.0}\n",
      "📊 Final multilingual evaluation...\n",
      "{'eval_loss': 3.9762558937072754, 'eval_bleu': 0.0016470835337751215, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 10.200801871032409, 'eval_avg_label_length': 21.59615952732644, 'eval_empty_predictions': 0.116066154754873, 'eval_runtime': 379.0717, 'eval_samples_per_second': 8.932, 'eval_steps_per_second': 2.234, 'epoch': 3.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_medium_hf_wordpiece_hf\n",
      "📈 Overall BLEU Score: 0.0016\n",
      "🎯 Overall Exact Match: 0.0000\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): ؟ ؟\n",
      "  zh (Chinese): ? ?\n",
      "  hi (Hindi): ? ?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 6/9\n",
      "🔧 Custom Tokenizer: medium_sp_unigram_hf\n",
      "📦 Base Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/sp_unigram_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 30002\n",
      "🔑 Special tokens - BOS: 1, EOS: 2, PAD: 30000\n",
      "🤖 Loading base multilingual mBART model...\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "🏋️  Starting multilingual training with custom tokenizer...\n",
      "{'loss': 9.4944, 'grad_norm': 5.53443717956543, 'learning_rate': 5.8800000000000005e-06, 'epoch': 0.05234231876472128}\n",
      "{'loss': 7.9941, 'grad_norm': 7.6406025886535645, 'learning_rate': 1.1880000000000001e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 7.3095, 'grad_norm': 4.046055316925049, 'learning_rate': 1.7879999999999998e-05, 'epoch': 0.15702695629416383}\n",
      "{'loss': 6.9236, 'grad_norm': 4.760957717895508, 'learning_rate': 2.3880000000000002e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 6.5662, 'grad_norm': 4.2515387535095215, 'learning_rate': 2.9880000000000002e-05, 'epoch': 0.26171159382360637}\n",
      "{'loss': 6.2047, 'grad_norm': 4.168789863586426, 'learning_rate': 2.9438180775845595e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 6.0355, 'grad_norm': 5.537264823913574, 'learning_rate': 2.886489585323906e-05, 'epoch': 0.36639623135304894}\n",
      "{'loss': 5.7529, 'grad_norm': 4.313342094421387, 'learning_rate': 2.8291610930632527e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 5.4921, 'grad_norm': 5.3026862144470215, 'learning_rate': 2.771832600802599e-05, 'epoch': 0.4710808688824915}\n",
      "{'loss': 5.459, 'grad_norm': 5.281454563140869, 'learning_rate': 2.7145041085419455e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 5.2519, 'grad_norm': 3.964506149291992, 'learning_rate': 2.657175616281292e-05, 'epoch': 0.575765506411934}\n",
      "{'loss': 5.2496, 'grad_norm': 5.151881217956543, 'learning_rate': 2.5998471240206383e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 5.1082, 'grad_norm': 5.329181671142578, 'learning_rate': 2.5425186317599845e-05, 'epoch': 0.6804501439413766}\n",
      "{'loss': 4.9952, 'grad_norm': 4.874823570251465, 'learning_rate': 2.485190139499331e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 5.1145, 'grad_norm': 4.750169277191162, 'learning_rate': 2.4278616472386777e-05, 'epoch': 0.7851347814708192}\n",
      "{'loss': 4.9749, 'grad_norm': 4.7702789306640625, 'learning_rate': 2.3705331549780243e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 4.8977, 'grad_norm': 5.350502014160156, 'learning_rate': 2.3132046627173705e-05, 'epoch': 0.8898194190002617}\n",
      "{'loss': 4.8606, 'grad_norm': 5.6561689376831055, 'learning_rate': 2.255876170456717e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 4.748, 'grad_norm': 4.766049385070801, 'learning_rate': 2.1985476781960636e-05, 'epoch': 0.9945040565297043}\n",
      "{'eval_loss': 4.720383167266846, 'eval_bleu': 0.0, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 1.0, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.9940933254577673, 'eval_runtime': 76.6118, 'eval_samples_per_second': 44.197, 'eval_steps_per_second': 11.056, 'epoch': 1.0}\n",
      "{'loss': 4.5828, 'grad_norm': 4.80112886428833, 'learning_rate': 2.1412191859354102e-05, 'epoch': 1.046584663700602}\n",
      "{'loss': 4.5896, 'grad_norm': 5.272393226623535, 'learning_rate': 2.083890693674756e-05, 'epoch': 1.0989269824653232}\n",
      "{'loss': 4.4979, 'grad_norm': 7.677516937255859, 'learning_rate': 2.0265622014141027e-05, 'epoch': 1.1512693012300446}\n",
      "{'loss': 4.5469, 'grad_norm': 5.118011474609375, 'learning_rate': 1.9692337091534492e-05, 'epoch': 1.2036116199947657}\n",
      "{'loss': 4.494, 'grad_norm': 5.300285339355469, 'learning_rate': 1.9119052168927958e-05, 'epoch': 1.255953938759487}\n",
      "{'loss': 4.4965, 'grad_norm': 5.109790325164795, 'learning_rate': 1.8545767246321424e-05, 'epoch': 1.3082962575242083}\n",
      "{'loss': 4.5112, 'grad_norm': 5.019096374511719, 'learning_rate': 1.7972482323714886e-05, 'epoch': 1.3606385762889297}\n",
      "{'loss': 4.4037, 'grad_norm': 5.230185031890869, 'learning_rate': 1.7399197401108352e-05, 'epoch': 1.4129808950536509}\n",
      "{'loss': 4.4036, 'grad_norm': 5.907418727874756, 'learning_rate': 1.6825912478501818e-05, 'epoch': 1.465323213818372}\n",
      "{'loss': 4.3372, 'grad_norm': 5.228430271148682, 'learning_rate': 1.6252627555895283e-05, 'epoch': 1.5176655325830934}\n",
      "{'loss': 4.3413, 'grad_norm': 5.174560546875, 'learning_rate': 1.5679342633288742e-05, 'epoch': 1.5700078513478148}\n",
      "{'loss': 4.3585, 'grad_norm': 4.772825717926025, 'learning_rate': 1.5106057710682208e-05, 'epoch': 1.622350170112536}\n",
      "{'loss': 4.3959, 'grad_norm': 4.550634860992432, 'learning_rate': 1.4532772788075674e-05, 'epoch': 1.6746924888772572}\n",
      "{'loss': 4.2887, 'grad_norm': 4.73565673828125, 'learning_rate': 1.3959487865469138e-05, 'epoch': 1.7270348076419786}\n",
      "{'loss': 4.3539, 'grad_norm': 5.2500386238098145, 'learning_rate': 1.3386202942862604e-05, 'epoch': 1.7793771264067}\n",
      "{'loss': 4.3551, 'grad_norm': 6.213389873504639, 'learning_rate': 1.2812918020256068e-05, 'epoch': 1.8317194451714212}\n",
      "{'loss': 4.3453, 'grad_norm': 5.752575397491455, 'learning_rate': 1.2239633097649532e-05, 'epoch': 1.8840617639361423}\n",
      "{'loss': 4.3214, 'grad_norm': 5.303824424743652, 'learning_rate': 1.1666348175042996e-05, 'epoch': 1.9364040827008635}\n",
      "{'loss': 4.2846, 'grad_norm': 5.912919998168945, 'learning_rate': 1.1093063252436462e-05, 'epoch': 1.988746401465585}\n",
      "{'eval_loss': 4.3022379875183105, 'eval_bleu': 7.868807518315602e-06, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 5.192488262910798, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.874187832250443, 'eval_runtime': 351.8613, 'eval_samples_per_second': 9.623, 'eval_steps_per_second': 2.407, 'epoch': 2.0}\n",
      "{'loss': 4.0651, 'grad_norm': 5.367408752441406, 'learning_rate': 1.0519778329829926e-05, 'epoch': 2.040827008636483}\n",
      "{'loss': 4.0435, 'grad_norm': 5.048388957977295, 'learning_rate': 9.946493407223391e-06, 'epoch': 2.093169327401204}\n",
      "{'loss': 4.0934, 'grad_norm': 5.386285781860352, 'learning_rate': 9.373208484616854e-06, 'epoch': 2.145511646165925}\n",
      "{'loss': 4.171, 'grad_norm': 6.1396565437316895, 'learning_rate': 8.79992356201032e-06, 'epoch': 2.1978539649306463}\n",
      "{'loss': 3.9887, 'grad_norm': 6.0431084632873535, 'learning_rate': 8.226638639403783e-06, 'epoch': 2.2501962836953675}\n",
      "{'loss': 4.0462, 'grad_norm': 5.0794243812561035, 'learning_rate': 7.653353716797249e-06, 'epoch': 2.302538602460089}\n",
      "{'loss': 3.998, 'grad_norm': 6.4397807121276855, 'learning_rate': 7.080068794190713e-06, 'epoch': 2.3548809212248103}\n",
      "{'loss': 3.9983, 'grad_norm': 5.869791030883789, 'learning_rate': 6.506783871584177e-06, 'epoch': 2.4072232399895315}\n",
      "{'loss': 4.037, 'grad_norm': 5.196213722229004, 'learning_rate': 5.933498948977642e-06, 'epoch': 2.4595655587542526}\n",
      "{'loss': 3.9667, 'grad_norm': 5.604588985443115, 'learning_rate': 5.360214026371107e-06, 'epoch': 2.511907877518974}\n",
      "{'loss': 4.0313, 'grad_norm': 5.89774227142334, 'learning_rate': 4.786929103764571e-06, 'epoch': 2.5642501962836954}\n",
      "{'loss': 3.923, 'grad_norm': 6.806248188018799, 'learning_rate': 4.213644181158036e-06, 'epoch': 2.6165925150484166}\n",
      "{'loss': 4.026, 'grad_norm': 6.707324504852295, 'learning_rate': 3.6403592585515e-06, 'epoch': 2.668934833813138}\n",
      "{'loss': 3.9557, 'grad_norm': 6.584537029266357, 'learning_rate': 3.0670743359449648e-06, 'epoch': 2.7212771525778594}\n",
      "{'loss': 4.0059, 'grad_norm': 6.012953281402588, 'learning_rate': 2.4937894133384292e-06, 'epoch': 2.7736194713425806}\n",
      "{'loss': 3.9973, 'grad_norm': 6.00121545791626, 'learning_rate': 1.9205044907318937e-06, 'epoch': 2.8259617901073018}\n",
      "{'loss': 3.9853, 'grad_norm': 6.382375717163086, 'learning_rate': 1.3472195681253584e-06, 'epoch': 2.878304108872023}\n",
      "{'loss': 4.0343, 'grad_norm': 6.077599048614502, 'learning_rate': 7.739346455188228e-07, 'epoch': 2.930646427636744}\n",
      "{'loss': 3.9796, 'grad_norm': 5.9381208419799805, 'learning_rate': 2.0064972291228742e-07, 'epoch': 2.9829887464014657}\n",
      "{'eval_loss': 4.186166763305664, 'eval_bleu': 0.0, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 6.510416666666667, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.9716479621972829, 'eval_runtime': 154.1362, 'eval_samples_per_second': 21.968, 'eval_steps_per_second': 5.495, 'epoch': 3.0}\n",
      "{'train_runtime': 2290.3774, 'train_samples_per_second': 40.036, 'train_steps_per_second': 2.503, 'train_loss': 4.777989378030359, 'epoch': 3.0}\n",
      "📊 Final multilingual evaluation...\n",
      "{'eval_loss': 4.3022379875183105, 'eval_bleu': 7.868807518315602e-06, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 5.192488262910798, 'eval_avg_label_length': 7.20939161252215, 'eval_empty_predictions': 0.874187832250443, 'eval_runtime': 352.5066, 'eval_samples_per_second': 9.605, 'eval_steps_per_second': 2.403, 'epoch': 3.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_medium_sp_unigram_hf\n",
      "📈 Overall BLEU Score: 0.0000\n",
      "🎯 Overall Exact Match: 0.0000\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): \n",
      "  zh (Chinese): \n",
      "  hi (Hindi): \n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 7/9\n",
      "🔧 Custom Tokenizer: large_hf_bpe_hf\n",
      "📦 Base Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finallarge/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 50002\n",
      "🔑 Special tokens - BOS: 50000, EOS: 50001, PAD: 0\n",
      "🤖 Loading base multilingual mBART model...\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with large_hf_bpe_hf: 100%|██████████| 3820/3820 [00:01<00:00, 2154.90 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:00<00:00, 8621.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "🏋️  Starting multilingual training with custom tokenizer...\n",
      "{'loss': 11.1783, 'grad_norm': 5.359941005706787, 'learning_rate': 5.82e-06, 'epoch': 0.05234231876472128}\n",
      "{'loss': 9.1278, 'grad_norm': 9.098174095153809, 'learning_rate': 1.182e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 8.1311, 'grad_norm': 6.191810607910156, 'learning_rate': 1.782e-05, 'epoch': 0.15702695629416383}\n",
      "{'loss': 7.714, 'grad_norm': 5.863392353057861, 'learning_rate': 2.3820000000000002e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 7.3491, 'grad_norm': 5.360826015472412, 'learning_rate': 2.982e-05, 'epoch': 0.26171159382360637}\n",
      "{'loss': 6.9923, 'grad_norm': 5.599084377288818, 'learning_rate': 2.9443913625071663e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 6.8377, 'grad_norm': 6.245006084442139, 'learning_rate': 2.8870628702465126e-05, 'epoch': 0.36639623135304894}\n",
      "{'loss': 6.6141, 'grad_norm': 4.975226402282715, 'learning_rate': 2.8297343779858588e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 6.232, 'grad_norm': 5.245513916015625, 'learning_rate': 2.7724058857252054e-05, 'epoch': 0.4710808688824915}\n",
      "{'loss': 6.261, 'grad_norm': 6.043541431427002, 'learning_rate': 2.715077393464552e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 6.0361, 'grad_norm': 4.279346466064453, 'learning_rate': 2.6577489012038985e-05, 'epoch': 0.575765506411934}\n",
      "{'loss': 6.0569, 'grad_norm': 5.520505428314209, 'learning_rate': 2.6004204089432447e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 5.9283, 'grad_norm': 6.593883037567139, 'learning_rate': 2.5430919166825913e-05, 'epoch': 0.6804501439413766}\n",
      "{'loss': 5.7938, 'grad_norm': 5.804290771484375, 'learning_rate': 2.485763424421938e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 5.8832, 'grad_norm': 5.428886413574219, 'learning_rate': 2.4284349321612845e-05, 'epoch': 0.7851347814708192}\n",
      "{'loss': 5.8004, 'grad_norm': 4.795729160308838, 'learning_rate': 2.3711064399006307e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 5.6937, 'grad_norm': 5.764179706573486, 'learning_rate': 2.313777947639977e-05, 'epoch': 0.8898194190002617}\n",
      "{'loss': 5.6239, 'grad_norm': 5.788395404815674, 'learning_rate': 2.2564494553793235e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 5.5172, 'grad_norm': 4.927469730377197, 'learning_rate': 2.19912096311867e-05, 'epoch': 0.9945040565297043}\n",
      "{'eval_loss': 5.514486789703369, 'eval_bleu': 0.000658890018444661, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 10.85258116019159, 'eval_avg_label_length': 16.87950383933845, 'eval_empty_predictions': 0.44506792675723567, 'eval_runtime': 293.44, 'eval_samples_per_second': 11.539, 'eval_steps_per_second': 2.886, 'epoch': 1.0}\n",
      "{'loss': 5.3756, 'grad_norm': 4.879241943359375, 'learning_rate': 2.1417924708580163e-05, 'epoch': 1.046584663700602}\n",
      "{'loss': 5.3396, 'grad_norm': 5.916141510009766, 'learning_rate': 2.084463978597363e-05, 'epoch': 1.0989269824653232}\n",
      "{'loss': 5.2058, 'grad_norm': 6.402354717254639, 'learning_rate': 2.0271354863367095e-05, 'epoch': 1.1512693012300446}\n",
      "{'loss': 5.3354, 'grad_norm': 5.986547470092773, 'learning_rate': 1.969806994076056e-05, 'epoch': 1.2036116199947657}\n",
      "{'loss': 5.2451, 'grad_norm': 6.027109146118164, 'learning_rate': 1.9124785018154023e-05, 'epoch': 1.255953938759487}\n",
      "{'loss': 5.2654, 'grad_norm': 6.0849456787109375, 'learning_rate': 1.855150009554749e-05, 'epoch': 1.3082962575242083}\n",
      "{'loss': 5.2724, 'grad_norm': 5.252466678619385, 'learning_rate': 1.797821517294095e-05, 'epoch': 1.3606385762889297}\n",
      "{'loss': 5.0963, 'grad_norm': 5.305815696716309, 'learning_rate': 1.7404930250334416e-05, 'epoch': 1.4129808950536509}\n",
      "{'loss': 5.1514, 'grad_norm': 6.5846405029296875, 'learning_rate': 1.683164532772788e-05, 'epoch': 1.465323213818372}\n",
      "{'loss': 5.0326, 'grad_norm': 5.983981609344482, 'learning_rate': 1.6258360405121345e-05, 'epoch': 1.5176655325830934}\n",
      "{'loss': 5.1003, 'grad_norm': 6.316936016082764, 'learning_rate': 1.568507548251481e-05, 'epoch': 1.5700078513478148}\n",
      "{'loss': 5.0829, 'grad_norm': 5.188393592834473, 'learning_rate': 1.5111790559908274e-05, 'epoch': 1.622350170112536}\n",
      "{'loss': 5.1215, 'grad_norm': 4.957768440246582, 'learning_rate': 1.453850563730174e-05, 'epoch': 1.6746924888772572}\n",
      "{'loss': 5.0124, 'grad_norm': 4.662117958068848, 'learning_rate': 1.3965220714695202e-05, 'epoch': 1.7270348076419786}\n",
      "{'loss': 5.0766, 'grad_norm': 5.448631763458252, 'learning_rate': 1.3391935792088668e-05, 'epoch': 1.7793771264067}\n",
      "{'loss': 5.0701, 'grad_norm': 7.096742153167725, 'learning_rate': 1.2818650869482134e-05, 'epoch': 1.8317194451714212}\n",
      "{'loss': 5.1269, 'grad_norm': 6.780191898345947, 'learning_rate': 1.2245365946875598e-05, 'epoch': 1.8840617639361423}\n",
      "{'loss': 5.045, 'grad_norm': 5.626232147216797, 'learning_rate': 1.1672081024269062e-05, 'epoch': 1.9364040827008635}\n",
      "{'loss': 5.0142, 'grad_norm': 6.091378688812256, 'learning_rate': 1.1098796101662526e-05, 'epoch': 1.988746401465585}\n",
      "{'eval_loss': 5.076735496520996, 'eval_bleu': 0.0011870808930333969, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 5.924435721295388, 'eval_avg_label_length': 16.87950383933845, 'eval_empty_predictions': 0.3981098641464855, 'eval_runtime': 184.8063, 'eval_samples_per_second': 18.322, 'eval_steps_per_second': 4.583, 'epoch': 2.0}\n",
      "{'loss': 4.731, 'grad_norm': 5.5157318115234375, 'learning_rate': 1.0525511179055992e-05, 'epoch': 2.040827008636483}\n",
      "{'loss': 4.7327, 'grad_norm': 5.167628765106201, 'learning_rate': 9.952226256449456e-06, 'epoch': 2.093169327401204}\n",
      "{'loss': 4.8336, 'grad_norm': 5.5249223709106445, 'learning_rate': 9.378941333842921e-06, 'epoch': 2.145511646165925}\n",
      "{'loss': 4.8938, 'grad_norm': 6.397688865661621, 'learning_rate': 8.805656411236384e-06, 'epoch': 2.1978539649306463}\n",
      "{'loss': 4.6519, 'grad_norm': 5.819018363952637, 'learning_rate': 8.23237148862985e-06, 'epoch': 2.2501962836953675}\n",
      "{'loss': 4.7544, 'grad_norm': 5.4873528480529785, 'learning_rate': 7.659086566023314e-06, 'epoch': 2.302538602460089}\n",
      "{'loss': 4.7161, 'grad_norm': 6.708868026733398, 'learning_rate': 7.085801643416778e-06, 'epoch': 2.3548809212248103}\n",
      "{'loss': 4.7052, 'grad_norm': 6.691185474395752, 'learning_rate': 6.5125167208102425e-06, 'epoch': 2.4072232399895315}\n",
      "{'loss': 4.8164, 'grad_norm': 5.736482620239258, 'learning_rate': 5.939231798203707e-06, 'epoch': 2.4595655587542526}\n",
      "{'loss': 4.6906, 'grad_norm': 6.076256275177002, 'learning_rate': 5.365946875597172e-06, 'epoch': 2.511907877518974}\n",
      "{'loss': 4.744, 'grad_norm': 5.839715957641602, 'learning_rate': 4.792661952990637e-06, 'epoch': 2.5642501962836954}\n",
      "{'loss': 4.6152, 'grad_norm': 7.154884338378906, 'learning_rate': 4.219377030384101e-06, 'epoch': 2.6165925150484166}\n",
      "{'loss': 4.7574, 'grad_norm': 7.91107702255249, 'learning_rate': 3.646092107777565e-06, 'epoch': 2.668934833813138}\n",
      "{'loss': 4.6666, 'grad_norm': 6.412744998931885, 'learning_rate': 3.0728071851710297e-06, 'epoch': 2.7212771525778594}\n",
      "{'loss': 4.7027, 'grad_norm': 7.009466171264648, 'learning_rate': 2.50525511179056e-06, 'epoch': 2.7736194713425806}\n",
      "{'loss': 4.7348, 'grad_norm': 6.247914791107178, 'learning_rate': 1.9319701891840247e-06, 'epoch': 2.8259617901073018}\n",
      "{'loss': 4.7253, 'grad_norm': 7.045144081115723, 'learning_rate': 1.358685266577489e-06, 'epoch': 2.878304108872023}\n",
      "{'loss': 4.8187, 'grad_norm': 5.316653251647949, 'learning_rate': 7.854003439709536e-07, 'epoch': 2.930646427636744}\n",
      "{'loss': 4.6983, 'grad_norm': 5.811032295227051, 'learning_rate': 2.1211542136441812e-07, 'epoch': 2.9829887464014657}\n",
      "{'eval_loss': 4.94482421875, 'eval_bleu': 0.0010875674395026847, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 7.307731177362304, 'eval_avg_label_length': 16.87950383933845, 'eval_empty_predictions': 0.41553455404607204, 'eval_runtime': 222.1391, 'eval_samples_per_second': 15.243, 'eval_steps_per_second': 3.813, 'epoch': 3.0}\n",
      "{'train_runtime': 2451.9016, 'train_samples_per_second': 37.399, 'train_steps_per_second': 2.338, 'train_loss': 5.549436083809776, 'epoch': 3.0}\n",
      "📊 Final multilingual evaluation...\n",
      "{'eval_loss': 5.076735496520996, 'eval_bleu': 0.0011870808930333969, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 5.924435721295388, 'eval_avg_label_length': 16.87950383933845, 'eval_empty_predictions': 0.3981098641464855, 'eval_runtime': 184.0327, 'eval_samples_per_second': 18.399, 'eval_steps_per_second': 4.602, 'epoch': 3.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "❌ Failed to train multilingual_large_hf_bpe_hf: [Errno 13] Permission denied: './MT_models_multilingual_custom_tokenizers\\\\multilingual_custom_tokenizer_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_26264\\3434637004.py\", line 409, in <module>\n",
      "    with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 324, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "PermissionError: [Errno 13] Permission denied: './MT_models_multilingual_custom_tokenizers\\\\multilingual_custom_tokenizer_results.csv'\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './MT_models_multilingual_custom_tokenizers\\\\multilingual_custom_tokenizer_results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 409\u001b[0m\n\u001b[0;32m    407\u001b[0m empty_preds \u001b[38;5;241m=\u001b[39m eval_results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_empty_predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults_log_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m    410\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvfile)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './MT_models_multilingual_custom_tokenizers\\\\multilingual_custom_tokenizer_results.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 456\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m    454\u001b[0m traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults_log_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m    457\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvfile)\n\u001b[0;32m    458\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\n\u001b[0;32m    459\u001b[0m         model_id, MODEL_CONFIG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), size, tok_type,\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28mlen\u001b[39m(languages), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAINING_FAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e)[:\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m    461\u001b[0m     ])\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './MT_models_multilingual_custom_tokenizers\\\\multilingual_custom_tokenizer_results.csv'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import torch\n",
    "# import warnings\n",
    "# import gc\n",
    "# from datasets import load_from_disk\n",
    "# from transformers import (\n",
    "#     EncoderDecoderModel,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "#     Seq2SeqTrainer,\n",
    "#     Seq2SeqTrainingArguments,\n",
    "#     logging as hf_logging,\n",
    "#     MBartForConditionalGeneration,\n",
    "#     MBart50TokenizerFast,\n",
    "#     T5ForConditionalGeneration,\n",
    "#     T5TokenizerFast\n",
    "# )\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# import numpy as np\n",
    "\n",
    "# # Suppress warnings\n",
    "# hf_logging.set_verbosity_error()\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Custom tokenizer settings - 3 sizes × 3 types = 9 combinations\n",
    "# tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
    "# tokenizer_types = [\"hf_bpe_hf\", \"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
    "\n",
    "# # Single multilingual model configuration\n",
    "# MODEL_CONFIG = {\n",
    "#     \"model_name\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "#     \"type\": \"mbart\",\n",
    "#     \"description\": \"Multilingual mBART model with custom tokenizers\"\n",
    "# }\n",
    "\n",
    "# # Languages for evaluation (model handles all simultaneously)\n",
    "# languages = {\n",
    "#     \"yo\": \"Yoruba\",\n",
    "#     \"ar\": \"Arabic\", \n",
    "#     \"zh\": \"Chinese\",\n",
    "#     \"ru\": \"Russian\",\n",
    "#     \"hi\": \"Hindi\",\n",
    "#     \"ja\": \"Japanese\",\n",
    "#     \"swa\": \"Swahili\",\n",
    "#     \"bn\": \"Bengali\",\n",
    "#     \"tr\": \"Turkish\"\n",
    "# }\n",
    "# SRC_LANG = \"en\"\n",
    "\n",
    "# # Load complete multilingual dataset\n",
    "# dataset_path = \"balanced_mt_dataset\"\n",
    "# print(\"📦 Loading complete multilingual dataset...\")\n",
    "# full_dataset = load_from_disk(dataset_path)\n",
    "# print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "# print(f\"🌍 All languages included: {list(languages.keys())}\")\n",
    "\n",
    "# # Enhanced BLEU computation for multilingual evaluation\n",
    "# def compute_multilingual_bleu(eval_pred):\n",
    "#     \"\"\"Multilingual BLEU computation across all target languages\"\"\"\n",
    "#     predictions, labels = eval_pred\n",
    "    \n",
    "#     if len(predictions.shape) == 3:\n",
    "#         predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "#     decoded_preds = []\n",
    "#     decoded_labels = []\n",
    "    \n",
    "#     for pred, label in zip(predictions, labels):\n",
    "#         # Replace -100 with pad token for decoding\n",
    "#         label = np.where(label != -100, label, tokenizer.pad_token_id)\n",
    "        \n",
    "#         decoded_pred = tokenizer.decode(pred, skip_special_tokens=True).strip()\n",
    "#         decoded_label = tokenizer.decode(label, skip_special_tokens=True).strip()\n",
    "        \n",
    "#         decoded_preds.append(decoded_pred)\n",
    "#         decoded_labels.append(decoded_label)\n",
    "    \n",
    "#     # Compute BLEU with smoothing\n",
    "#     smoothing = SmoothingFunction().method1\n",
    "#     bleu_scores = []\n",
    "#     exact_matches = 0\n",
    "    \n",
    "#     for pred, label in zip(decoded_preds, decoded_labels):\n",
    "#         if not pred.strip() or not label.strip():\n",
    "#             bleu_scores.append(0.0)\n",
    "#             continue\n",
    "            \n",
    "#         pred_tokens = pred.split()\n",
    "#         label_tokens = label.split()\n",
    "        \n",
    "#         # Check exact match\n",
    "#         if pred.lower().strip() == label.lower().strip():\n",
    "#             exact_matches += 1\n",
    "        \n",
    "#         if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "#             bleu_scores.append(0.0)\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             bleu = sentence_bleu(\n",
    "#                 [label_tokens], \n",
    "#                 pred_tokens,\n",
    "#                 smoothing_function=smoothing,\n",
    "#                 weights=(0.25, 0.25, 0.25, 0.25)\n",
    "#             )\n",
    "#             bleu_scores.append(bleu)\n",
    "#         except:\n",
    "#             bleu_scores.append(0.0)\n",
    "    \n",
    "#     avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "#     exact_match_ratio = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "    \n",
    "#     return {\n",
    "#         \"bleu\": avg_bleu,\n",
    "#         \"exact_match\": exact_match_ratio,\n",
    "#         \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "#         \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "#         \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "#     }\n",
    "\n",
    "# def setup_custom_tokenizer_for_mbart(tokenizer_path):\n",
    "#     \"\"\"Setup custom tokenizer with proper mBART special tokens\"\"\"\n",
    "#     # Load custom tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "    \n",
    "#     # Define mBART special tokens\n",
    "#     special_tokens = {\n",
    "#         'bos_token': '<s>',\n",
    "#         'eos_token': '</s>',\n",
    "#         'sep_token': '</s>',\n",
    "#         'pad_token': '<pad>',\n",
    "#         'unk_token': '<unk>',\n",
    "#         'mask_token': '<mask>'\n",
    "#     }\n",
    "    \n",
    "#     # Add missing special tokens\n",
    "#     tokens_to_add = {}\n",
    "#     for token_name, token_value in special_tokens.items():\n",
    "#         if getattr(tokenizer, token_name, None) is None:\n",
    "#             tokens_to_add[token_name] = token_value\n",
    "    \n",
    "#     if tokens_to_add:\n",
    "#         tokenizer.add_special_tokens(tokens_to_add)\n",
    "    \n",
    "#     # Ensure we have the required tokens\n",
    "#     assert tokenizer.bos_token is not None, \"BOS token is required\"\n",
    "#     assert tokenizer.eos_token is not None, \"EOS token is required\"\n",
    "#     assert tokenizer.pad_token is not None, \"PAD token is required\"\n",
    "    \n",
    "#     return tokenizer\n",
    "\n",
    "# def configure_model_for_custom_tokenizer(model, tokenizer):\n",
    "#     \"\"\"Configure mBART model for custom tokenizer\"\"\"\n",
    "#     # Resize embeddings\n",
    "#     print(\"🔄 Resizing model embeddings to match custom tokenizer...\")\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "#     # Set token IDs\n",
    "#     model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "#     model.config.pad_token_id = tokenizer.pad_token_id\n",
    "#     model.config.bos_token_id = tokenizer.bos_token_id\n",
    "#     model.config.eos_token_id = tokenizer.eos_token_id\n",
    "#     model.config.sep_token_id = tokenizer.eos_token_id  # mBART uses EOS as SEP\n",
    "#     model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "#     # Update generation config if it exists\n",
    "#     if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "#         model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "#         model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "#         model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "#         model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "#         model.generation_config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Setup logging\n",
    "# log_dir = \"./MT_models_multilingual_custom_tokenizers\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# results_log_file = os.path.join(log_dir, \"multilingual_custom_tokenizer_results.csv\")\n",
    "\n",
    "# if not os.path.exists(results_log_file):\n",
    "#     with open(results_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow([\"Model_ID\", \"Base_Model\", \"Tokenizer_Size\", \"Tokenizer_Type\", \n",
    "#                         \"Total_Languages\", \"Total_Train_Samples\", \"Total_Test_Samples\", \n",
    "#                         \"Custom_Vocab_Size\", \"Overall_BLEU\", \"Overall_Exact_Match\", \n",
    "#                         \"Avg_Pred_Length\", \"Avg_Label_Length\", \"Empty_Predictions\", \n",
    "#                         \"Training_Status\", \"Notes\"])\n",
    "\n",
    "# print(\"🚀 Starting multilingual training with 9 CUSTOM TOKENIZERS...\")\n",
    "# print(f\"📊 Training approach: ONE model per tokenizer handling ALL {len(languages)} languages\")\n",
    "\n",
    "# # Main training loop: 3 sizes × 3 types = 9 models total\n",
    "# for size in tokenizer_sizes:\n",
    "#     for tok_type in tokenizer_types:\n",
    "#         # Path to custom tokenizer\n",
    "#         tokenizer_path = f\"vocab_final/vocab_final{size}/{tok_type}\"\n",
    "#         model_id = f\"multilingual_{size}_{tok_type}\"\n",
    "        \n",
    "#         print(f\"\\n{'='*100}\")\n",
    "#         print(f\"🚀 Training Model {tokenizer_sizes.index(size)*3 + tokenizer_types.index(tok_type) + 1}/9\")\n",
    "#         print(f\"🔧 Custom Tokenizer: {size}_{tok_type}\")\n",
    "#         print(f\"📦 Base Model: {MODEL_CONFIG['model_name']}\")\n",
    "#         print(f\"🌍 Target Languages: {list(languages.keys())} (ALL SIMULTANEOUSLY)\")\n",
    "        \n",
    "#         # Initialize variables\n",
    "#         model = None\n",
    "#         tokenizer = None\n",
    "#         trainer = None\n",
    "        \n",
    "#         try:\n",
    "#             # Load and setup custom tokenizer\n",
    "#             print(f\"🔧 Loading custom tokenizer: {tokenizer_path}\")\n",
    "#             tokenizer = setup_custom_tokenizer_for_mbart(tokenizer_path)\n",
    "            \n",
    "#             print(f\"✅ Custom tokenizer loaded successfully!\")\n",
    "#             print(f\"📊 Custom vocab size: {len(tokenizer)}\")\n",
    "#             print(f\"🔑 Special tokens - BOS: {tokenizer.bos_token_id}, EOS: {tokenizer.eos_token_id}, PAD: {tokenizer.pad_token_id}\")\n",
    "            \n",
    "#             # Load base multilingual model\n",
    "#             print(\"🤖 Loading base multilingual mBART model...\")\n",
    "#             model = MBartForConditionalGeneration.from_pretrained(MODEL_CONFIG[\"model_name\"])\n",
    "            \n",
    "#             # Configure model for custom tokenizer\n",
    "#             model = configure_model_for_custom_tokenizer(model, tokenizer)\n",
    "            \n",
    "#             print(f\"✅ Model configured with custom tokenizer!\")\n",
    "            \n",
    "#             # Use COMPLETE multilingual dataset (all languages together)\n",
    "#             print(f\"📊 Using complete multilingual dataset:\")\n",
    "#             print(f\"   • Train samples: {len(full_dataset['train'])}\")\n",
    "#             print(f\"   • Test samples: {len(full_dataset['test'])}\")\n",
    "#             print(f\"   • Languages: {len(languages)} ({list(languages.keys())})\")\n",
    "            \n",
    "#             # FIXED: Preprocessing function for multilingual data with custom tokenizer\n",
    "#             def preprocess_multilingual_custom(examples):\n",
    "#                 \"\"\"Preprocess multilingual data with custom tokenizer - FIXED VERSION\"\"\"\n",
    "#                 sources = []\n",
    "#                 targets = []\n",
    "                \n",
    "#                 # Handle both single examples and batches\n",
    "#                 if not isinstance(examples[\"translation\"], list):\n",
    "#                     examples = {\n",
    "#                         \"translation\": [examples[\"translation\"]], \n",
    "#                         \"language\": [examples[\"language\"]]\n",
    "#                     }\n",
    "                \n",
    "#                 for translation, lang in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "#                     if isinstance(translation, dict) and lang in languages:\n",
    "#                         source = translation.get(SRC_LANG, \"\")\n",
    "#                         target = translation.get(lang, \"\")\n",
    "                        \n",
    "#                         if source and target:\n",
    "#                             # Add language information to source for multilingual training\n",
    "#                             source_formatted = f\"translate English to {languages[lang]}: {source}\"\n",
    "#                             sources.append(source_formatted)\n",
    "#                             targets.append(target)\n",
    "                \n",
    "#                 if not sources or not targets:\n",
    "#                     return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "                \n",
    "#                 # Tokenize with custom tokenizer\n",
    "#                 max_length = 128\n",
    "                \n",
    "#                 model_inputs = tokenizer(\n",
    "#                     sources,\n",
    "#                     max_length=max_length,\n",
    "#                     truncation=True,\n",
    "#                     padding=\"max_length\",\n",
    "#                     return_tensors=None,\n",
    "#                     return_token_type_ids=False\n",
    "#                 )\n",
    "#                 model_inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "#                 labels = tokenizer(\n",
    "#                     targets,\n",
    "#                     max_length=max_length,\n",
    "#                     truncation=True,\n",
    "#                     padding=\"max_length\",\n",
    "#                     return_tensors=None,\n",
    "#                     return_token_type_ids=False\n",
    "#                 )\n",
    "#                 labels.pop(\"token_type_ids\", None)\n",
    "\n",
    "#                 # CRITICAL FIX: Ensure EOS token is present in labels\n",
    "#                 processed_labels = []\n",
    "#                 for label_seq in labels[\"input_ids\"]:\n",
    "#                     # Remove padding tokens first\n",
    "#                     label_tokens = [token for token in label_seq if token != tokenizer.pad_token_id]\n",
    "                    \n",
    "#                     # Ensure EOS token is at the end (required for mBART)\n",
    "#                     if not label_tokens or label_tokens[-1] != tokenizer.eos_token_id:\n",
    "#                         label_tokens.append(tokenizer.eos_token_id)\n",
    "                    \n",
    "#                     # Pad to max_length and replace padding with -100\n",
    "#                     while len(label_tokens) < max_length:\n",
    "#                         label_tokens.append(-100)\n",
    "                    \n",
    "#                     # Truncate if too long\n",
    "#                     label_tokens = label_tokens[:max_length]\n",
    "#                     processed_labels.append(label_tokens)\n",
    "                \n",
    "#                 model_inputs[\"labels\"] = processed_labels\n",
    "#                 return model_inputs\n",
    "            \n",
    "#             # Preprocess complete multilingual dataset\n",
    "#             print(\"⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\")\n",
    "#             processed_dataset = full_dataset.map(\n",
    "#                 preprocess_multilingual_custom,\n",
    "#                 batched=True,\n",
    "#                 remove_columns=full_dataset[\"train\"].column_names,\n",
    "#                 desc=f\"Preprocessing with {size}_{tok_type}\",\n",
    "#                 batch_size=100\n",
    "#             )\n",
    "            \n",
    "#             train_dataset = processed_dataset[\"train\"]\n",
    "#             eval_dataset = processed_dataset[\"test\"]\n",
    "            \n",
    "#             # Filter out empty examples\n",
    "#             def filter_empty(example):\n",
    "#                 return (len(example[\"input_ids\"]) > 0 and \n",
    "#                        len(example[\"labels\"]) > 0 and\n",
    "#                        any(label != -100 for label in example[\"labels\"]))  # Ensure non-empty labels\n",
    "            \n",
    "#             train_dataset = train_dataset.filter(filter_empty)\n",
    "#             eval_dataset = eval_dataset.filter(filter_empty)\n",
    "            \n",
    "#             print(f\"✅ Preprocessed multilingual dataset:\")\n",
    "#             print(f\"   • Train samples: {len(train_dataset)}\")\n",
    "#             print(f\"   • Eval samples: {len(eval_dataset)}\")\n",
    "            \n",
    "#             if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "#                 print(\"❌ No valid samples after preprocessing\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Setup training directory\n",
    "#             output_dir = f\"./MT_models_multilingual_custom_tokenizers/{model_id}\"\n",
    "#             os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "#             # Training arguments for multilingual model\n",
    "#             training_args = Seq2SeqTrainingArguments(\n",
    "#                 output_dir=output_dir,\n",
    "#                 num_train_epochs=3,\n",
    "#                 per_device_train_batch_size=4,  # Reduced batch size for stability\n",
    "#                 per_device_eval_batch_size=4,\n",
    "#                 gradient_accumulation_steps=4,  # Increased to maintain effective batch size\n",
    "#                 learning_rate=3e-5,  # Slightly lower learning rate\n",
    "#                 weight_decay=0.01,\n",
    "#                 warmup_steps=500,\n",
    "#                 eval_strategy=\"epoch\",\n",
    "#                 save_strategy=\"epoch\",\n",
    "#                 save_total_limit=2,\n",
    "#                 logging_steps=100,\n",
    "#                 report_to=\"none\",\n",
    "#                 predict_with_generate=True,\n",
    "#                 generation_max_length=128,\n",
    "#                 generation_num_beams=2,\n",
    "#                 fp16=torch.cuda.is_available(),\n",
    "#                 load_best_model_at_end=True,\n",
    "#                 metric_for_best_model=\"bleu\",\n",
    "#                 greater_is_better=True,\n",
    "#                 dataloader_num_workers=0,\n",
    "#                 remove_unused_columns=False,\n",
    "#                 ignore_data_skip=True,  # Skip problematic data points\n",
    "#             )\n",
    "            \n",
    "#             # FIXED: Data collator with proper configuration\n",
    "#             data_collator = DataCollatorForSeq2Seq(\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 model=model,\n",
    "#                 padding=True,\n",
    "#                 pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "            \n",
    "#             # Trainer\n",
    "#             trainer = Seq2SeqTrainer(\n",
    "#                 model=model,\n",
    "#                 args=training_args,\n",
    "#                 train_dataset=train_dataset,\n",
    "#                 eval_dataset=eval_dataset,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 data_collator=data_collator,\n",
    "#                 compute_metrics=compute_multilingual_bleu\n",
    "#             )\n",
    "            \n",
    "#             # Train multilingual model\n",
    "#             print(\"🏋️  Starting multilingual training with custom tokenizer...\")\n",
    "#             trainer.train()\n",
    "            \n",
    "#             # Evaluate\n",
    "#             print(\"📊 Final multilingual evaluation...\")\n",
    "#             eval_results = trainer.evaluate()\n",
    "            \n",
    "#             # Save model and custom tokenizer\n",
    "#             print(\"💾 Saving multilingual model and custom tokenizer...\")\n",
    "#             trainer.save_model()\n",
    "#             tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "#             # Log results\n",
    "#             overall_bleu = eval_results.get(\"eval_bleu\", 0.0)\n",
    "#             overall_exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "#             avg_pred_len = eval_results.get(\"eval_avg_pred_length\", 0.0)\n",
    "#             avg_label_len = eval_results.get(\"eval_avg_label_length\", 0.0)\n",
    "#             empty_preds = eval_results.get(\"eval_empty_predictions\", 0.0)\n",
    "            \n",
    "#             with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#                 writer = csv.writer(csvfile)\n",
    "#                 writer.writerow([\n",
    "#                     model_id, MODEL_CONFIG[\"model_name\"], size, tok_type,\n",
    "#                     len(languages), len(train_dataset), len(eval_dataset), len(tokenizer),\n",
    "#                     round(overall_bleu, 4), round(overall_exact_match, 4), \n",
    "#                     round(avg_pred_len, 2), round(avg_label_len, 2),\n",
    "#                     round(empty_preds, 4), \"SUCCESS\", \n",
    "#                     f\"Multilingual model with custom {size}_{tok_type} tokenizer\"\n",
    "#                 ])\n",
    "            \n",
    "#             print(f\"✅ Completed: {model_id}\")\n",
    "#             print(f\"📈 Overall BLEU Score: {overall_bleu:.4f}\")\n",
    "#             print(f\"🎯 Overall Exact Match: {overall_exact_match:.4f}\")\n",
    "            \n",
    "#             # Quick translation tests for different languages\n",
    "#             print(f\"\\n🧪 Quick multilingual translation tests:\")\n",
    "#             test_input = \"Hello, how are you today?\"\n",
    "            \n",
    "#             for test_lang in [\"ar\", \"zh\", \"hi\"]:  # Test 3 languages\n",
    "#                 formatted_input = f\"translate English to {languages[test_lang]}: {test_input}\"\n",
    "#                 inputs = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, max_length=128, truncation=True)\n",
    "\n",
    "#                 # Remove token_type_ids and move to same device\n",
    "#                 inputs = {k: v for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "#                 device = next(model.parameters()).device\n",
    "#                 inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     outputs = model.generate(\n",
    "#                         **inputs,\n",
    "#                         max_length=50,\n",
    "#                         num_beams=2,\n",
    "#                         early_stopping=True,\n",
    "#                         do_sample=False,\n",
    "#                         forced_eos_token_id=tokenizer.eos_token_id\n",
    "#                     )\n",
    "\n",
    "                \n",
    "#                 translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#                 print(f\"  {test_lang} ({languages[test_lang]}): {translation}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed to train {model_id}: {str(e)}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "            \n",
    "#             with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#                 writer = csv.writer(csvfile)\n",
    "#                 writer.writerow([\n",
    "#                     model_id, MODEL_CONFIG.get(\"model_name\", \"\"), size, tok_type,\n",
    "#                     len(languages), 0, 0, 0, 0, 0, 0, 0, 0, \"TRAINING_FAILED\", str(e)[:100]\n",
    "#                 ])\n",
    "        \n",
    "#         finally:\n",
    "#             # Cleanup\n",
    "#             if model is not None:\n",
    "#                 del model\n",
    "#             if trainer is not None:\n",
    "#                 del trainer\n",
    "#             torch.cuda.empty_cache()\n",
    "#             gc.collect()\n",
    "\n",
    "# print(\"\\n🎉 Multilingual training with custom tokenizers completed!\")\n",
    "# print(f\"📋 Results saved to: {results_log_file}\")\n",
    "# print(f\"🔢 Total models trained: 9 (3 sizes × 3 types)\")\n",
    "# print(f\"🌍 Each model handles all {len(languages)} languages simultaneously\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34e91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading complete multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "🌍 All languages included: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr']\n",
      "🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\n",
      "📊 Training approach: ONE model per tokenizer handling ALL 9 languages\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 1/9\n",
      "🔧 Custom Tokenizer: small_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 15002\n",
      "🔑 Special tokens - EOS: 15001, PAD: 0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 15002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 12.7906, 'grad_norm': 75.5894546508789, 'learning_rate': 4.079497907949791e-06, 'epoch': 0.02617287181836027}\n",
      "{'loss': 9.4366, 'grad_norm': 10.594938278198242, 'learning_rate': 9.205020920502094e-06, 'epoch': 0.05234574363672054}\n",
      "{'loss': 8.6657, 'grad_norm': 62.03167724609375, 'learning_rate': 1.4435146443514645e-05, 'epoch': 0.0785186154550808}\n",
      "{'loss': 8.3478, 'grad_norm': 40.246273040771484, 'learning_rate': 1.96652719665272e-05, 'epoch': 0.10469148727344108}\n",
      "{'loss': 7.9785, 'grad_norm': 6.261781692504883, 'learning_rate': 2.489539748953975e-05, 'epoch': 0.13086435909180136}\n",
      "{'loss': 7.55, 'grad_norm': 11.229682922363281, 'learning_rate': 3.0125523012552304e-05, 'epoch': 0.1570372309101616}\n",
      "{'loss': 7.3127, 'grad_norm': 5.195091247558594, 'learning_rate': 3.5355648535564854e-05, 'epoch': 0.1832101027285219}\n",
      "{'loss': 7.0342, 'grad_norm': 6.228858470916748, 'learning_rate': 4.0585774058577406e-05, 'epoch': 0.20938297454688215}\n",
      "{'loss': 6.738, 'grad_norm': 5.553564071655273, 'learning_rate': 4.581589958158996e-05, 'epoch': 0.23555584636524243}\n",
      "{'loss': 6.5568, 'grad_norm': 4.986939907073975, 'learning_rate': 5.104602510460251e-05, 'epoch': 0.2617287181836027}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 528\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# Train multilingual model\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏋️  Starting multilingual training with compatible model-tokenizer pair...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 528\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Final multilingual evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:2237\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:2660\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2660\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:3133\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3131\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3133\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3134\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:3082\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3082\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3085\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:4249\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4246\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4248\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4249\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4259\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:4444\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4441\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4443\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4444\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4445\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4446\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4448\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer_seq2seq.py:327\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m summon_full_params_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    321\u001b[0m     FullyShardedDataParallel\u001b[38;5;241m.\u001b[39msummon_full_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, FullyShardedDataParallel)\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summon_full_params_context:\n\u001b[1;32m--> 327\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\generation\\utils.py:2652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2645\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2646\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2647\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2648\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2649\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2650\u001b[0m     )\n\u001b[0;32m   2651\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2652\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2653\u001b[0m         input_ids,\n\u001b[0;32m   2654\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2655\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2656\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2657\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2658\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2659\u001b[0m     )\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2662\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   2663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2665\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\generation\\utils.py:4097\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   4094\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   4095\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m-> 4097\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   4100\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   4101\u001b[0m     model_outputs,\n\u001b[0;32m   4102\u001b[0m     model_kwargs,\n\u001b[0;32m   4103\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   4104\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1471\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1467\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1468\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1469\u001b[0m         )\n\u001b[1;32m-> 1471\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1490\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1491\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1288\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1282\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1283\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1284\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1285\u001b[0m     )\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1288\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1115\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1127\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:413\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    410\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    412\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    422\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:235\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m curr_past_key_value\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(current_states)\n\u001b[0;32m    237\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mkv_input_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-----Working but slow\n",
    "# import os\n",
    "# import csv\n",
    "# import torch\n",
    "# import warnings\n",
    "# import gc\n",
    "# from datasets import load_from_disk\n",
    "# from transformers import (\n",
    "#     EncoderDecoderModel,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "#     Seq2SeqTrainer,\n",
    "#     Seq2SeqTrainingArguments,\n",
    "#     logging as hf_logging,\n",
    "#     MBartForConditionalGeneration,\n",
    "#     MBart50TokenizerFast,\n",
    "#     T5ForConditionalGeneration,\n",
    "#     T5TokenizerFast,\n",
    "#     BertTokenizerFast,\n",
    "#     GPT2TokenizerFast,\n",
    "#     BartForConditionalGeneration\n",
    "# )\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# import numpy as np\n",
    "\n",
    "# # Suppress warnings\n",
    "# hf_logging.set_verbosity_error()\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # FIXED: Model-Tokenizer Compatibility Mapping\n",
    "# MODEL_TOKENIZER_CONFIGS = {\n",
    "#     \"hf_bpe_hf\": {\n",
    "#         \"base_model\": \"facebook/bart-large\",  # BART uses BPE\n",
    "#         \"model_class\": BartForConditionalGeneration,\n",
    "#         \"description\": \"BART with BPE tokenization\"\n",
    "#     },\n",
    "#     \"hf_wordpiece_hf\": {\n",
    "#         \"base_model\": \"google/mt5-base\",  # mT5 can work with WordPiece\n",
    "#         \"model_class\": T5ForConditionalGeneration,\n",
    "#         \"description\": \"mT5 with WordPiece tokenization\"\n",
    "#     },\n",
    "#     \"sp_unigram_hf\": {\n",
    "#         \"base_model\": \"google/mt5-base\",  # mT5 uses SentencePiece Unigram\n",
    "#         \"model_class\": T5ForConditionalGeneration,\n",
    "#         \"description\": \"mT5 with SentencePiece Unigram\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Custom tokenizer settings\n",
    "# tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
    "# tokenizer_types = [\"hf_bpe_hf\", \"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
    "\n",
    "# # Languages for evaluation\n",
    "# languages = {\n",
    "#     \"yo\": \"Yoruba\",\n",
    "#     \"ar\": \"Arabic\", \n",
    "#     \"zh\": \"Chinese\",\n",
    "#     \"ru\": \"Russian\",\n",
    "#     \"hi\": \"Hindi\",\n",
    "#     \"ja\": \"Japanese\",\n",
    "#     \"swa\": \"Swahili\",\n",
    "#     \"bn\": \"Bengali\",\n",
    "#     \"tr\": \"Turkish\"\n",
    "# }\n",
    "# SRC_LANG = \"en\"\n",
    "\n",
    "# # Load complete multilingual dataset\n",
    "# dataset_path = \"balanced_mt_dataset\"\n",
    "# print(\"📦 Loading complete multilingual dataset...\")\n",
    "# full_dataset = load_from_disk(dataset_path)\n",
    "# print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "# print(f\"🌍 All languages included: {list(languages.keys())}\")\n",
    "\n",
    "# # Enhanced BLEU computation with progress tracking\n",
    "# def compute_multilingual_bleu(eval_pred):\n",
    "#     \"\"\"Multilingual BLEU computation with progress tracking\"\"\"\n",
    "#     import time\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     predictions, labels = eval_pred\n",
    "    \n",
    "#     if len(predictions.shape) == 3:\n",
    "#         predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "#     decoded_preds = []\n",
    "#     decoded_labels = []\n",
    "    \n",
    "#     total_samples = len(predictions)\n",
    "#     print(f\"🔄 Starting evaluation of {total_samples} samples...\")\n",
    "    \n",
    "#     # Process in chunks with progress updates\n",
    "#     chunk_size = 50\n",
    "#     for i in range(0, total_samples, chunk_size):\n",
    "#         end_idx = min(i + chunk_size, total_samples)\n",
    "#         chunk_preds = predictions[i:end_idx]\n",
    "#         chunk_labels = labels[i:end_idx]\n",
    "        \n",
    "#         try:\n",
    "#             for pred, label in zip(chunk_preds, chunk_labels):\n",
    "#                 # CRITICAL FIX: Handle negative token IDs properly\n",
    "#                 # Filter out negative IDs and pad tokens before decoding\n",
    "#                 pred_clean = [token for token in pred if token >= 0 and token < len(tokenizer)]\n",
    "#                 label_clean = [token for token in label if token != -100 and token >= 0 and token < len(tokenizer)]\n",
    "                \n",
    "#                 try:\n",
    "#                     decoded_pred = tokenizer.decode(pred_clean, skip_special_tokens=True).strip() if pred_clean else \"\"\n",
    "#                     decoded_label = tokenizer.decode(label_clean, skip_special_tokens=True).strip() if label_clean else \"\"\n",
    "#                 except Exception as e:\n",
    "#                     # Fallback for any decode errors\n",
    "#                     decoded_pred = \"\"\n",
    "#                     decoded_label = \"\"\n",
    "                \n",
    "#                 decoded_preds.append(decoded_pred)\n",
    "#                 decoded_labels.append(decoded_label)\n",
    "            \n",
    "#             # Progress update every chunk\n",
    "#             if (i + chunk_size) % (chunk_size * 5) == 0 or end_idx == total_samples:\n",
    "#                 elapsed = time.time() - start_time\n",
    "#                 print(f\"📊 Evaluation progress: {end_idx}/{total_samples} samples ({elapsed:.1f}s elapsed)\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️  Batch decode failed for chunk {i}-{end_idx}: {str(e)}\")\n",
    "#             # Add empty strings for failed batch\n",
    "#             for _ in range(end_idx - i):\n",
    "#                 decoded_preds.append(\"\")\n",
    "#                 decoded_labels.append(\"\")\n",
    "    \n",
    "#     # Compute BLEU with smoothing\n",
    "#     print(\"🧮 Computing BLEU scores...\")\n",
    "#     smoothing = SmoothingFunction().method1\n",
    "#     bleu_scores = []\n",
    "#     exact_matches = 0\n",
    "    \n",
    "#     for idx, (pred, label) in enumerate(zip(decoded_preds, decoded_labels)):\n",
    "#         if idx % 1000 == 0 and idx > 0:\n",
    "#             print(f\"   BLEU calculation: {idx}/{len(decoded_preds)} samples\")\n",
    "            \n",
    "#         if not pred.strip() or not label.strip():\n",
    "#             bleu_scores.append(0.0)\n",
    "#             continue\n",
    "            \n",
    "#         pred_tokens = pred.split()\n",
    "#         label_tokens = label.split()\n",
    "        \n",
    "#         # Check exact match\n",
    "#         if pred.lower().strip() == label.lower().strip():\n",
    "#             exact_matches += 1\n",
    "        \n",
    "#         if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "#             bleu_scores.append(0.0)\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             bleu = sentence_bleu(\n",
    "#                 [label_tokens], \n",
    "#                 pred_tokens,\n",
    "#                 smoothing_function=smoothing,\n",
    "#                 weights=(0.25, 0.25, 0.25, 0.25)\n",
    "#             )\n",
    "#             bleu_scores.append(bleu)\n",
    "#         except:\n",
    "#             bleu_scores.append(0.0)\n",
    "    \n",
    "#     avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "#     exact_match_ratio = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "    \n",
    "#     total_time = time.time() - start_time\n",
    "#     print(f\"✅ Evaluation complete: BLEU={avg_bleu:.4f}, Exact Match={exact_match_ratio:.4f} ({total_time:.1f}s total)\")\n",
    "    \n",
    "#     return {\n",
    "#         \"bleu\": avg_bleu,\n",
    "#         \"exact_match\": exact_match_ratio,\n",
    "#         \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "#         \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "#         \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "#     }\n",
    "\n",
    "# def setup_custom_tokenizer_for_model(tokenizer_path, model_type):\n",
    "#     \"\"\"Setup custom tokenizer with proper special tokens for specific model\"\"\"\n",
    "#     # Load custom tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "    \n",
    "#     # Model-specific special token configuration\n",
    "#     if model_type == \"bart\":\n",
    "#         special_tokens = {\n",
    "#             'bos_token': '<s>',\n",
    "#             'eos_token': '</s>',\n",
    "#             'sep_token': '</s>',\n",
    "#             'pad_token': '<pad>',\n",
    "#             'unk_token': '<unk>',\n",
    "#             'mask_token': '<mask>'\n",
    "#         }\n",
    "#     elif model_type == \"t5\":\n",
    "#         special_tokens = {\n",
    "#             'pad_token': '<pad>',\n",
    "#             'eos_token': '</s>',\n",
    "#             'unk_token': '<unk>',\n",
    "#             'bos_token': '<pad>',  # T5 doesn't use BOS\n",
    "#             'sep_token': '</s>',\n",
    "#             'mask_token': '<extra_id_0>'\n",
    "#         }\n",
    "#     else:  # Default fallback\n",
    "#         special_tokens = {\n",
    "#             'bos_token': '<s>',\n",
    "#             'eos_token': '</s>',\n",
    "#             'sep_token': '</s>',\n",
    "#             'pad_token': '<pad>',\n",
    "#             'unk_token': '<unk>',\n",
    "#             'mask_token': '<mask>'\n",
    "#         }\n",
    "    \n",
    "#     # Add missing special tokens\n",
    "#     tokens_to_add = {}\n",
    "#     for token_name, token_value in special_tokens.items():\n",
    "#         if getattr(tokenizer, token_name, None) is None:\n",
    "#             tokens_to_add[token_name] = token_value\n",
    "    \n",
    "#     if tokens_to_add:\n",
    "#         tokenizer.add_special_tokens(tokens_to_add)\n",
    "    \n",
    "#     # Ensure we have the required tokens\n",
    "#     assert tokenizer.eos_token is not None, \"EOS token is required\"\n",
    "#     assert tokenizer.pad_token is not None, \"PAD token is required\"\n",
    "    \n",
    "#     return tokenizer\n",
    "\n",
    "# def configure_model_for_custom_tokenizer(model, tokenizer, model_type):\n",
    "#     \"\"\"Configure model for custom tokenizer based on model type\"\"\"\n",
    "#     # Resize embeddings\n",
    "#     print(\"🔄 Resizing model embeddings to match custom tokenizer...\")\n",
    "#     old_vocab_size = model.config.vocab_size\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "#     print(f\"   Vocab size: {old_vocab_size} → {len(tokenizer)}\")\n",
    "    \n",
    "#     # Model-specific configuration\n",
    "#     if model_type == \"bart\":\n",
    "#         model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "#         model.config.pad_token_id = tokenizer.pad_token_id\n",
    "#         model.config.bos_token_id = tokenizer.bos_token_id\n",
    "#         model.config.eos_token_id = tokenizer.eos_token_id\n",
    "#         model.config.sep_token_id = tokenizer.eos_token_id\n",
    "#         model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "#     elif model_type == \"t5\":\n",
    "#         model.config.pad_token_id = tokenizer.pad_token_id\n",
    "#         model.config.eos_token_id = tokenizer.eos_token_id\n",
    "#         model.config.decoder_start_token_id = tokenizer.pad_token_id  # T5 uses pad_token_id as decoder start\n",
    "#         model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "#     # Update generation config if it exists\n",
    "#     if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "#         model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "#         model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "#         model.generation_config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "#         if model_type == \"bart\":\n",
    "#             model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "#             model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "#         elif model_type == \"t5\":\n",
    "#             model.generation_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Setup logging\n",
    "# log_dir = \"./MT_models_multilingual_custom_tokenizers\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# results_log_file = os.path.join(log_dir, \"multilingual_custom_tokenizer_results.csv\")\n",
    "\n",
    "# if not os.path.exists(results_log_file):\n",
    "#     with open(results_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow([\"Model_ID\", \"Base_Model\", \"Tokenizer_Size\", \"Tokenizer_Type\", \n",
    "#                         \"Total_Languages\", \"Total_Train_Samples\", \"Total_Test_Samples\", \n",
    "#                         \"Custom_Vocab_Size\", \"Overall_BLEU\", \"Overall_Exact_Match\", \n",
    "#                         \"Avg_Pred_Length\", \"Avg_Label_Length\", \"Empty_Predictions\", \n",
    "#                         \"Training_Status\", \"Notes\"])\n",
    "\n",
    "# print(\"🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\")\n",
    "# print(f\"📊 Training approach: ONE model per tokenizer handling ALL {len(languages)} languages\")\n",
    "\n",
    "# # Main training loop: 3 sizes × 3 types = 9 models total\n",
    "# for size in tokenizer_sizes:\n",
    "#     for tok_type in tokenizer_types:\n",
    "#         # Get compatible model configuration\n",
    "#         model_config = MODEL_TOKENIZER_CONFIGS[tok_type]\n",
    "        \n",
    "#         # Path to custom tokenizer\n",
    "#         tokenizer_path = f\"vocab_final/vocab_final{size}/{tok_type}\"\n",
    "#         model_id = f\"multilingual_{size}_{tok_type}\"\n",
    "        \n",
    "#         print(f\"\\n{'='*100}\")\n",
    "#         print(f\"🚀 Training Model {tokenizer_sizes.index(size)*3 + tokenizer_types.index(tok_type) + 1}/9\")\n",
    "#         print(f\"🔧 Custom Tokenizer: {size}_{tok_type}\")\n",
    "#         print(f\"📦 Compatible Base Model: {model_config['base_model']}\")\n",
    "#         print(f\"🌍 Target Languages: {list(languages.keys())} (ALL SIMULTANEOUSLY)\")\n",
    "        \n",
    "#         # Initialize variables\n",
    "#         model = None\n",
    "#         tokenizer = None\n",
    "#         trainer = None\n",
    "        \n",
    "#         try:\n",
    "#             # Load and setup custom tokenizer\n",
    "#             print(f\"🔧 Loading custom tokenizer: {tokenizer_path}\")\n",
    "#             model_type = \"bart\" if \"bart\" in model_config['base_model'] else \"t5\"\n",
    "#             tokenizer = setup_custom_tokenizer_for_model(tokenizer_path, model_type)\n",
    "            \n",
    "#             print(f\"✅ Custom tokenizer loaded successfully!\")\n",
    "#             print(f\"📊 Custom vocab size: {len(tokenizer)}\")\n",
    "#             print(f\"🔑 Special tokens - EOS: {tokenizer.eos_token_id}, PAD: {tokenizer.pad_token_id}\")\n",
    "            \n",
    "#             # Load compatible base model\n",
    "#             print(f\"🤖 Loading compatible base model: {model_config['base_model']}\")\n",
    "#             model = model_config['model_class'].from_pretrained(model_config['base_model'])\n",
    "            \n",
    "#             # Configure model for custom tokenizer\n",
    "#             model = configure_model_for_custom_tokenizer(model, tokenizer, model_type)\n",
    "            \n",
    "#             print(f\"✅ Model configured with custom tokenizer!\")\n",
    "            \n",
    "#             # Use COMPLETE multilingual dataset (all languages together)\n",
    "#             print(f\"📊 Using complete multilingual dataset:\")\n",
    "#             print(f\"   • Train samples: {len(full_dataset['train'])}\")\n",
    "#             print(f\"   • Test samples: {len(full_dataset['test'])}\")\n",
    "#             print(f\"   • Languages: {len(languages)} ({list(languages.keys())})\")\n",
    "            \n",
    "#             # IMPROVED: Preprocessing function for multilingual data\n",
    "#             def preprocess_multilingual_improved(examples):\n",
    "#                 \"\"\"Improved preprocessing for multilingual data with model-specific formatting\"\"\"\n",
    "#                 sources = []\n",
    "#                 targets = []\n",
    "                \n",
    "#                 # Handle both single examples and batches\n",
    "#                 if not isinstance(examples[\"translation\"], list):\n",
    "#                     examples = {\n",
    "#                         \"translation\": [examples[\"translation\"]], \n",
    "#                         \"language\": [examples[\"language\"]]\n",
    "#                     }\n",
    "                \n",
    "#                 for translation, lang in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "#                     if isinstance(translation, dict) and lang in languages:\n",
    "#                         source = translation.get(SRC_LANG, \"\")\n",
    "#                         target = translation.get(lang, \"\")\n",
    "                        \n",
    "#                         if source.strip() and target.strip():  # Ensure non-empty\n",
    "#                             # Model-specific formatting\n",
    "#                             if model_type == \"t5\":\n",
    "#                                 # T5 style: \"translate English to German: Hello\"\n",
    "#                                 source_formatted = f\"translate English to {languages[lang]}: {source}\"\n",
    "#                             else:  # BART style\n",
    "#                                 # BART style with language token\n",
    "#                                 source_formatted = f\"{source} </s> {lang}_XX\"  # Add language code\n",
    "                            \n",
    "#                             sources.append(source_formatted)\n",
    "#                             targets.append(target)\n",
    "                \n",
    "#                 if not sources or not targets:\n",
    "#                     return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "                \n",
    "#                 # Tokenize with custom tokenizer\n",
    "#                 max_length = 256  # INCREASED for better performance\n",
    "                \n",
    "#                 # Input tokenization - REMOVE token_type_ids\n",
    "#                 model_inputs = tokenizer(\n",
    "#                     sources,\n",
    "#                     max_length=max_length,\n",
    "#                     truncation=True,\n",
    "#                     padding=\"max_length\",\n",
    "#                     return_tensors=None,\n",
    "#                     return_token_type_ids=False  # CRITICAL FIX\n",
    "#                 )\n",
    "#                 # Ensure token_type_ids is removed\n",
    "#                 model_inputs.pop(\"token_type_ids\", None)\n",
    "                \n",
    "#                 # Target tokenization  \n",
    "#                 with tokenizer.as_target_tokenizer():\n",
    "#                     labels = tokenizer(\n",
    "#                         targets,\n",
    "#                         max_length=max_length,\n",
    "#                         truncation=True,\n",
    "#                         padding=\"max_length\",\n",
    "#                         return_tensors=None,\n",
    "#                         return_token_type_ids=False  # CRITICAL FIX\n",
    "#                     )\n",
    "#                 # Ensure token_type_ids is removed\n",
    "#                 labels.pop(\"token_type_ids\", None)\n",
    "                \n",
    "#                 # IMPROVED: Label processing with proper EOS handling\n",
    "#                 processed_labels = []\n",
    "#                 for label_seq in labels[\"input_ids\"]:\n",
    "#                     # Find actual end of sequence (before padding)\n",
    "#                     try:\n",
    "#                         pad_start = label_seq.index(tokenizer.pad_token_id)\n",
    "#                         actual_tokens = label_seq[:pad_start]\n",
    "#                     except ValueError:\n",
    "#                         actual_tokens = label_seq\n",
    "                    \n",
    "#                     # Ensure EOS token at end\n",
    "#                     if actual_tokens and actual_tokens[-1] != tokenizer.eos_token_id:\n",
    "#                         actual_tokens.append(tokenizer.eos_token_id)\n",
    "#                     elif not actual_tokens:\n",
    "#                         actual_tokens = [tokenizer.eos_token_id]\n",
    "                    \n",
    "#                     # Create final sequence with -100 for padding\n",
    "#                     final_labels = actual_tokens + [-100] * (max_length - len(actual_tokens))\n",
    "#                     final_labels = final_labels[:max_length]  # Ensure correct length\n",
    "                    \n",
    "#                     processed_labels.append(final_labels)\n",
    "                \n",
    "#                 model_inputs[\"labels\"] = processed_labels\n",
    "#                 return model_inputs\n",
    "            \n",
    "#             # Preprocess complete multilingual dataset\n",
    "#             print(\"⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\")\n",
    "#             processed_dataset = full_dataset.map(\n",
    "#                 preprocess_multilingual_improved,\n",
    "#                 batched=True,\n",
    "#                 remove_columns=full_dataset[\"train\"].column_names,\n",
    "#                 desc=f\"Preprocessing with {size}_{tok_type}\",\n",
    "#                 batch_size=50,  # Smaller batch for stability\n",
    "#                 num_proc=1  # Single process to avoid issues\n",
    "#             )\n",
    "            \n",
    "#             train_dataset = processed_dataset[\"train\"]\n",
    "#             eval_dataset = processed_dataset[\"test\"]\n",
    "            \n",
    "#             # Filter out empty examples\n",
    "#             def filter_valid_examples(example):\n",
    "#                 return (\n",
    "#                     len(example[\"input_ids\"]) > 0 and \n",
    "#                     len(example[\"labels\"]) > 0 and\n",
    "#                     any(label != -100 for label in example[\"labels\"]) and\n",
    "#                     sum(1 for token in example[\"input_ids\"] if token != tokenizer.pad_token_id) > 0\n",
    "#                 )\n",
    "            \n",
    "#             train_dataset = train_dataset.filter(filter_valid_examples)\n",
    "#             eval_dataset = eval_dataset.filter(filter_valid_examples)\n",
    "            \n",
    "#             # Use smaller eval dataset to avoid memory issues\n",
    "#             print(f\"✅ Preprocessed multilingual dataset:\")\n",
    "#             print(f\"   • Train samples: {len(train_dataset)}\")\n",
    "#             print(f\"   • Eval samples: {len(eval_dataset)}\")\n",
    "            \n",
    "#             # REDUCE eval dataset size to avoid hanging\n",
    "#             if len(eval_dataset) > 1000:\n",
    "#                 eval_dataset = eval_dataset.select(range(1000))\n",
    "#                 print(f\"   • Reduced eval samples to: {len(eval_dataset)} (to avoid memory issues)\")\n",
    "            \n",
    "#             if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "#                 print(\"❌ No valid samples after preprocessing\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Setup training directory\n",
    "#             output_dir = f\"./MT_models_multilingual_custom_tokenizers/{model_id}\"\n",
    "#             os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "#             # IMPROVED: Training arguments with simpler evaluation\n",
    "#             training_args = Seq2SeqTrainingArguments(\n",
    "#                 output_dir=output_dir,\n",
    "#                 num_train_epochs=5,  # More epochs for custom tokenizers\n",
    "#                 per_device_train_batch_size=2,  # Smaller batch size\n",
    "#                 per_device_eval_batch_size=1,  # REDUCED eval batch size\n",
    "#                 gradient_accumulation_steps=8,  # Higher accumulation\n",
    "#                 learning_rate=1e-4,  # Lower learning rate for stability\n",
    "#                 weight_decay=0.01,\n",
    "#                 warmup_ratio=0.1,  # Warmup as ratio\n",
    "#                 eval_strategy=\"steps\",  # Change to steps-based evaluation\n",
    "#                 eval_steps=500,  # Evaluate every 500 steps instead of epoch end\n",
    "#                 save_strategy=\"epoch\",\n",
    "#                 save_total_limit=2,\n",
    "#                 logging_steps=50,\n",
    "#                 report_to=\"none\",\n",
    "#                 predict_with_generate=True,\n",
    "#                 generation_max_length=128,  # REDUCED generation length\n",
    "#                 generation_num_beams=2,  # REDUCED beams\n",
    "#                 fp16=torch.cuda.is_available(),\n",
    "#                 load_best_model_at_end=False,  # DISABLED to avoid issues\n",
    "#                 dataloader_num_workers=0,\n",
    "#                 remove_unused_columns=False,  # CRITICAL: Keep all columns\n",
    "#                 ignore_data_skip=True,\n",
    "#                 label_smoothing_factor=0.1,  # Label smoothing\n",
    "#                 max_grad_norm=1.0,  # Gradient clipping\n",
    "#                 dataloader_pin_memory=False,  # Disable pin memory\n",
    "#                 skip_memory_metrics=True,  # Skip memory tracking\n",
    "                \n",
    "                \n",
    "#             )\n",
    "            \n",
    "#             # Data collator with explicit token_type_ids handling\n",
    "#             data_collator = DataCollatorForSeq2Seq(\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 model=model,\n",
    "#                 padding=True,\n",
    "#                 pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 label_pad_token_id=-100\n",
    "#             )\n",
    "            \n",
    "#             # CRITICAL FIX: Custom data collator that removes token_type_ids\n",
    "#             class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "#                 def __call__(self, features):\n",
    "#                     batch = super().__call__(features)\n",
    "#                     # Remove token_type_ids if present\n",
    "#                     batch.pop(\"token_type_ids\", None)\n",
    "#                     return batch\n",
    "            \n",
    "#             data_collator = CustomDataCollator(\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 model=model,\n",
    "#                 padding=True,\n",
    "#                 pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 label_pad_token_id=-100\n",
    "#             )\n",
    "            \n",
    "#             # Trainer\n",
    "#             trainer = Seq2SeqTrainer(\n",
    "#                 model=model,\n",
    "#                 args=training_args,\n",
    "#                 train_dataset=train_dataset,\n",
    "#                 eval_dataset=eval_dataset,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 data_collator=data_collator,\n",
    "#                 compute_metrics=compute_multilingual_bleu\n",
    "#             )\n",
    "            \n",
    "#             # Train multilingual model\n",
    "#             print(\"🏋️  Starting multilingual training with compatible model-tokenizer pair...\")\n",
    "#             trainer.train()\n",
    "            \n",
    "#             # Evaluate\n",
    "#             print(\"📊 Final multilingual evaluation...\")\n",
    "#             eval_results = trainer.evaluate()\n",
    "            \n",
    "#             # Save model and custom tokenizer\n",
    "#             print(\"💾 Saving multilingual model and custom tokenizer...\")\n",
    "#             trainer.save_model()\n",
    "#             tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "#             # Log results\n",
    "#             overall_bleu = eval_results.get(\"eval_bleu\", 0.0)\n",
    "#             overall_exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "#             avg_pred_len = eval_results.get(\"eval_avg_pred_length\", 0.0)\n",
    "#             avg_label_len = eval_results.get(\"eval_avg_label_length\", 0.0)\n",
    "#             empty_preds = eval_results.get(\"eval_empty_predictions\", 0.0)\n",
    "            \n",
    "#             with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#                 writer = csv.writer(csvfile)\n",
    "#                 writer.writerow([\n",
    "#                     model_id, model_config[\"base_model\"], size, tok_type,\n",
    "#                     len(languages), len(train_dataset), len(eval_dataset), len(tokenizer),\n",
    "#                     round(overall_bleu, 4), round(overall_exact_match, 4), \n",
    "#                     round(avg_pred_len, 2), round(avg_label_len, 2),\n",
    "#                     round(empty_preds, 4), \"SUCCESS\", \n",
    "#                     f\"Compatible {model_config['description']} with {size}_{tok_type}\"\n",
    "#                 ])\n",
    "            \n",
    "#             print(f\"✅ Completed: {model_id}\")\n",
    "#             print(f\"📈 Overall BLEU Score: {overall_bleu:.4f}\")\n",
    "#             print(f\"🎯 Overall Exact Match: {overall_exact_match:.4f}\")\n",
    "            \n",
    "#             # Quick translation tests for different languages\n",
    "#             print(f\"\\n🧪 Quick multilingual translation tests:\")\n",
    "#             test_input = \"Hello, how are you today?\"\n",
    "            \n",
    "#             for test_lang in [\"ar\", \"zh\", \"hi\"]:  # Test 3 languages\n",
    "#                 if model_type == \"t5\":\n",
    "#                     formatted_input = f\"translate English to {languages[test_lang]}: {test_input}\"\n",
    "#                 else:  # BART\n",
    "#                     formatted_input = f\"{test_input} </s> {test_lang}_XX\"\n",
    "                    \n",
    "#                 inputs = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, max_length=256, truncation=True)\n",
    "                \n",
    "#                 # Move to device\n",
    "#                 device = next(model.parameters()).device\n",
    "#                 inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "                \n",
    "#                 with torch.no_grad():\n",
    "#                     outputs = model.generate(\n",
    "#                         **inputs,\n",
    "#                         max_length=128,\n",
    "#                         num_beams=4,\n",
    "#                         early_stopping=True,\n",
    "#                         do_sample=False,\n",
    "#                         forced_eos_token_id=tokenizer.eos_token_id\n",
    "#                     )\n",
    "                \n",
    "#                 translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#                 print(f\"  {test_lang} ({languages[test_lang]}): {translation}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Failed to train {model_id}: {str(e)}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "            \n",
    "#             with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#                 writer = csv.writer(csvfile)\n",
    "#                 writer.writerow([\n",
    "#                     model_id, model_config.get(\"base_model\", \"\"), size, tok_type,\n",
    "#                     len(languages), 0, 0, 0, 0, 0, 0, 0, 0, \"TRAINING_FAILED\", str(e)[:100]\n",
    "#                 ])\n",
    "        \n",
    "#         finally:\n",
    "#             # Cleanup\n",
    "#             if model is not None:\n",
    "#                 del model\n",
    "#             if trainer is not None:\n",
    "#                 del trainer\n",
    "#             torch.cuda.empty_cache()\n",
    "#             gc.collect()\n",
    "\n",
    "# print(\"\\n🎉 Multilingual training with compatible model-tokenizer pairs completed!\")\n",
    "# print(f\"📋 Results saved to: {results_log_file}\")\n",
    "# print(f\"🔢 Total models trained: 9 (3 sizes × 3 types)\")\n",
    "# print(f\"🌍 Each model handles all {len(languages)} languages simultaneously\")\n",
    "# print(\"\\n📊 Expected improvements:\")\n",
    "# print(\"• BPE tokenizers → BART models (proper compatibility)\")\n",
    "# print(\"• WordPiece tokenizers → mT5 models (better multilingual support)\")\n",
    "# print(\"• Unigram tokenizers → mT5 models (native compatibility)\")\n",
    "# print(\"• Increased sequence length (256 vs 128)\")\n",
    "# print(\"• Better hyperparameters and training setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f81a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading complete multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "🌍 All languages included: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr']\n",
      "🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\n",
      "📊 Training approach: ONE model per tokenizer handling ALL 9 languages\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 1/9\n",
      "🔧 Custom Tokenizer: small_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 15002\n",
      "🔑 Special tokens - EOS: 15001, PAD: 0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 15002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 11.328, 'grad_norm': 34.87146759033203, 'learning_rate': 2.1354166666666666e-05, 'epoch': 0.02617287181836027}\n",
      "{'loss': 8.7568, 'grad_norm': 6.192747592926025, 'learning_rate': 4.739583333333333e-05, 'epoch': 0.05234574363672054}\n",
      "{'loss': 8.0742, 'grad_norm': 5.292508602142334, 'learning_rate': 7.34375e-05, 'epoch': 0.0785186154550808}\n",
      "{'loss': 7.5425, 'grad_norm': 4.414766788482666, 'learning_rate': 9.947916666666666e-05, 'epoch': 0.10469148727344108}\n",
      "{'loss': 7.2528, 'grad_norm': 4.821774005889893, 'learning_rate': 9.714950552646889e-05, 'epoch': 0.13086435909180136}\n",
      "{'loss': 7.0378, 'grad_norm': 4.3806986808776855, 'learning_rate': 9.424083769633509e-05, 'epoch': 0.1570372309101616}\n",
      "{'loss': 6.8572, 'grad_norm': 3.6972146034240723, 'learning_rate': 9.133216986620129e-05, 'epoch': 0.1832101027285219}\n",
      "{'loss': 6.6867, 'grad_norm': 4.6747331619262695, 'learning_rate': 8.842350203606748e-05, 'epoch': 0.20938297454688215}\n",
      "{'loss': 6.5646, 'grad_norm': 3.990095376968384, 'learning_rate': 8.55148342059337e-05, 'epoch': 0.23555584636524243}\n",
      "{'loss': 6.4902, 'grad_norm': 3.708364248275757, 'learning_rate': 8.260616637579988e-05, 'epoch': 0.2617287181836027}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 529\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# Train multilingual model\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏋️  Starting multilingual training with compatible model-tokenizer pair...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 529\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Final multilingual evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:2237\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:2660\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2660\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:3133\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3131\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3133\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3134\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:3082\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3082\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3085\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:4249\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4246\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4248\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4249\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4259\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:4444\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4441\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4443\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4444\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4445\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4446\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4448\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer_seq2seq.py:327\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m summon_full_params_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    321\u001b[0m     FullyShardedDataParallel\u001b[38;5;241m.\u001b[39msummon_full_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, FullyShardedDataParallel)\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summon_full_params_context:\n\u001b[1;32m--> 327\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\generation\\utils.py:2652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2645\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2646\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2647\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2648\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2649\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2650\u001b[0m     )\n\u001b[0;32m   2651\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2652\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2653\u001b[0m         input_ids,\n\u001b[0;32m   2654\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2655\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2656\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2657\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2658\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2659\u001b[0m     )\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2662\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   2663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2665\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\generation\\utils.py:4097\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   4094\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   4095\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m-> 4097\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   4100\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   4101\u001b[0m     model_outputs,\n\u001b[0;32m   4102\u001b[0m     model_kwargs,\n\u001b[0;32m   4103\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   4104\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1471\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1467\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1468\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1469\u001b[0m         )\n\u001b[1;32m-> 1471\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1490\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1491\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1288\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1282\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1283\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1284\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1285\u001b[0m     )\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1288\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1115\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1127\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:447\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    445\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states))\n\u001b[0;32m    446\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m--> 447\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    449\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##working ... but slow\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    logging as hf_logging,\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    BertTokenizerFast,\n",
    "    GPT2TokenizerFast,\n",
    "    BartForConditionalGeneration\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# FIXED: Model-Tokenizer Compatibility Mapping\n",
    "MODEL_TOKENIZER_CONFIGS = {\n",
    "    \"hf_bpe_hf\": {\n",
    "        \"base_model\": \"facebook/bart-large\",  # BART uses BPE\n",
    "        \"model_class\": BartForConditionalGeneration,\n",
    "        \"description\": \"BART with BPE tokenization\"\n",
    "    },\n",
    "    \"hf_wordpiece_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",  # mT5 can work with WordPiece\n",
    "        \"model_class\": T5ForConditionalGeneration,\n",
    "        \"description\": \"mT5 with WordPiece tokenization\"\n",
    "    },\n",
    "    \"sp_unigram_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",  # mT5 uses SentencePiece Unigram\n",
    "        \"model_class\": T5ForConditionalGeneration,\n",
    "        \"description\": \"mT5 with SentencePiece Unigram\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Custom tokenizer settings\n",
    "tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
    "tokenizer_types = [\"hf_bpe_hf\", \"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
    "\n",
    "# Languages for evaluation\n",
    "languages = {\n",
    "    \"yo\": \"Yoruba\",\n",
    "    \"ar\": \"Arabic\", \n",
    "    \"zh\": \"Chinese\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"swa\": \"Swahili\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"tr\": \"Turkish\"\n",
    "}\n",
    "SRC_LANG = \"en\"\n",
    "\n",
    "# Load complete multilingual dataset\n",
    "dataset_path = \"balanced_mt_dataset\"\n",
    "print(\"📦 Loading complete multilingual dataset...\")\n",
    "full_dataset = load_from_disk(dataset_path)\n",
    "print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "print(f\"🌍 All languages included: {list(languages.keys())}\")\n",
    "\n",
    "# Enhanced BLEU computation with progress tracking\n",
    "def compute_multilingual_bleu(eval_pred):\n",
    "    \"\"\"Multilingual BLEU computation with progress tracking\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if len(predictions.shape) == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    total_samples = len(predictions)\n",
    "    print(f\"🔄 Starting evaluation of {total_samples} samples...\")\n",
    "    \n",
    "    # Process in chunks with progress updates\n",
    "    chunk_size = 50\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        end_idx = min(i + chunk_size, total_samples)\n",
    "        chunk_preds = predictions[i:end_idx]\n",
    "        chunk_labels = labels[i:end_idx]\n",
    "        \n",
    "        try:\n",
    "            for pred, label in zip(chunk_preds, chunk_labels):\n",
    "                # CRITICAL FIX: Handle negative token IDs properly\n",
    "                # Filter out negative IDs and pad tokens before decoding\n",
    "                pred_clean = [token for token in pred if token >= 0 and token < len(tokenizer)]\n",
    "                label_clean = [token for token in label if token != -100 and token >= 0 and token < len(tokenizer)]\n",
    "                \n",
    "                try:\n",
    "                    decoded_pred = tokenizer.decode(pred_clean, skip_special_tokens=True).strip() if pred_clean else \"\"\n",
    "                    decoded_label = tokenizer.decode(label_clean, skip_special_tokens=True).strip() if label_clean else \"\"\n",
    "                except Exception as e:\n",
    "                    # Fallback for any decode errors\n",
    "                    decoded_pred = \"\"\n",
    "                    decoded_label = \"\"\n",
    "                \n",
    "                decoded_preds.append(decoded_pred)\n",
    "                decoded_labels.append(decoded_label)\n",
    "            \n",
    "            # Progress update every chunk\n",
    "            if (i + chunk_size) % (chunk_size * 5) == 0 or end_idx == total_samples:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"📊 Evaluation progress: {end_idx}/{total_samples} samples ({elapsed:.1f}s elapsed)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Batch decode failed for chunk {i}-{end_idx}: {str(e)}\")\n",
    "            # Add empty strings for failed batch\n",
    "            for _ in range(end_idx - i):\n",
    "                decoded_preds.append(\"\")\n",
    "                decoded_labels.append(\"\")\n",
    "    \n",
    "    # Compute BLEU with smoothing\n",
    "    print(\"🧮 Computing BLEU scores...\")\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for idx, (pred, label) in enumerate(zip(decoded_preds, decoded_labels)):\n",
    "        if idx % 1000 == 0 and idx > 0:\n",
    "            print(f\"   BLEU calculation: {idx}/{len(decoded_preds)} samples\")\n",
    "            \n",
    "        if not pred.strip() or not label.strip():\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "        \n",
    "        # Check exact match\n",
    "        if pred.lower().strip() == label.lower().strip():\n",
    "            exact_matches += 1\n",
    "        \n",
    "        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            bleu = sentence_bleu(\n",
    "                [label_tokens], \n",
    "                pred_tokens,\n",
    "                smoothing_function=smoothing,\n",
    "                weights=(0.25, 0.25, 0.25, 0.25)\n",
    "            )\n",
    "            bleu_scores.append(bleu)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match_ratio = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"✅ Evaluation complete: BLEU={avg_bleu:.4f}, Exact Match={exact_match_ratio:.4f} ({total_time:.1f}s total)\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"exact_match\": exact_match_ratio,\n",
    "        \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "        \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "        \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "    }\n",
    "\n",
    "def setup_custom_tokenizer_for_model(tokenizer_path, model_type):\n",
    "    \"\"\"Setup custom tokenizer with proper special tokens for specific model\"\"\"\n",
    "    # Load custom tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "    \n",
    "    # Model-specific special token configuration\n",
    "    if model_type == \"bart\":\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "    elif model_type == \"t5\":\n",
    "        special_tokens = {\n",
    "            'pad_token': '<pad>',\n",
    "            'eos_token': '</s>',\n",
    "            'unk_token': '<unk>',\n",
    "            'bos_token': '<pad>',  # T5 doesn't use BOS\n",
    "            'sep_token': '</s>',\n",
    "            'mask_token': '<extra_id_0>'\n",
    "        }\n",
    "    else:  # Default fallback\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "    \n",
    "    # Add missing special tokens\n",
    "    tokens_to_add = {}\n",
    "    for token_name, token_value in special_tokens.items():\n",
    "        if getattr(tokenizer, token_name, None) is None:\n",
    "            tokens_to_add[token_name] = token_value\n",
    "    \n",
    "    if tokens_to_add:\n",
    "        tokenizer.add_special_tokens(tokens_to_add)\n",
    "    \n",
    "    # Ensure we have the required tokens\n",
    "    assert tokenizer.eos_token is not None, \"EOS token is required\"\n",
    "    assert tokenizer.pad_token is not None, \"PAD token is required\"\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def configure_model_for_custom_tokenizer(model, tokenizer, model_type):\n",
    "    \"\"\"Configure model for custom tokenizer based on model type\"\"\"\n",
    "    # Resize embeddings\n",
    "    print(\"🔄 Resizing model embeddings to match custom tokenizer...\")\n",
    "    old_vocab_size = model.config.vocab_size\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   Vocab size: {old_vocab_size} → {len(tokenizer)}\")\n",
    "    \n",
    "    # Model-specific configuration\n",
    "    if model_type == \"bart\":\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.sep_token_id = tokenizer.eos_token_id\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    elif model_type == \"t5\":\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.decoder_start_token_id = tokenizer.pad_token_id  # T5 uses pad_token_id as decoder start\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Update generation config if it exists\n",
    "    if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        if model_type == \"bart\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "            model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        elif model_type == \"t5\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup logging\n",
    "log_dir = \"./MT_models_multilingual_custom_tokenizers\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "results_log_file = os.path.join(log_dir, \"multilingual_custom_tokenizer_results.csv\")\n",
    "\n",
    "if not os.path.exists(results_log_file):\n",
    "    with open(results_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Model_ID\", \"Base_Model\", \"Tokenizer_Size\", \"Tokenizer_Type\", \n",
    "                        \"Total_Languages\", \"Total_Train_Samples\", \"Total_Test_Samples\", \n",
    "                        \"Custom_Vocab_Size\", \"Overall_BLEU\", \"Overall_Exact_Match\", \n",
    "                        \"Avg_Pred_Length\", \"Avg_Label_Length\", \"Empty_Predictions\", \n",
    "                        \"Training_Status\", \"Notes\"])\n",
    "\n",
    "print(\"🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\")\n",
    "print(f\"📊 Training approach: ONE model per tokenizer handling ALL {len(languages)} languages\")\n",
    "\n",
    "# Main training loop: 3 sizes × 3 types = 9 models total\n",
    "for size in tokenizer_sizes:\n",
    "    for tok_type in tokenizer_types:\n",
    "        # Get compatible model configuration\n",
    "        model_config = MODEL_TOKENIZER_CONFIGS[tok_type]\n",
    "        \n",
    "        # Path to custom tokenizer\n",
    "        tokenizer_path = f\"vocab_final/vocab_final{size}/{tok_type}\"\n",
    "        model_id = f\"multilingual_{size}_{tok_type}\"\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"🚀 Training Model {tokenizer_sizes.index(size)*3 + tokenizer_types.index(tok_type) + 1}/9\")\n",
    "        print(f\"🔧 Custom Tokenizer: {size}_{tok_type}\")\n",
    "        print(f\"📦 Compatible Base Model: {model_config['base_model']}\")\n",
    "        print(f\"🌍 Target Languages: {list(languages.keys())} (ALL SIMULTANEOUSLY)\")\n",
    "        \n",
    "        # Initialize variables\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        trainer = None\n",
    "        \n",
    "        try:\n",
    "            # Load and setup custom tokenizer\n",
    "            print(f\"🔧 Loading custom tokenizer: {tokenizer_path}\")\n",
    "            model_type = \"bart\" if \"bart\" in model_config['base_model'] else \"t5\"\n",
    "            tokenizer = setup_custom_tokenizer_for_model(tokenizer_path, model_type)\n",
    "            \n",
    "            print(f\"✅ Custom tokenizer loaded successfully!\")\n",
    "            print(f\"📊 Custom vocab size: {len(tokenizer)}\")\n",
    "            print(f\"🔑 Special tokens - EOS: {tokenizer.eos_token_id}, PAD: {tokenizer.pad_token_id}\")\n",
    "            \n",
    "            # Load compatible base model\n",
    "            print(f\"🤖 Loading compatible base model: {model_config['base_model']}\")\n",
    "            model = model_config['model_class'].from_pretrained(model_config['base_model'])\n",
    "            \n",
    "            # Configure model for custom tokenizer\n",
    "            model = configure_model_for_custom_tokenizer(model, tokenizer, model_type)\n",
    "            \n",
    "            print(f\"✅ Model configured with custom tokenizer!\")\n",
    "            \n",
    "            # Use COMPLETE multilingual dataset (all languages together)\n",
    "            print(f\"📊 Using complete multilingual dataset:\")\n",
    "            print(f\"   • Train samples: {len(full_dataset['train'])}\")\n",
    "            print(f\"   • Test samples: {len(full_dataset['test'])}\")\n",
    "            print(f\"   • Languages: {len(languages)} ({list(languages.keys())})\")\n",
    "            \n",
    "            # IMPROVED: Preprocessing function for multilingual data\n",
    "            def preprocess_multilingual_improved(examples):\n",
    "                \"\"\"Improved preprocessing for multilingual data with model-specific formatting\"\"\"\n",
    "                sources = []\n",
    "                targets = []\n",
    "                \n",
    "                # Handle both single examples and batches\n",
    "                if not isinstance(examples[\"translation\"], list):\n",
    "                    examples = {\n",
    "                        \"translation\": [examples[\"translation\"]], \n",
    "                        \"language\": [examples[\"language\"]]\n",
    "                    }\n",
    "                \n",
    "                for translation, lang in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "                    if isinstance(translation, dict) and lang in languages:\n",
    "                        source = translation.get(SRC_LANG, \"\")\n",
    "                        target = translation.get(lang, \"\")\n",
    "                        \n",
    "                        if source.strip() and target.strip():  # Ensure non-empty\n",
    "                            # Model-specific formatting\n",
    "                            if model_type == \"t5\":\n",
    "                                # T5 style: \"translate English to German: Hello\"\n",
    "                                source_formatted = f\"translate English to {languages[lang]}: {source}\"\n",
    "                            else:  # BART style\n",
    "                                # BART style with language token\n",
    "                                source_formatted = f\"{source} </s> {lang}_XX\"  # Add language code\n",
    "                            \n",
    "                            sources.append(source_formatted)\n",
    "                            targets.append(target)\n",
    "                \n",
    "                if not sources or not targets:\n",
    "                    return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "                \n",
    "                # Tokenize with custom tokenizer\n",
    "                max_length = 256  # INCREASED for better performance\n",
    "                \n",
    "                # Input tokenization - REMOVE token_type_ids\n",
    "                model_inputs = tokenizer(\n",
    "                    sources,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=None,\n",
    "                    return_token_type_ids=False  # CRITICAL FIX\n",
    "                )\n",
    "                # Ensure token_type_ids is removed\n",
    "                model_inputs.pop(\"token_type_ids\", None)\n",
    "                \n",
    "                # Target tokenization  \n",
    "                with tokenizer.as_target_tokenizer():\n",
    "                    labels = tokenizer(\n",
    "                        targets,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\",\n",
    "                        return_tensors=None,\n",
    "                        return_token_type_ids=False  # CRITICAL FIX\n",
    "                    )\n",
    "                # Ensure token_type_ids is removed\n",
    "                labels.pop(\"token_type_ids\", None)\n",
    "                \n",
    "                # IMPROVED: Label processing with proper EOS handling\n",
    "                processed_labels = []\n",
    "                for label_seq in labels[\"input_ids\"]:\n",
    "                    # Find actual end of sequence (before padding)\n",
    "                    try:\n",
    "                        pad_start = label_seq.index(tokenizer.pad_token_id)\n",
    "                        actual_tokens = label_seq[:pad_start]\n",
    "                    except ValueError:\n",
    "                        actual_tokens = label_seq\n",
    "                    \n",
    "                    # Ensure EOS token at end\n",
    "                    if actual_tokens and actual_tokens[-1] != tokenizer.eos_token_id:\n",
    "                        actual_tokens.append(tokenizer.eos_token_id)\n",
    "                    elif not actual_tokens:\n",
    "                        actual_tokens = [tokenizer.eos_token_id]\n",
    "                    \n",
    "                    # Create final sequence with -100 for padding\n",
    "                    final_labels = actual_tokens + [-100] * (max_length - len(actual_tokens))\n",
    "                    final_labels = final_labels[:max_length]  # Ensure correct length\n",
    "                    \n",
    "                    processed_labels.append(final_labels)\n",
    "                \n",
    "                model_inputs[\"labels\"] = processed_labels\n",
    "                return model_inputs\n",
    "            \n",
    "            # Preprocess complete multilingual dataset\n",
    "            print(\"⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\")\n",
    "            processed_dataset = full_dataset.map(\n",
    "                preprocess_multilingual_improved,\n",
    "                batched=True,\n",
    "                remove_columns=full_dataset[\"train\"].column_names,\n",
    "                desc=f\"Preprocessing with {size}_{tok_type}\",\n",
    "                batch_size=50,  # Smaller batch for stability\n",
    "                num_proc=1  # Single process to avoid issues\n",
    "            )\n",
    "            \n",
    "            train_dataset = processed_dataset[\"train\"]\n",
    "            eval_dataset = processed_dataset[\"test\"]\n",
    "            \n",
    "            # Filter out empty examples\n",
    "            def filter_valid_examples(example):\n",
    "                return (\n",
    "                    len(example[\"input_ids\"]) > 0 and \n",
    "                    len(example[\"labels\"]) > 0 and\n",
    "                    any(label != -100 for label in example[\"labels\"]) and\n",
    "                    sum(1 for token in example[\"input_ids\"] if token != tokenizer.pad_token_id) > 0\n",
    "                )\n",
    "            \n",
    "            train_dataset = train_dataset.filter(filter_valid_examples)\n",
    "            eval_dataset = eval_dataset.filter(filter_valid_examples)\n",
    "            \n",
    "            # Use smaller eval dataset to avoid memory issues\n",
    "            print(f\"✅ Preprocessed multilingual dataset:\")\n",
    "            print(f\"   • Train samples: {len(train_dataset)}\")\n",
    "            print(f\"   • Eval samples: {len(eval_dataset)}\")\n",
    "            \n",
    "            # REDUCE eval dataset size to avoid hanging\n",
    "            if len(eval_dataset) > 1000:\n",
    "                eval_dataset = eval_dataset.select(range(1000))\n",
    "                print(f\"   • Reduced eval samples to: {len(eval_dataset)} (to avoid memory issues)\")\n",
    "            \n",
    "            if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "                print(\"❌ No valid samples after preprocessing\")\n",
    "                continue\n",
    "            \n",
    "            # Setup training directory\n",
    "            output_dir = f\"./MT_models_multilingual_custom_tokenizers/{model_id}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # IMPROVED: Training arguments with simpler evaluation\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                num_train_epochs=1,  # More epochs for custom tokenizers\n",
    "                per_device_train_batch_size=2,  # Smaller batch size\n",
    "                per_device_eval_batch_size=1,  # REDUCED eval batch size\n",
    "                gradient_accumulation_steps=8,  # Higher accumulation\n",
    "                learning_rate=1e-4,  # Lower learning rate for stability\n",
    "                weight_decay=0.01,\n",
    "                warmup_ratio=0.1,  # Warmup as ratio\n",
    "                eval_strategy=\"steps\",  # Change to steps-based evaluation\n",
    "                eval_steps=500,  # Evaluate every 500 steps instead of epoch end\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=2,\n",
    "                logging_steps=50,\n",
    "                report_to=\"none\",\n",
    "                predict_with_generate=True,\n",
    "                generation_max_length=128,  # REDUCED generation length\n",
    "                generation_num_beams=2,  # REDUCED beams\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                load_best_model_at_end=False,  # DISABLED to avoid issues\n",
    "                dataloader_num_workers=0,\n",
    "                remove_unused_columns=False,  # CRITICAL: Keep all columns\n",
    "                ignore_data_skip=True,\n",
    "                label_smoothing_factor=0.1,  # Label smoothing\n",
    "                max_grad_norm=1.0,  # Gradient clipping\n",
    "                dataloader_pin_memory=False,  # Disable pin memory\n",
    "                skip_memory_metrics=True,  # Skip memory tracking\n",
    "                \n",
    "                \n",
    "            )\n",
    "            \n",
    "            # Data collator with explicit token_type_ids handling\n",
    "            data_collator = DataCollatorForSeq2Seq(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "            \n",
    "            # CRITICAL FIX: Custom data collator that removes token_type_ids\n",
    "            class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "                def __call__(self, features):\n",
    "                    batch = super().__call__(features)\n",
    "                    # Remove token_type_ids if present\n",
    "                    batch.pop(\"token_type_ids\", None)\n",
    "                    return batch\n",
    "            \n",
    "            data_collator = CustomDataCollator(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "            \n",
    "            # Trainer\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_multilingual_bleu\n",
    "            )\n",
    "            \n",
    "            # Train multilingual model\n",
    "            print(\"🏋️  Starting multilingual training with compatible model-tokenizer pair...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            print(\"📊 Final multilingual evaluation...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Save model and custom tokenizer\n",
    "            print(\"💾 Saving multilingual model and custom tokenizer...\")\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "            # Log results\n",
    "            overall_bleu = eval_results.get(\"eval_bleu\", 0.0)\n",
    "            overall_exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "            avg_pred_len = eval_results.get(\"eval_avg_pred_length\", 0.0)\n",
    "            avg_label_len = eval_results.get(\"eval_avg_label_length\", 0.0)\n",
    "            empty_preds = eval_results.get(\"eval_empty_predictions\", 0.0)\n",
    "            \n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config[\"base_model\"], size, tok_type,\n",
    "                    len(languages), len(train_dataset), len(eval_dataset), len(tokenizer),\n",
    "                    round(overall_bleu, 4), round(overall_exact_match, 4), \n",
    "                    round(avg_pred_len, 2), round(avg_label_len, 2),\n",
    "                    round(empty_preds, 4), \"SUCCESS\", \n",
    "                    f\"Compatible {model_config['description']} with {size}_{tok_type}\"\n",
    "                ])\n",
    "            \n",
    "            print(f\"✅ Completed: {model_id}\")\n",
    "            print(f\"📈 Overall BLEU Score: {overall_bleu:.4f}\")\n",
    "            print(f\"🎯 Overall Exact Match: {overall_exact_match:.4f}\")\n",
    "            \n",
    "            # Quick translation tests for different languages\n",
    "            print(f\"\\n🧪 Quick multilingual translation tests:\")\n",
    "            test_input = \"Hello, how are you today?\"\n",
    "            \n",
    "            for test_lang in [\"ar\", \"zh\", \"hi\"]:  # Test 3 languages\n",
    "                if model_type == \"t5\":\n",
    "                    formatted_input = f\"translate English to {languages[test_lang]}: {test_input}\"\n",
    "                else:  # BART\n",
    "                    formatted_input = f\"{test_input} </s> {test_lang}_XX\"\n",
    "                    \n",
    "                inputs = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, max_length=256, truncation=True)\n",
    "                \n",
    "                # Move to device\n",
    "                device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=128,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        forced_eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"  {test_lang} ({languages[test_lang]}): {translation}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to train {model_id}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config.get(\"base_model\", \"\"), size, tok_type,\n",
    "                    len(languages), 0, 0, 0, 0, 0, 0, 0, 0, \"TRAINING_FAILED\", str(e)[:100]\n",
    "                ])\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if model is not None:\n",
    "                del model\n",
    "            if trainer is not None:\n",
    "                del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "print(\"\\n🎉 Multilingual training with compatible model-tokenizer pairs completed!\")\n",
    "print(f\"📋 Results saved to: {results_log_file}\")\n",
    "print(f\"🔢 Total models trained: 9 (3 sizes × 3 types)\")\n",
    "print(f\"🌍 Each model handles all {len(languages)} languages simultaneously\")\n",
    "print(\"\\n📊 Expected improvements:\")\n",
    "print(\"• BPE tokenizers → BART models (proper compatibility)\")\n",
    "print(\"• WordPiece tokenizers → mT5 models (better multilingual support)\")\n",
    "print(\"• Unigram tokenizers → mT5 models (native compatibility)\")\n",
    "print(\"• Increased sequence length (256 vs 128)\")\n",
    "print(\"• Better hyperparameters and training setup\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239db6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading complete multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "🌍 All languages included: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr']\n",
      "🚀 Starting FAST multilingual training with COMPATIBLE model-tokenizer pairs...\n",
      "📊 Training approach: ONE model per tokenizer handling ALL 9 languages\n",
      "⚡ SPEED OPTIMIZATION: Pre-processing dataset for all tokenizer types...\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 1/9\n",
      "🔧 Custom Tokenizer: small_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 15002\n",
      "🔑 Special tokens - EOS: 15001, PAD: 0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 15002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  FAST preprocessing (single process for stability)...\n",
      "   • Reduced eval samples to: 500 for faster evaluation\n",
      "✅ FAST preprocessed dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 500\n",
      "⚡ Compiling model for faster training...\n",
      "✅ Model compiled successfully!\n",
      "🏋️  Starting FAST multilingual training...\n"
     ]
    }
   ],
   "source": [
    "##working ... but slow\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    logging as hf_logging,\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    BertTokenizerFast,\n",
    "    GPT2TokenizerFast,\n",
    "    BartForConditionalGeneration\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# FIXED: Model-Tokenizer Compatibility Mapping\n",
    "MODEL_TOKENIZER_CONFIGS = {\n",
    "    \"hf_bpe_hf\": {\n",
    "        \"base_model\": \"facebook/bart-large\",  # BART uses BPE\n",
    "        \"model_class\": BartForConditionalGeneration,\n",
    "        \"description\": \"BART with BPE tokenization\"\n",
    "    },\n",
    "    \"hf_wordpiece_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",  # mT5 can work with WordPiece\n",
    "        \"model_class\": T5ForConditionalGeneration,\n",
    "        \"description\": \"mT5 with WordPiece tokenization\"\n",
    "    },\n",
    "    \"sp_unigram_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",  # mT5 uses SentencePiece Unigram\n",
    "        \"model_class\": T5ForConditionalGeneration,\n",
    "        \"description\": \"mT5 with SentencePiece Unigram\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Custom tokenizer settings\n",
    "tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
    "tokenizer_types = [\"hf_bpe_hf\", \"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
    "\n",
    "# Languages for evaluation\n",
    "languages = {\n",
    "    \"yo\": \"Yoruba\",\n",
    "    \"ar\": \"Arabic\", \n",
    "    \"zh\": \"Chinese\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"swa\": \"Swahili\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"tr\": \"Turkish\"\n",
    "}\n",
    "SRC_LANG = \"en\"\n",
    "\n",
    "# Load complete multilingual dataset\n",
    "dataset_path = \"balanced_mt_dataset\"\n",
    "print(\"📦 Loading complete multilingual dataset...\")\n",
    "full_dataset = load_from_disk(dataset_path)\n",
    "print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "print(f\"🌍 All languages included: {list(languages.keys())}\")\n",
    "\n",
    "# Enhanced BLEU computation with progress tracking\n",
    "def compute_multilingual_bleu(eval_pred):\n",
    "    \"\"\"Multilingual BLEU computation with progress tracking\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if len(predictions.shape) == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    total_samples = len(predictions)\n",
    "    print(f\"🔄 Starting evaluation of {total_samples} samples...\")\n",
    "    \n",
    "    # Process in chunks with progress updates\n",
    "    chunk_size = 50\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        end_idx = min(i + chunk_size, total_samples)\n",
    "        chunk_preds = predictions[i:end_idx]\n",
    "        chunk_labels = labels[i:end_idx]\n",
    "        \n",
    "        try:\n",
    "            for pred, label in zip(chunk_preds, chunk_labels):\n",
    "                # CRITICAL FIX: Handle negative token IDs properly\n",
    "                # Filter out negative IDs and pad tokens before decoding\n",
    "                pred_clean = [token for token in pred if token >= 0 and token < len(tokenizer)]\n",
    "                label_clean = [token for token in label if token != -100 and token >= 0 and token < len(tokenizer)]\n",
    "                \n",
    "                try:\n",
    "                    decoded_pred = tokenizer.decode(pred_clean, skip_special_tokens=True).strip() if pred_clean else \"\"\n",
    "                    decoded_label = tokenizer.decode(label_clean, skip_special_tokens=True).strip() if label_clean else \"\"\n",
    "                except Exception as e:\n",
    "                    # Fallback for any decode errors\n",
    "                    decoded_pred = \"\"\n",
    "                    decoded_label = \"\"\n",
    "                \n",
    "                decoded_preds.append(decoded_pred)\n",
    "                decoded_labels.append(decoded_label)\n",
    "            \n",
    "            # Progress update every chunk\n",
    "            if (i + chunk_size) % (chunk_size * 5) == 0 or end_idx == total_samples:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"📊 Evaluation progress: {end_idx}/{total_samples} samples ({elapsed:.1f}s elapsed)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Batch decode failed for chunk {i}-{end_idx}: {str(e)}\")\n",
    "            # Add empty strings for failed batch\n",
    "            for _ in range(end_idx - i):\n",
    "                decoded_preds.append(\"\")\n",
    "                decoded_labels.append(\"\")\n",
    "    \n",
    "    # Compute BLEU with smoothing\n",
    "    print(\"🧮 Computing BLEU scores...\")\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for idx, (pred, label) in enumerate(zip(decoded_preds, decoded_labels)):\n",
    "        if idx % 1000 == 0 and idx > 0:\n",
    "            print(f\"   BLEU calculation: {idx}/{len(decoded_preds)} samples\")\n",
    "            \n",
    "        if not pred.strip() or not label.strip():\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "        \n",
    "        # Check exact match\n",
    "        if pred.lower().strip() == label.lower().strip():\n",
    "            exact_matches += 1\n",
    "        \n",
    "        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            bleu = sentence_bleu(\n",
    "                [label_tokens], \n",
    "                pred_tokens,\n",
    "                smoothing_function=smoothing,\n",
    "                weights=(0.25, 0.25, 0.25, 0.25)\n",
    "            )\n",
    "            bleu_scores.append(bleu)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match_ratio = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"✅ Evaluation complete: BLEU={avg_bleu:.4f}, Exact Match={exact_match_ratio:.4f} ({total_time:.1f}s total)\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"exact_match\": exact_match_ratio,\n",
    "        \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "        \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "        \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "    }\n",
    "\n",
    "def setup_custom_tokenizer_for_model(tokenizer_path, model_type):\n",
    "    \"\"\"Setup custom tokenizer with proper special tokens for specific model\"\"\"\n",
    "    # Load custom tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "    \n",
    "    # Model-specific special token configuration\n",
    "    if model_type == \"bart\":\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "    elif model_type == \"t5\":\n",
    "        special_tokens = {\n",
    "            'pad_token': '<pad>',\n",
    "            'eos_token': '</s>',\n",
    "            'unk_token': '<unk>',\n",
    "            'bos_token': '<pad>',  # T5 doesn't use BOS\n",
    "            'sep_token': '</s>',\n",
    "            'mask_token': '<extra_id_0>'\n",
    "        }\n",
    "    else:  # Default fallback\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "    \n",
    "    # Add missing special tokens\n",
    "    tokens_to_add = {}\n",
    "    for token_name, token_value in special_tokens.items():\n",
    "        if getattr(tokenizer, token_name, None) is None:\n",
    "            tokens_to_add[token_name] = token_value\n",
    "    \n",
    "    if tokens_to_add:\n",
    "        tokenizer.add_special_tokens(tokens_to_add)\n",
    "    \n",
    "    # Ensure we have the required tokens\n",
    "    assert tokenizer.eos_token is not None, \"EOS token is required\"\n",
    "    assert tokenizer.pad_token is not None, \"PAD token is required\"\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def configure_model_for_custom_tokenizer(model, tokenizer, model_type):\n",
    "    \"\"\"Configure model for custom tokenizer based on model type\"\"\"\n",
    "    # Resize embeddings\n",
    "    print(\"🔄 Resizing model embeddings to match custom tokenizer...\")\n",
    "    old_vocab_size = model.config.vocab_size\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   Vocab size: {old_vocab_size} → {len(tokenizer)}\")\n",
    "    \n",
    "    # Model-specific configuration\n",
    "    if model_type == \"bart\":\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.sep_token_id = tokenizer.eos_token_id\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    elif model_type == \"t5\":\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.decoder_start_token_id = tokenizer.pad_token_id  # T5 uses pad_token_id as decoder start\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Update generation config if it exists\n",
    "    if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        if model_type == \"bart\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "            model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        elif model_type == \"t5\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup logging\n",
    "log_dir = \"./MT_models_multilingual_custom_tokenizers\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "results_log_file = os.path.join(log_dir, \"multilingual_custom_tokenizer_results.csv\")\n",
    "\n",
    "if not os.path.exists(results_log_file):\n",
    "    with open(results_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Model_ID\", \"Base_Model\", \"Tokenizer_Size\", \"Tokenizer_Type\", \n",
    "                        \"Total_Languages\", \"Total_Train_Samples\", \"Total_Test_Samples\", \n",
    "                        \"Custom_Vocab_Size\", \"Overall_BLEU\", \"Overall_Exact_Match\", \n",
    "                        \"Avg_Pred_Length\", \"Avg_Label_Length\", \"Empty_Predictions\", \n",
    "                        \"Training_Status\", \"Notes\"])\n",
    "\n",
    "print(\"🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\")\n",
    "print(f\"📊 Training approach: ONE model per tokenizer handling ALL {len(languages)} languages\")\n",
    "\n",
    "# Main training loop: 3 sizes × 3 types = 9 models total\n",
    "for size in tokenizer_sizes:\n",
    "    for tok_type in tokenizer_types:\n",
    "        # Get compatible model configuration\n",
    "        model_config = MODEL_TOKENIZER_CONFIGS[tok_type]\n",
    "        \n",
    "        # Path to custom tokenizer\n",
    "        tokenizer_path = f\"vocab_final/vocab_final{size}/{tok_type}\"\n",
    "        model_id = f\"multilingual_{size}_{tok_type}\"\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"🚀 Training Model {tokenizer_sizes.index(size)*3 + tokenizer_types.index(tok_type) + 1}/9\")\n",
    "        print(f\"🔧 Custom Tokenizer: {size}_{tok_type}\")\n",
    "        print(f\"📦 Compatible Base Model: {model_config['base_model']}\")\n",
    "        print(f\"🌍 Target Languages: {list(languages.keys())} (ALL SIMULTANEOUSLY)\")\n",
    "        \n",
    "        # Initialize variables\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        trainer = None\n",
    "        \n",
    "        try:\n",
    "            # Load and setup custom tokenizer\n",
    "            print(f\"🔧 Loading custom tokenizer: {tokenizer_path}\")\n",
    "            model_type = \"bart\" if \"bart\" in model_config['base_model'] else \"t5\"\n",
    "            tokenizer = setup_custom_tokenizer_for_model(tokenizer_path, model_type)\n",
    "            \n",
    "            print(f\"✅ Custom tokenizer loaded successfully!\")\n",
    "            print(f\"📊 Custom vocab size: {len(tokenizer)}\")\n",
    "            print(f\"🔑 Special tokens - EOS: {tokenizer.eos_token_id}, PAD: {tokenizer.pad_token_id}\")\n",
    "            \n",
    "            # Load compatible base model\n",
    "            print(f\"🤖 Loading compatible base model: {model_config['base_model']}\")\n",
    "            model = model_config['model_class'].from_pretrained(model_config['base_model'])\n",
    "            \n",
    "            # Configure model for custom tokenizer\n",
    "            model = configure_model_for_custom_tokenizer(model, tokenizer, model_type)\n",
    "            \n",
    "            print(f\"✅ Model configured with custom tokenizer!\")\n",
    "            \n",
    "            # Use COMPLETE multilingual dataset (all languages together)\n",
    "            print(f\"📊 Using complete multilingual dataset:\")\n",
    "            print(f\"   • Train samples: {len(full_dataset['train'])}\")\n",
    "            print(f\"   • Test samples: {len(full_dataset['test'])}\")\n",
    "            print(f\"   • Languages: {len(languages)} ({list(languages.keys())})\")\n",
    "            \n",
    "            # IMPROVED: Preprocessing function for multilingual data\n",
    "            def preprocess_multilingual_improved(examples):\n",
    "                \"\"\"Improved preprocessing for multilingual data with model-specific formatting\"\"\"\n",
    "                sources = []\n",
    "                targets = []\n",
    "                \n",
    "                # Handle both single examples and batches\n",
    "                if not isinstance(examples[\"translation\"], list):\n",
    "                    examples = {\n",
    "                        \"translation\": [examples[\"translation\"]], \n",
    "                        \"language\": [examples[\"language\"]]\n",
    "                    }\n",
    "                \n",
    "                for translation, lang in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "                    if isinstance(translation, dict) and lang in languages:\n",
    "                        source = translation.get(SRC_LANG, \"\")\n",
    "                        target = translation.get(lang, \"\")\n",
    "                        \n",
    "                        if source.strip() and target.strip():  # Ensure non-empty\n",
    "                            # Model-specific formatting\n",
    "                            if model_type == \"t5\":\n",
    "                                # T5 style: \"translate English to German: Hello\"\n",
    "                                source_formatted = f\"translate English to {languages[lang]}: {source}\"\n",
    "                            else:  # BART style\n",
    "                                # BART style with language token\n",
    "                                source_formatted = f\"{source} </s> {lang}_XX\"  # Add language code\n",
    "                            \n",
    "                            sources.append(source_formatted)\n",
    "                            targets.append(target)\n",
    "                \n",
    "                if not sources or not targets:\n",
    "                    return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "                \n",
    "                # Tokenize with custom tokenizer\n",
    "                max_length = 256  # INCREASED for better performance\n",
    "                \n",
    "                # Input tokenization - REMOVE token_type_ids\n",
    "                model_inputs = tokenizer(\n",
    "                    sources,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=None,\n",
    "                    return_token_type_ids=False  # CRITICAL FIX\n",
    "                )\n",
    "                # Ensure token_type_ids is removed\n",
    "                model_inputs.pop(\"token_type_ids\", None)\n",
    "                \n",
    "                # Target tokenization  \n",
    "                with tokenizer.as_target_tokenizer():\n",
    "                    labels = tokenizer(\n",
    "                        targets,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\",\n",
    "                        return_tensors=None,\n",
    "                        return_token_type_ids=False  # CRITICAL FIX\n",
    "                    )\n",
    "                # Ensure token_type_ids is removed\n",
    "                labels.pop(\"token_type_ids\", None)\n",
    "                \n",
    "                # IMPROVED: Label processing with proper EOS handling\n",
    "                processed_labels = []\n",
    "                for label_seq in labels[\"input_ids\"]:\n",
    "                    # Find actual end of sequence (before padding)\n",
    "                    try:\n",
    "                        pad_start = label_seq.index(tokenizer.pad_token_id)\n",
    "                        actual_tokens = label_seq[:pad_start]\n",
    "                    except ValueError:\n",
    "                        actual_tokens = label_seq\n",
    "                    \n",
    "                    # Ensure EOS token at end\n",
    "                    if actual_tokens and actual_tokens[-1] != tokenizer.eos_token_id:\n",
    "                        actual_tokens.append(tokenizer.eos_token_id)\n",
    "                    elif not actual_tokens:\n",
    "                        actual_tokens = [tokenizer.eos_token_id]\n",
    "                    \n",
    "                    # Create final sequence with -100 for padding\n",
    "                    final_labels = actual_tokens + [-100] * (max_length - len(actual_tokens))\n",
    "                    final_labels = final_labels[:max_length]  # Ensure correct length\n",
    "                    \n",
    "                    processed_labels.append(final_labels)\n",
    "                \n",
    "                model_inputs[\"labels\"] = processed_labels\n",
    "                return model_inputs\n",
    "            \n",
    "            # Preprocess complete multilingual dataset\n",
    "            print(\"⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\")\n",
    "            processed_dataset = full_dataset.map(\n",
    "                preprocess_multilingual_improved,\n",
    "                batched=True,\n",
    "                remove_columns=full_dataset[\"train\"].column_names,\n",
    "                desc=f\"Preprocessing with {size}_{tok_type}\",\n",
    "                batch_size=50,  # Smaller batch for stability\n",
    "                num_proc=1  # Single process to avoid issues\n",
    "            )\n",
    "            \n",
    "            train_dataset = processed_dataset[\"train\"]\n",
    "            eval_dataset = processed_dataset[\"test\"]\n",
    "            \n",
    "            # Filter out empty examples\n",
    "            def filter_valid_examples(example):\n",
    "                return (\n",
    "                    len(example[\"input_ids\"]) > 0 and \n",
    "                    len(example[\"labels\"]) > 0 and\n",
    "                    any(label != -100 for label in example[\"labels\"]) and\n",
    "                    sum(1 for token in example[\"input_ids\"] if token != tokenizer.pad_token_id) > 0\n",
    "                )\n",
    "            \n",
    "            train_dataset = train_dataset.filter(filter_valid_examples)\n",
    "            eval_dataset = eval_dataset.filter(filter_valid_examples)\n",
    "            \n",
    "            # Use smaller eval dataset to avoid memory issues\n",
    "            print(f\"✅ Preprocessed multilingual dataset:\")\n",
    "            print(f\"   • Train samples: {len(train_dataset)}\")\n",
    "            print(f\"   • Eval samples: {len(eval_dataset)}\")\n",
    "            \n",
    "            # REDUCE eval dataset size to avoid hanging\n",
    "            if len(eval_dataset) > 1000:\n",
    "                eval_dataset = eval_dataset.select(range(1000))\n",
    "                print(f\"   • Reduced eval samples to: {len(eval_dataset)} (to avoid memory issues)\")\n",
    "            \n",
    "            if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "                print(\"❌ No valid samples after preprocessing\")\n",
    "                continue\n",
    "            \n",
    "            # Setup training directory\n",
    "            output_dir = f\"./MT_models_multilingual_custom_tokenizers/{model_id}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # IMPROVED: Training arguments with simpler evaluation\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                num_train_epochs=1,  # More epochs for custom tokenizers\n",
    "                per_device_train_batch_size=2,  # Smaller batch size\n",
    "                per_device_eval_batch_size=1,  # REDUCED eval batch size\n",
    "                gradient_accumulation_steps=8,  # Higher accumulation\n",
    "                learning_rate=1e-4,  # Lower learning rate for stability\n",
    "                weight_decay=0.01,\n",
    "                warmup_ratio=0.1,  # Warmup as ratio\n",
    "                eval_strategy=\"steps\",  # Change to steps-based evaluation\n",
    "                eval_steps=500,  # Evaluate every 500 steps instead of epoch end\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=2,\n",
    "                logging_steps=50,\n",
    "                report_to=\"none\",\n",
    "                predict_with_generate=True,\n",
    "                generation_max_length=128,  # REDUCED generation length\n",
    "                generation_num_beams=2,  # REDUCED beams\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                load_best_model_at_end=False,  # DISABLED to avoid issues\n",
    "                dataloader_num_workers=0,\n",
    "                remove_unused_columns=False,  # CRITICAL: Keep all columns\n",
    "                ignore_data_skip=True,\n",
    "                label_smoothing_factor=0.1,  # Label smoothing\n",
    "                max_grad_norm=1.0,  # Gradient clipping\n",
    "                dataloader_pin_memory=False,  # Disable pin memory\n",
    "                skip_memory_metrics=True,  # Skip memory tracking\n",
    "                \n",
    "                \n",
    "            )\n",
    "            \n",
    "            # Data collator with explicit token_type_ids handling\n",
    "            data_collator = DataCollatorForSeq2Seq(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "            \n",
    "            # CRITICAL FIX: Custom data collator that removes token_type_ids\n",
    "            class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "                def __call__(self, features):\n",
    "                    batch = super().__call__(features)\n",
    "                    # Remove token_type_ids if present\n",
    "                    batch.pop(\"token_type_ids\", None)\n",
    "                    return batch\n",
    "            \n",
    "            data_collator = CustomDataCollator(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "            \n",
    "            # Trainer\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_multilingual_bleu\n",
    "            )\n",
    "            \n",
    "            # Train multilingual model\n",
    "            print(\"🏋️  Starting multilingual training with compatible model-tokenizer pair...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            print(\"📊 Final multilingual evaluation...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Save model and custom tokenizer\n",
    "            print(\"💾 Saving multilingual model and custom tokenizer...\")\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "            # Log results\n",
    "            overall_bleu = eval_results.get(\"eval_bleu\", 0.0)\n",
    "            overall_exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "            avg_pred_len = eval_results.get(\"eval_avg_pred_length\", 0.0)\n",
    "            avg_label_len = eval_results.get(\"eval_avg_label_length\", 0.0)\n",
    "            empty_preds = eval_results.get(\"eval_empty_predictions\", 0.0)\n",
    "            \n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config[\"base_model\"], size, tok_type,\n",
    "                    len(languages), len(train_dataset), len(eval_dataset), len(tokenizer),\n",
    "                    round(overall_bleu, 4), round(overall_exact_match, 4), \n",
    "                    round(avg_pred_len, 2), round(avg_label_len, 2),\n",
    "                    round(empty_preds, 4), \"SUCCESS\", \n",
    "                    f\"Compatible {model_config['description']} with {size}_{tok_type}\"\n",
    "                ])\n",
    "            \n",
    "            print(f\"✅ Completed: {model_id}\")\n",
    "            print(f\"📈 Overall BLEU Score: {overall_bleu:.4f}\")\n",
    "            print(f\"🎯 Overall Exact Match: {overall_exact_match:.4f}\")\n",
    "            \n",
    "            # Quick translation tests for different languages\n",
    "            print(f\"\\n🧪 Quick multilingual translation tests:\")\n",
    "            test_input = \"Hello, how are you today?\"\n",
    "            \n",
    "            for test_lang in [\"ar\", \"zh\", \"hi\"]:  # Test 3 languages\n",
    "                if model_type == \"t5\":\n",
    "                    formatted_input = f\"translate English to {languages[test_lang]}: {test_input}\"\n",
    "                else:  # BART\n",
    "                    formatted_input = f\"{test_input} </s> {test_lang}_XX\"\n",
    "                    \n",
    "                inputs = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, max_length=256, truncation=True)\n",
    "                \n",
    "                # Move to device\n",
    "                device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=128,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        forced_eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"  {test_lang} ({languages[test_lang]}): {translation}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to train {model_id}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config.get(\"base_model\", \"\"), size, tok_type,\n",
    "                    len(languages), 0, 0, 0, 0, 0, 0, 0, 0, \"TRAINING_FAILED\", str(e)[:100]\n",
    "                ])\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if model is not None:\n",
    "                del model\n",
    "            if trainer is not None:\n",
    "                del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "print(\"\\n🎉 Multilingual training with compatible model-tokenizer pairs completed!\")\n",
    "print(f\"📋 Results saved to: {results_log_file}\")\n",
    "print(f\"🔢 Total models trained: 9 (3 sizes × 3 types)\")\n",
    "print(f\"🌍 Each model handles all {len(languages)} languages simultaneously\")\n",
    "print(\"\\n📊 Expected improvements:\")\n",
    "print(\"• BPE tokenizers → BART models (proper compatibility)\")\n",
    "print(\"• WordPiece tokenizers → mT5 models (better multilingual support)\")\n",
    "print(\"• Unigram tokenizers → mT5 models (native compatibility)\")\n",
    "print(\"• Increased sequence length (256 vs 128)\")\n",
    "print(\"• Better hyperparameters and training setup\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73ae275",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16=torch.cuda.is_available(),\n",
    "bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),  # Better than fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2210d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True,)\n",
      "(True,)\n"
     ]
    }
   ],
   "source": [
    "print(fp16)\n",
    "print(bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb92c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading complete multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "🌍 All languages included: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr']\n",
      "🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\n",
      "📊 Training approach: ONE model per tokenizer handling ALL 9 languages\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 1/9\n",
      "🔧 Custom Tokenizer: small_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded: vocab_size=15002 | EOS=15001 PAD=0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 15002\n",
      "⚡ Gradient checkpointing enabled\n",
      "🔒 Frozen embeddings + first 6 encoder layers\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with small_hf_bpe_hf: 100%|██████████| 34376/34376 [00:04<00:00, 7625.72 examples/s]\n",
      "Preprocessing with small_hf_bpe_hf: 100%|██████████| 3820/3820 [00:00<00:00, 7224.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\small_hf_bpe_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 429450.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 338608.40 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:30<00:00, 1003.21 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 1010.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_small_hf_bpe_hf: BartForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: BartForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 2/9\n",
      "🔧 Custom Tokenizer: small_hf_wordpiece_hf\n",
      "📦 Compatible Base Model: google/mt5-base\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/hf_wordpiece_hf\n",
      "✅ Custom tokenizer loaded: vocab_size=23634 | EOS=23632 PAD=0\n",
      "🤖 Loading compatible base model: google/mt5-base\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 250112 → 23634\n",
      "⚡ Gradient checkpointing enabled\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with small_hf_wordpiece_hf: 100%|██████████| 34376/34376 [00:05<00:00, 5775.57 examples/s]\n",
      "Preprocessing with small_hf_wordpiece_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6416.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\small_hf_wordpiece_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 436078.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 338608.40 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:42<00:00, 714.26 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 871.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_small_hf_wordpiece_hf: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 3/9\n",
      "🔧 Custom Tokenizer: small_sp_unigram_hf\n",
      "📦 Compatible Base Model: google/mt5-base\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalsmall/sp_unigram_hf\n",
      "✅ Custom tokenizer loaded: vocab_size=15002 | EOS=2 PAD=15000\n",
      "🤖 Loading compatible base model: google/mt5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 250112 → 15002\n",
      "⚡ Gradient checkpointing enabled\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with small_sp_unigram_hf: 100%|██████████| 34376/34376 [00:05<00:00, 6356.57 examples/s]\n",
      "Preprocessing with small_sp_unigram_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6204.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\small_sp_unigram_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 446145.72 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 338632.62 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:35<00:00, 871.39 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 876.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_small_sp_unigram_hf: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 4/9\n",
      "🔧 Custom Tokenizer: medium_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded: vocab_size=30002 | EOS=30001 PAD=0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 30002\n",
      "⚡ Gradient checkpointing enabled\n",
      "🔒 Frozen embeddings + first 6 encoder layers\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with medium_hf_bpe_hf: 100%|██████████| 34376/34376 [00:05<00:00, 6654.87 examples/s]\n",
      "Preprocessing with medium_hf_bpe_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6558.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\medium_hf_bpe_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 470238.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 338600.32 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:34<00:00, 874.98 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 881.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_medium_hf_bpe_hf: BartForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 5/9\n",
      "🔧 Custom Tokenizer: medium_hf_wordpiece_hf\n",
      "📦 Compatible Base Model: google/mt5-base\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/hf_wordpiece_hf\n",
      "✅ Custom tokenizer loaded: vocab_size=30002 | EOS=30000 PAD=0\n",
      "🤖 Loading compatible base model: google/mt5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: BartForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 250112 → 30002\n",
      "⚡ Gradient checkpointing enabled\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with medium_hf_wordpiece_hf: 100%|██████████| 34376/34376 [00:05<00:00, 6701.98 examples/s]\n",
      "Preprocessing with medium_hf_wordpiece_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6588.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\medium_hf_wordpiece_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 446120.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 307740.44 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:34<00:00, 881.09 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:04<00:00, 842.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_medium_hf_wordpiece_hf: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 6/9\n",
      "🔧 Custom Tokenizer: medium_sp_unigram_hf\n",
      "📦 Compatible Base Model: google/mt5-base\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/sp_unigram_hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom tokenizer loaded: vocab_size=30002 | EOS=2 PAD=30000\n",
      "🤖 Loading compatible base model: google/mt5-base\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 250112 → 30002\n",
      "⚡ Gradient checkpointing enabled\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with medium_sp_unigram_hf: 100%|██████████| 34376/34376 [00:05<00:00, 6325.41 examples/s]\n",
      "Preprocessing with medium_sp_unigram_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6183.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\medium_sp_unigram_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 459581.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 338600.32 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:35<00:00, 856.78 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:04<00:00, 844.55 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_medium_sp_unigram_hf: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 7/9\n",
      "🔧 Custom Tokenizer: large_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finallarge/hf_bpe_hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom tokenizer loaded: vocab_size=50002 | EOS=50001 PAD=0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 50002\n",
      "⚡ Gradient checkpointing enabled\n",
      "🔒 Frozen embeddings + first 6 encoder layers\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with large_hf_bpe_hf: 100%|██████████| 34376/34376 [00:05<00:00, 6445.92 examples/s]\n",
      "Preprocessing with large_hf_bpe_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6313.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\large_hf_bpe_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 452752.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 307820.48 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:36<00:00, 848.79 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:04<00:00, 690.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_large_hf_bpe_hf: BartForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: BartForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 8/9\n",
      "🔧 Custom Tokenizer: large_hf_wordpiece_hf\n",
      "📦 Compatible Base Model: google/mt5-base\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finallarge/hf_wordpiece_hf\n",
      "✅ Custom tokenizer loaded: vocab_size=50002 | EOS=50000 PAD=0\n",
      "🤖 Loading compatible base model: google/mt5-base\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 250112 → 50002\n",
      "⚡ Gradient checkpointing enabled\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with large_hf_wordpiece_hf: 100%|██████████| 34376/34376 [00:07<00:00, 4540.14 examples/s]\n",
      "Preprocessing with large_hf_wordpiece_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6455.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\large_hf_wordpiece_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 394325.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 307800.46 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:36<00:00, 846.23 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 996.17 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset: Train=30566, Eval=3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "❌ Failed to train multilingual_large_hf_wordpiece_hf: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 9/9\n",
      "🔧 Custom Tokenizer: large_sp_unigram_hf\n",
      "📦 Compatible Base Model: google/mt5-base\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finallarge/sp_unigram_hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User 4\\AppData\\Local\\Temp\\ipykernel_2508\\769564752.py\", line 544, in <module>\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 655, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: T5ForConditionalGeneration.forward() got an unexpected keyword argument 'num_items_in_batch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom tokenizer loaded: vocab_size=50002 | EOS=2 PAD=50000\n",
      "🤖 Loading compatible base model: google/mt5-base\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 250112 → 50002\n",
      "⚡ Gradient checkpointing enabled\n",
      "⚡ torch.compile applied (PyTorch 2.x)\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with large_sp_unigram_hf: 100%|██████████| 34376/34376 [00:04<00:00, 7775.44 examples/s]\n",
      "Preprocessing with large_sp_unigram_hf: 100%|██████████| 3820/3820 [00:00<00:00, 7571.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenized cache to processed_cached\\large_sp_unigram_hf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 30566/30566 [00:00<00:00, 488992.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3386/3386 [00:00<00:00, 376229.56 examples/s]\n",
      "Filter:  26%|██▌       | 8000/30566 [00:08<00:23, 967.10 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 465\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilter_valid_examples\u001b[39m(example):\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28many\u001b[39m(label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n\u001b[1;32m--> 465\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_valid_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mfilter(filter_valid_examples)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Preprocessed multilingual dataset: Train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Eval=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eval_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m func(dataset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:3895\u001b[0m, in \u001b[0;36mDataset.filter\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39miscoroutinefunction(function) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[0;32m   3893\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 3895\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[43m        \u001b[49m\u001b[43masync_get_indices_from_mask_function\u001b[49m\n\u001b[0;32m   3898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miscoroutinefunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3899\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_indices_from_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3905\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3919\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3921\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3923\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3924\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   3925\u001b[0m new_dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:3318\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[0;32m   3316\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3317\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[1;32m-> 3318\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munprocessed_kwargs):\n\u001b[0;32m   3319\u001b[0m                 check_if_shard_done(rank, done, content)\n\u001b[0;32m   3321\u001b[0m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:3674\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[0;32m   3672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3673\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3674\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[0;32m   3675\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[0;32m   3676\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:3624\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[1;34m(shard_iterable)\u001b[0m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3623\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3624\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[1;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[0;32m   3545\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3546\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m-> 3547\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\datasets\\arrow_dataset.py:6651\u001b[0m, in \u001b[0;36mget_indices_from_mask_function\u001b[1;34m(function, batched, with_indices, with_rank, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[0m\n\u001b[0;32m   6649\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   6650\u001b[0m             additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 6651\u001b[0m         mask\u001b[38;5;241m.\u001b[39mappend(function(example, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs))\n\u001b[0;32m   6652\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6653\u001b[0m     \u001b[38;5;66;03m# inputs is a list of columns\u001b[39;00m\n\u001b[0;32m   6654\u001b[0m     columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs\n",
      "Cell \u001b[1;32mIn[2], line 462\u001b[0m, in \u001b[0;36mfilter_valid_examples\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilter_valid_examples\u001b[39m(example):\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28many\u001b[39m(label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m         \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[2], line 462\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilter_valid_examples\u001b[39m(example):\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28mlen\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28many\u001b[39m(label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m         \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    463\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1079\u001b[0m, in \u001b[0;36mSpecialTokensMixin.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   1078\u001b[0m     key_without_id \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m-> 1079\u001b[0m     key_is_special_id \u001b[38;5;241m=\u001b[39m \u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m key\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_is_special_id:\n\u001b[0;32m   1081\u001b[0m         key_without_id \u001b[38;5;241m=\u001b[39m key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    logging as hf_logging,\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    BertTokenizerFast,\n",
    "    GPT2TokenizerFast,\n",
    "    BartForConditionalGeneration,\n",
    "    TrainerCallback\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# USER-TUNABLE SPEED HYPERPARAMS\n",
    "# -------------------------\n",
    "FREEZE_ENCODER_LAYERS = 6    # how many encoder layers to freeze initially\n",
    "FREEZE_EPOCHS = 2            # unfreeze after this many epochs\n",
    "TRAIN_BATCH_SIZE = 4         # per-device train batch\n",
    "GRAD_ACCUM_STEPS = 4         # gradient accumulation -> effective batch = TRAIN_BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "EVAL_BEAMS_TRAIN = 2         # beams used during training/eval cycle (faster)\n",
    "MAX_TOKENS = 256             # tokenization max length (you used 256 already)\n",
    "PROCESSED_CACHE_ROOT = \"processed_cached\"  # per-tokenizer cache dir root\n",
    "\n",
    "# FIXED: Model-Tokenizer Compatibility Mapping\n",
    "MODEL_TOKENIZER_CONFIGS = {\n",
    "    \"hf_bpe_hf\": {\n",
    "        \"base_model\": \"facebook/bart-large\",\n",
    "        \"model_class\": BartForConditionalGeneration,\n",
    "        \"description\": \"BART with BPE tokenization\"\n",
    "    },\n",
    "    \"hf_wordpiece_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",\n",
    "        \"model_class\": T5ForConditionalGeneration,\n",
    "        \"description\": \"mT5 with WordPiece tokenization\"\n",
    "    },\n",
    "    \"sp_unigram_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",\n",
    "        \"model_class\": T5ForConditionalGeneration,\n",
    "        \"description\": \"mT5 with SentencePiece Unigram\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Custom tokenizer settings\n",
    "tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
    "tokenizer_types = [\"hf_bpe_hf\", \"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
    "\n",
    "# Languages for evaluation\n",
    "languages = {\n",
    "    \"yo\": \"Yoruba\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"zh\": \"Chinese\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"swa\": \"Swahili\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"tr\": \"Turkish\"\n",
    "}\n",
    "SRC_LANG = \"en\"\n",
    "\n",
    "# Load complete multilingual dataset\n",
    "dataset_path = \"balanced_mt_dataset\"\n",
    "print(\"📦 Loading complete multilingual dataset...\")\n",
    "full_dataset = load_from_disk(dataset_path)\n",
    "print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "print(f\"🌍 All languages included: {list(languages.keys())}\")\n",
    "\n",
    "# -------------------------\n",
    "# Metric (keeps using global tokenizer at runtime)\n",
    "# -------------------------\n",
    "def compute_multilingual_bleu(eval_pred):\n",
    "    \"\"\"Multilingual BLEU computation with progress tracking\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # If predictions are logits/probs, pick argmax\n",
    "    try:\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    total_samples = len(predictions)\n",
    "    print(f\"🔄 Starting evaluation of {total_samples} samples...\")\n",
    "\n",
    "    chunk_size = 50\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        end_idx = min(i + chunk_size, total_samples)\n",
    "        chunk_preds = predictions[i:end_idx]\n",
    "        chunk_labels = labels[i:end_idx]\n",
    "\n",
    "        try:\n",
    "            for pred, label in zip(chunk_preds, chunk_labels):\n",
    "                # Handle negative token IDs properly\n",
    "                pred_clean = [token for token in pred if token >= 0 and token < len(tokenizer)]\n",
    "                label_clean = [token for token in label if token != -100 and token >= 0 and token < len(tokenizer)]\n",
    "\n",
    "                try:\n",
    "                    decoded_pred = tokenizer.decode(pred_clean, skip_special_tokens=True).strip() if pred_clean else \"\"\n",
    "                    decoded_label = tokenizer.decode(label_clean, skip_special_tokens=True).strip() if label_clean else \"\"\n",
    "                except Exception:\n",
    "                    decoded_pred = \"\"\n",
    "                    decoded_label = \"\"\n",
    "\n",
    "                decoded_preds.append(decoded_pred)\n",
    "                decoded_labels.append(decoded_label)\n",
    "\n",
    "            if (i + chunk_size) % (chunk_size * 5) == 0 or end_idx == total_samples:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"📊 Evaluation progress: {end_idx}/{total_samples} samples ({elapsed:.1f}s elapsed)\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Batch decode failed for chunk {i}-{end_idx}: {str(e)}\")\n",
    "            for _ in range(end_idx - i):\n",
    "                decoded_preds.append(\"\")\n",
    "                decoded_labels.append(\"\")\n",
    "\n",
    "    # Compute BLEU with smoothing\n",
    "    print(\"🧮 Computing BLEU scores...\")\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "\n",
    "    for idx, (pred, label) in enumerate(zip(decoded_preds, decoded_labels)):\n",
    "        if idx % 1000 == 0 and idx > 0:\n",
    "            print(f\"   BLEU calculation: {idx}/{len(decoded_preds)} samples\")\n",
    "\n",
    "        if not pred.strip() or not label.strip():\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "\n",
    "        if pred.lower().strip() == label.lower().strip():\n",
    "            exact_matches += 1\n",
    "\n",
    "        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            bleu = sentence_bleu(\n",
    "                [label_tokens],\n",
    "                pred_tokens,\n",
    "                smoothing_function=smoothing,\n",
    "                weights=(0.25, 0.25, 0.25, 0.25)\n",
    "            )\n",
    "            bleu_scores.append(bleu)\n",
    "        except Exception:\n",
    "            bleu_scores.append(0.0)\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match_ratio = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"✅ Evaluation complete: BLEU={avg_bleu:.4f}, Exact Match={exact_match_ratio:.4f} ({total_time:.1f}s total)\")\n",
    "\n",
    "    return {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"exact_match\": exact_match_ratio,\n",
    "        \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "        \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "        \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer helper + model config helpers (unchanged logic)\n",
    "# -------------------------\n",
    "def setup_custom_tokenizer_for_model(tokenizer_path, model_type):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "\n",
    "    if model_type == \"bart\":\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "    elif model_type == \"t5\":\n",
    "        special_tokens = {\n",
    "            'pad_token': '<pad>',\n",
    "            'eos_token': '</s>',\n",
    "            'unk_token': '<unk>',\n",
    "            'bos_token': '<pad>',\n",
    "            'sep_token': '</s>',\n",
    "            'mask_token': '<extra_id_0>'\n",
    "        }\n",
    "    else:\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "\n",
    "    tokens_to_add = {}\n",
    "    for token_name, token_value in special_tokens.items():\n",
    "        if getattr(tokenizer, token_name, None) is None:\n",
    "            tokens_to_add[token_name] = token_value\n",
    "\n",
    "    if tokens_to_add:\n",
    "        tokenizer.add_special_tokens(tokens_to_add)\n",
    "\n",
    "    assert tokenizer.eos_token is not None, \"EOS token is required\"\n",
    "    assert tokenizer.pad_token is not None, \"PAD token is required\"\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def configure_model_for_custom_tokenizer(model, tokenizer, model_type):\n",
    "    print(\"🔄 Resizing model embeddings to match custom tokenizer...\")\n",
    "    old_vocab_size = model.config.vocab_size\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   Vocab size: {old_vocab_size} → {len(tokenizer)}\")\n",
    "\n",
    "    if model_type == \"bart\":\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.sep_token_id = tokenizer.eos_token_id\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    elif model_type == \"t5\":\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        if model_type == \"bart\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "            model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        elif model_type == \"t5\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model\n",
    "\n",
    "# Setup logging / results CSV\n",
    "log_dir = \"./MT_models_multilingual_custom_tokenizers\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "results_log_file = os.path.join(log_dir, \"multilingual_custom_tokenizer_results.csv\")\n",
    "\n",
    "if not os.path.exists(results_log_file):\n",
    "    with open(results_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Model_ID\", \"Base_Model\", \"Tokenizer_Size\", \"Tokenizer_Type\",\n",
    "                         \"Total_Languages\", \"Total_Train_Samples\", \"Total_Test_Samples\",\n",
    "                         \"Custom_Vocab_Size\", \"Overall_BLEU\", \"Overall_Exact_Match\",\n",
    "                         \"Avg_Pred_Length\", \"Avg_Label_Length\", \"Empty_Predictions\",\n",
    "                         \"Training_Status\", \"Notes\"])\n",
    "\n",
    "print(\"🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\")\n",
    "print(f\"📊 Training approach: ONE model per tokenizer handling ALL {len(languages)} languages\")\n",
    "\n",
    "# Callback to unfreeze after FREEZE_EPOCHS\n",
    "class UnfreezeCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # state.epoch is float. Compare to FREEZE_EPOCHS - 1 because epoch increments at end\n",
    "        if int(state.epoch) == FREEZE_EPOCHS:\n",
    "            model_ref = kwargs.get(\"model\")\n",
    "            if model_ref is None:\n",
    "                return\n",
    "            print(f\"🔓 Unfreezing embeddings + first {FREEZE_ENCODER_LAYERS} encoder layers at epoch {state.epoch}\")\n",
    "            # Unfreeze embed tokens\n",
    "            try:\n",
    "                for p in model_ref.model.encoder.embed_tokens.parameters():\n",
    "                    p.requires_grad = True\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Unfreeze first N encoder layers\n",
    "            try:\n",
    "                for layer in model_ref.model.encoder.layers[:FREEZE_ENCODER_LAYERS]:\n",
    "                    for p in layer.parameters():\n",
    "                        p.requires_grad = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# Main training loop\n",
    "for size in tokenizer_sizes:\n",
    "    for tok_type in tokenizer_types:\n",
    "        model_config = MODEL_TOKENIZER_CONFIGS[tok_type]\n",
    "        tokenizer_path = f\"vocab_final/vocab_final{size}/{tok_type}\"\n",
    "        model_id = f\"multilingual_{size}_{tok_type}\"\n",
    "\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"🚀 Training Model {tokenizer_sizes.index(size)*3 + tokenizer_types.index(tok_type) + 1}/9\")\n",
    "        print(f\"🔧 Custom Tokenizer: {size}_{tok_type}\")\n",
    "        print(f\"📦 Compatible Base Model: {model_config['base_model']}\")\n",
    "        print(f\"🌍 Target Languages: {list(languages.keys())} (ALL SIMULTANEOUSLY)\")\n",
    "\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        trainer = None\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer and model\n",
    "            print(f\"🔧 Loading custom tokenizer: {tokenizer_path}\")\n",
    "            model_type = \"bart\" if \"bart\" in model_config['base_model'] else \"t5\"\n",
    "            tokenizer = setup_custom_tokenizer_for_model(tokenizer_path, model_type)\n",
    "            print(f\"✅ Custom tokenizer loaded: vocab_size={len(tokenizer)} | EOS={tokenizer.eos_token_id} PAD={tokenizer.pad_token_id}\")\n",
    "\n",
    "            print(f\"🤖 Loading compatible base model: {model_config['base_model']}\")\n",
    "            model = model_config['model_class'].from_pretrained(model_config['base_model'])\n",
    "\n",
    "            # Configure model/tokenizer\n",
    "            model = configure_model_for_custom_tokenizer(model, tokenizer, model_type)\n",
    "\n",
    "            # Speed-ups: gradient checkpointing and optional torch.compile + bf16\n",
    "            try:\n",
    "                model.gradient_checkpointing_enable()\n",
    "                print(\"⚡ Gradient checkpointing enabled\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Freeze embeddings + first layers\n",
    "            try:\n",
    "                for p in model.model.encoder.embed_tokens.parameters():\n",
    "                    p.requires_grad = False\n",
    "                for layer in model.model.encoder.layers[:FREEZE_ENCODER_LAYERS]:\n",
    "                    for p in layer.parameters():\n",
    "                        p.requires_grad = False\n",
    "                print(f\"🔒 Frozen embeddings + first {FREEZE_ENCODER_LAYERS} encoder layers\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Try to compile (PyTorch 2.x)\n",
    "            if hasattr(torch, \"compile\"):\n",
    "                try:\n",
    "                    model = torch.compile(model)\n",
    "                    print(\"⚡ torch.compile applied (PyTorch 2.x)\")\n",
    "                except Exception as e:\n",
    "                    print(\"⚠️ torch.compile failed or unsupported: \", e)\n",
    "\n",
    "            # Preprocess & cache per tokenizer to avoid re-tokenizing multiple times\n",
    "            cache_dir = os.path.join(PROCESSED_CACHE_ROOT, f\"{size}_{tok_type}\")\n",
    "            if not os.path.exists(PROCESSED_CACHE_ROOT):\n",
    "                os.makedirs(PROCESSED_CACHE_ROOT, exist_ok=True)\n",
    "\n",
    "            if os.path.exists(cache_dir):\n",
    "                print(f\"Loading cached tokenized dataset from {cache_dir} ...\")\n",
    "                processed_dataset = load_from_disk(cache_dir)\n",
    "            else:\n",
    "                print(\"⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\")\n",
    "\n",
    "                def preprocess_multilingual_improved(examples):\n",
    "                    sources = []\n",
    "                    targets = []\n",
    "\n",
    "                    if not isinstance(examples[\"translation\"], list):\n",
    "                        examples = {\n",
    "                            \"translation\": [examples[\"translation\"]],\n",
    "                            \"language\": [examples[\"language\"]]\n",
    "                        }\n",
    "\n",
    "                    for translation, lang in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "                        if isinstance(translation, dict) and lang in languages:\n",
    "                            source = translation.get(SRC_LANG, \"\")\n",
    "                            target = translation.get(lang, \"\")\n",
    "\n",
    "                            if source.strip() and target.strip():\n",
    "                                if model_type == \"t5\":\n",
    "                                    source_formatted = f\"translate English to {languages[lang]}: {source}\"\n",
    "                                else:\n",
    "                                    source_formatted = f\"{source} </s> {lang}_XX\"\n",
    "                                sources.append(source_formatted)\n",
    "                                targets.append(target)\n",
    "\n",
    "                    if not sources or not targets:\n",
    "                        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "                    max_length = MAX_TOKENS\n",
    "\n",
    "                    model_inputs = tokenizer(\n",
    "                        sources,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\",\n",
    "                        return_tensors=None,\n",
    "                        return_token_type_ids=False\n",
    "                    )\n",
    "                    model_inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "                    with tokenizer.as_target_tokenizer():\n",
    "                        labels = tokenizer(\n",
    "                            targets,\n",
    "                            max_length=max_length,\n",
    "                            truncation=True,\n",
    "                            padding=\"max_length\",\n",
    "                            return_tensors=None,\n",
    "                            return_token_type_ids=False\n",
    "                        )\n",
    "                    labels.pop(\"token_type_ids\", None)\n",
    "\n",
    "                    processed_labels = []\n",
    "                    for label_seq in labels[\"input_ids\"]:\n",
    "                        try:\n",
    "                            pad_start = label_seq.index(tokenizer.pad_token_id)\n",
    "                            actual_tokens = label_seq[:pad_start]\n",
    "                        except ValueError:\n",
    "                            actual_tokens = label_seq\n",
    "\n",
    "                        if actual_tokens and actual_tokens[-1] != tokenizer.eos_token_id:\n",
    "                            actual_tokens.append(tokenizer.eos_token_id)\n",
    "                        elif not actual_tokens:\n",
    "                            actual_tokens = [tokenizer.eos_token_id]\n",
    "\n",
    "                        final_labels = actual_tokens + [-100] * (max_length - len(actual_tokens))\n",
    "                        final_labels = final_labels[:max_length]\n",
    "                        processed_labels.append(final_labels)\n",
    "\n",
    "                    model_inputs[\"labels\"] = processed_labels\n",
    "                    return model_inputs\n",
    "\n",
    "                processed_dataset = full_dataset.map(\n",
    "                    preprocess_multilingual_improved,\n",
    "                    batched=True,\n",
    "                    remove_columns=full_dataset[\"train\"].column_names,\n",
    "                    desc=f\"Preprocessing with {size}_{tok_type}\",\n",
    "                    batch_size=50,\n",
    "                    num_proc=1\n",
    "                )\n",
    "\n",
    "                print(f\"Saving tokenized cache to {cache_dir} ...\")\n",
    "                processed_dataset.save_to_disk(cache_dir)\n",
    "\n",
    "            train_dataset = processed_dataset[\"train\"]\n",
    "            eval_dataset = processed_dataset[\"test\"]\n",
    "\n",
    "            # Filter out empty examples\n",
    "            def filter_valid_examples(example):\n",
    "                return (\n",
    "                    len(example[\"input_ids\"]) > 0 and\n",
    "                    len(example[\"labels\"]) > 0 and\n",
    "                    any(label != -100 for label in example[\"labels\"]) and\n",
    "                    sum(1 for token in example[\"input_ids\"] if token != tokenizer.pad_token_id) > 0\n",
    "                )\n",
    "\n",
    "            train_dataset = train_dataset.filter(filter_valid_examples)\n",
    "            eval_dataset = eval_dataset.filter(filter_valid_examples)\n",
    "\n",
    "            print(f\"✅ Preprocessed multilingual dataset: Train={len(train_dataset)}, Eval={len(eval_dataset)}\")\n",
    "\n",
    "            # Reduce eval dataset to avoid memory issues (you can change threshold)\n",
    "            if len(eval_dataset) > 1000:\n",
    "                eval_dataset = eval_dataset.select(range(1000))\n",
    "                print(f\"   • Reduced eval samples to: {len(eval_dataset)} (to avoid memory issues)\")\n",
    "\n",
    "            if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "                print(\"❌ No valid samples after preprocessing - skipping this config\")\n",
    "                continue\n",
    "\n",
    "            # Output dir for this model\n",
    "            output_dir = f\"./MT_models_multilingual_custom_tokenizers/{model_id}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Training args focused on speed & stability\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                num_train_epochs=5,\n",
    "                per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=max(1, TRAIN_BATCH_SIZE),\n",
    "                gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "                learning_rate=1e-4,\n",
    "                weight_decay=0.01,\n",
    "                warmup_ratio=0.1,\n",
    "                eval_strategy=\"epoch\",          # evaluate once per epoch (faster)\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=2,\n",
    "                logging_steps=50,\n",
    "                report_to=\"none\",\n",
    "                predict_with_generate=True,\n",
    "                generation_max_length=128,\n",
    "                generation_num_beams=EVAL_BEAMS_TRAIN,  # small beam during training/eval\n",
    "                fp16=False,\n",
    "                bf16=torch.cuda.is_available(),    # use bf16 if GPU supports it\n",
    "                optim=\"adafactor\",                 # memory-efficient optimizer\n",
    "                load_best_model_at_end=False,\n",
    "                dataloader_num_workers=0,\n",
    "                remove_unused_columns=False,\n",
    "                ignore_data_skip=True,\n",
    "                label_smoothing_factor=0.1,\n",
    "                max_grad_norm=1.0,\n",
    "                dataloader_pin_memory=False,\n",
    "                skip_memory_metrics=True,\n",
    "            )\n",
    "\n",
    "            # Data collator that removes token_type_ids\n",
    "            class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "                def __call__(self, features):\n",
    "                    batch = super().__call__(features)\n",
    "                    batch.pop(\"token_type_ids\", None)\n",
    "                    return batch\n",
    "\n",
    "            data_collator = CustomDataCollator(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                pad_to_multiple_of=8 if training_args.fp16 or training_args.bf16 else None,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "\n",
    "            # Trainer\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_multilingual_bleu,\n",
    "                callbacks=[UnfreezeCallback()]\n",
    "            )\n",
    "\n",
    "            # Train\n",
    "            print(\"🏋️  Starting multilingual training with compatible model-tokenizer pair...\")\n",
    "            trainer.train()\n",
    "\n",
    "            # Final evaluation (you may want to run with larger beams here if desired)\n",
    "            print(\"📊 Final multilingual evaluation...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "\n",
    "            # Save\n",
    "            print(\"💾 Saving multilingual model and custom tokenizer...\")\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            # Log results\n",
    "            overall_bleu = eval_results.get(\"eval_bleu\", 0.0)\n",
    "            overall_exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "            avg_pred_len = eval_results.get(\"eval_avg_pred_length\", 0.0)\n",
    "            avg_label_len = eval_results.get(\"eval_avg_label_length\", 0.0)\n",
    "            empty_preds = eval_results.get(\"eval_empty_predictions\", 0.0)\n",
    "\n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config[\"base_model\"], size, tok_type,\n",
    "                    len(languages), len(train_dataset), len(eval_dataset), len(tokenizer),\n",
    "                    round(overall_bleu, 4), round(overall_exact_match, 4),\n",
    "                    round(avg_pred_len, 2), round(avg_label_len, 2),\n",
    "                    round(empty_preds, 4), \"SUCCESS\",\n",
    "                    f\"Compatible {model_config['description']} with {size}_{tok_type}\"\n",
    "                ])\n",
    "\n",
    "            print(f\"✅ Completed: {model_id}\")\n",
    "            print(f\"📈 Overall BLEU Score: {overall_bleu:.4f}\")\n",
    "            print(f\"🎯 Overall Exact Match: {overall_exact_match:.4f}\")\n",
    "\n",
    "            # Quick multilingual translation tests (use larger beams here for quality)\n",
    "            print(f\"\\n🧪 Quick multilingual translation tests:\")\n",
    "            test_input = \"Hello, how are you today?\"\n",
    "            for test_lang in [\"ar\", \"zh\", \"hi\"]:\n",
    "                if model_type == \"t5\":\n",
    "                    formatted_input = f\"translate English to {languages[test_lang]}: {test_input}\"\n",
    "                else:\n",
    "                    formatted_input = f\"{test_input} </s> {test_lang}_XX\"\n",
    "\n",
    "                inputs = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, max_length=256, truncation=True)\n",
    "                device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=128,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        forced_eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"  {test_lang} ({languages[test_lang]}): {translation}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to train {model_id}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config.get(\"base_model\", \"\"), size, tok_type,\n",
    "                    len(languages), 0, 0, 0, 0, 0, 0, 0, 0, \"TRAINING_FAILED\", str(e)[:200]\n",
    "                ])\n",
    "        finally:\n",
    "            if model is not None:\n",
    "                del model\n",
    "            if trainer is not None:\n",
    "                del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "print(\"\\n🎉 Multilingual training with compatible model-tokenizer pairs completed!\")\n",
    "print(f\"📋 Results saved to: {results_log_file}\")\n",
    "print(f\"🔢 Total models trained: {len(tokenizer_sizes) * len(tokenizer_types)}\")\n",
    "print(f\"🌍 Each model handles all {len(languages)} languages simultaneously\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "\n",
      "📊 Language distribution:\n",
      "  zh: 3,807 pairs (mBART: zh_CN)\n",
      "  bn: 3,829 pairs (mBART: bn_IN)\n",
      "  ru: 3,824 pairs (mBART: ru_RU)\n",
      "  ar: 3,838 pairs (mBART: ar_AR)\n",
      "  ja: 3,808 pairs (mBART: ja_XX)\n",
      "  sw: 3,810 pairs (mBART: sw_KE)\n",
      "  yo: 3,820 pairs (mBART: yo_NG)\n",
      "  tr: 3,814 pairs (mBART: tr_TR)\n",
      "  hi: 3,826 pairs (mBART: hi_IN)\n",
      "🚀 Starting training with BETTER pre-trained models...\n",
      "\n",
      "====================================================================================================\n",
      "🔧 Training with Pre-trained multilingual MT model\n",
      "📦 Model: facebook/mbart-large-50-many-to-many-mmt\n",
      "🤖 Loading mBART for multilingual translation...\n",
      "✅ Model loaded successfully!\n",
      "📊 Vocab size: 250054\n",
      "⚙️  Preprocessing data for better models...\n",
      "✅ Preprocessed: 34376 train, 3820 eval samples\n",
      "🏋️  Starting training with better pre-trained model...\n",
      "{'loss': 7.3144, 'grad_norm': 5.2044358253479, 'learning_rate': 5.940000000000001e-06, 'epoch': 0.09308820107051431}\n",
      "{'loss': 5.6617, 'grad_norm': 4.844237804412842, 'learning_rate': 1.1940000000000001e-05, 'epoch': 0.18617640214102862}\n",
      "{'loss': 5.0044, 'grad_norm': 4.143449306488037, 'learning_rate': 1.794e-05, 'epoch': 0.27926460321154295}\n",
      "{'loss': 4.6734, 'grad_norm': 4.718307018280029, 'learning_rate': 2.394e-05, 'epoch': 0.37235280428205725}\n",
      "{'loss': 4.5418, 'grad_norm': 4.214032173156738, 'learning_rate': 2.994e-05, 'epoch': 0.46544100535257155}\n",
      "{'eval_loss': 4.308953762054443, 'eval_bleu': 0.027234471945094322, 'eval_exact_match': 0.004188481675392671, 'eval_avg_pred_length': 6.554188481675393, 'eval_avg_label_length': 7.73848167539267, 'eval_empty_predictions': 0.0, 'eval_runtime': 562.5499, 'eval_samples_per_second': 6.791, 'eval_steps_per_second': 1.698, 'epoch': 0.49988363974866185}\n",
      "{'loss': 4.3777, 'grad_norm': 3.6810014247894287, 'learning_rate': 2.4834782608695652e-05, 'epoch': 0.5585292064230859}\n",
      "{'loss': 4.2126, 'grad_norm': 3.6935713291168213, 'learning_rate': 1.9617391304347827e-05, 'epoch': 0.6516174074936002}\n",
      "{'loss': 4.1568, 'grad_norm': 4.257287979125977, 'learning_rate': 1.44e-05, 'epoch': 0.7447056085641145}\n",
      "{'loss': 4.095, 'grad_norm': 4.493302822113037, 'learning_rate': 9.182608695652174e-06, 'epoch': 0.8377938096346288}\n",
      "{'loss': 4.0766, 'grad_norm': 4.246417045593262, 'learning_rate': 3.965217391304348e-06, 'epoch': 0.9308820107051431}\n",
      "{'eval_loss': 4.014238357543945, 'eval_bleu': 0.05423596267646436, 'eval_exact_match': 0.025130890052356022, 'eval_avg_pred_length': 6.540837696335078, 'eval_avg_label_length': 7.73848167539267, 'eval_empty_predictions': 0.0, 'eval_runtime': 570.9955, 'eval_samples_per_second': 6.69, 'eval_steps_per_second': 1.673, 'epoch': 0.9997672794973237}\n",
      "{'train_runtime': 1896.5188, 'train_samples_per_second': 18.126, 'train_steps_per_second': 0.567, 'train_loss': 4.75332269712936, 'epoch': 1.0}\n",
      "📊 Final evaluation...\n",
      "{'eval_loss': 4.014238357543945, 'eval_bleu': 0.05423596267646436, 'eval_exact_match': 0.025130890052356022, 'eval_avg_pred_length': 6.540837696335078, 'eval_avg_label_length': 7.73848167539267, 'eval_empty_predictions': 0.0, 'eval_runtime': 568.2079, 'eval_samples_per_second': 6.723, 'eval_steps_per_second': 1.681, 'epoch': 1.0}\n",
      "💾 Saving model...\n",
      "✅ Completed: mbart_small\n",
      "📈 BLEU Score: 0.0542\n",
      "🎯 Exact Match: 0.0251\n",
      "📏 Lengths - Pred: 6.5, Label: 7.7\n",
      "\n",
      "🧪 Quick translation test:\n",
      "  Input: Hello, how are you today?\n",
      "  Output: 你好 你好吗?\n",
      "\n",
      "🎉 Better model training completed!\n",
      "📋 Results: ./MT_models_better\\better_multilingual_results.csv\n",
      "\n",
      "💡 Usage examples:\n",
      "# mBART:\n",
      "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
      "model = MBartForConditionalGeneration.from_pretrained('./MT_models_better/mbart_small')\n",
      "tokenizer = MBart50TokenizerFast.from_pretrained('./MT_models_better/mbart_small')\n",
      "\n",
      "# Expected improvements:\n",
      "  • BLEU: 0.15-0.35 (instead of 0.0044)\n",
      "  • Better multilingual understanding\n",
      "  • More natural translations\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import torch\n",
    "# import warnings\n",
    "# import gc\n",
    "# from datasets import load_from_disk\n",
    "# from transformers import (\n",
    "#     EncoderDecoderModel,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSeq2SeqLM,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "#     Seq2SeqTrainer,\n",
    "#     Seq2SeqTrainingArguments,\n",
    "#     logging as hf_logging,\n",
    "#     MBartForConditionalGeneration,\n",
    "#     MBart50TokenizerFast,\n",
    "#     T5ForConditionalGeneration,\n",
    "#     T5TokenizerFast\n",
    "# )\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# import numpy as np\n",
    "\n",
    "# # Suppress warnings\n",
    "# hf_logging.set_verbosity_error()\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Configuration with BETTER base models\n",
    "# BETTER_MODELS = {\n",
    "#     \"mbart_small\": {\n",
    "#         \"model_name\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "#         \"tokenizer_name\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "#         \"type\": \"mbart\",\n",
    "#         \"description\": \"Pre-trained multilingual MT model\"\n",
    "#     },\n",
    "#     # \"mt5_small\": {\n",
    "#     #     \"model_name\": \"google/mt5-small\",\n",
    "#     #     \"tokenizer_name\": \"google/mt5-small\", \n",
    "#     #     \"type\": \"t5\",\n",
    "#     #     \"description\": \"Multilingual T5 for translation\"\n",
    "#     # },\n",
    "#     # \"opus_mt\": {\n",
    "#     #     \"model_name\": \"Helsinki-NLP/opus-mt-en-mul\", \n",
    "#     #     \"tokenizer_name\": \"Helsinki-NLP/opus-mt-en-mul\",\n",
    "#     #     \"type\": \"marian\",\n",
    "#     #     \"description\": \"OPUS multilingual translation\"\n",
    "#     # }\n",
    "# }\n",
    "\n",
    "# # Languages with proper mBART language codes\n",
    "# MBART_LANG_CODES = {\n",
    "#     \"yo\": \"yo_NG\",  # Yoruba\n",
    "#     \"ar\": \"ar_AR\",  # Arabic  \n",
    "#     \"zh\": \"zh_CN\",  # Chinese\n",
    "#     \"ru\": \"ru_RU\",  # Russian\n",
    "#     \"hi\": \"hi_IN\",  # Hindi\n",
    "#     \"ja\": \"ja_XX\",  # Japanese\n",
    "#     \"sw\": \"sw_KE\",  # Swahili (if available, else skip)\n",
    "#     \"bn\": \"bn_IN\",  # Bengali\n",
    "#     \"tr\": \"tr_TR\",  # Turkish\n",
    "#     \"en\": \"en_XX\"   # English\n",
    "# }\n",
    "\n",
    "# # Load dataset\n",
    "# dataset_path = \"balanced_mt_dataset\"\n",
    "# print(\"📦 Loading multilingual dataset...\")\n",
    "# full_dataset = load_from_disk(dataset_path)\n",
    "# print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "\n",
    "# # Check language distribution\n",
    "# train_lang_dist = {}\n",
    "# for example in full_dataset['train']:\n",
    "#     lang = example['language']\n",
    "#     train_lang_dist[lang] = train_lang_dist.get(lang, 0) + 1\n",
    "\n",
    "# print(\"\\n📊 Language distribution:\")\n",
    "# available_langs = []\n",
    "# for lang_code, count in train_lang_dist.items():\n",
    "#     if count > 0:\n",
    "#         available_langs.append(lang_code)\n",
    "#         mbart_code = MBART_LANG_CODES.get(lang_code, \"UNKNOWN\")\n",
    "#         print(f\"  {lang_code}: {count:,} pairs (mBART: {mbart_code})\")\n",
    "\n",
    "# # Enhanced BLEU computation\n",
    "# def compute_bleu_advanced(eval_pred):\n",
    "#     \"\"\"Advanced BLEU computation with better handling\"\"\"\n",
    "#     predictions, labels = eval_pred\n",
    "    \n",
    "#     if len(predictions.shape) == 3:\n",
    "#         predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "#     decoded_preds = []\n",
    "#     decoded_labels = []\n",
    "    \n",
    "#     for pred, label in zip(predictions, labels):\n",
    "#         # Replace -100 with pad token for decoding\n",
    "#         label = np.where(label != -100, label, tokenizer.pad_token_id)\n",
    "        \n",
    "#         decoded_pred = tokenizer.decode(pred, skip_special_tokens=True).strip()\n",
    "#         decoded_label = tokenizer.decode(label, skip_special_tokens=True).strip()\n",
    "        \n",
    "#         decoded_preds.append(decoded_pred)\n",
    "#         decoded_labels.append(decoded_label)\n",
    "    \n",
    "#     # Compute BLEU with smoothing\n",
    "#     smoothing = SmoothingFunction().method1\n",
    "#     bleu_scores = []\n",
    "#     exact_matches = 0\n",
    "    \n",
    "#     for pred, label in zip(decoded_preds, decoded_labels):\n",
    "#         if not pred.strip() or not label.strip():\n",
    "#             bleu_scores.append(0.0)\n",
    "#             continue\n",
    "            \n",
    "#         pred_tokens = pred.split()\n",
    "#         label_tokens = label.split()\n",
    "        \n",
    "#         # Check exact match\n",
    "#         if pred.lower().strip() == label.lower().strip():\n",
    "#             exact_matches += 1\n",
    "        \n",
    "#         if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "#             bleu_scores.append(0.0)\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             bleu = sentence_bleu(\n",
    "#                 [label_tokens], \n",
    "#                 pred_tokens,\n",
    "#                 smoothing_function=smoothing,\n",
    "#                 weights=(0.25, 0.25, 0.25, 0.25)\n",
    "#             )\n",
    "#             bleu_scores.append(bleu)\n",
    "#         except:\n",
    "#             bleu_scores.append(0.0)\n",
    "    \n",
    "#     avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "#     exact_match_ratio = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "    \n",
    "#     return {\n",
    "#         \"bleu\": avg_bleu,\n",
    "#         \"exact_match\": exact_match_ratio,\n",
    "#         \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "#         \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "#         \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "#     }\n",
    "\n",
    "# # Setup logging\n",
    "# log_dir = \"./MT_models_better\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# results_log_file = os.path.join(log_dir, \"better_multilingual_results.csv\")\n",
    "\n",
    "# if not os.path.exists(results_log_file):\n",
    "#     with open(results_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow([\"Model_ID\", \"Base_Model\", \"Model_Type\", \"Total_Train_Samples\", \"Total_Test_Samples\", \n",
    "#                         \"Vocab_Size\", \"BLEU_Score\", \"Exact_Match\", \"Avg_Pred_Length\", \"Avg_Label_Length\", \n",
    "#                         \"Empty_Predictions\", \"Training_Status\", \"Notes\"])\n",
    "\n",
    "# print(\"🚀 Starting training with BETTER pre-trained models...\")\n",
    "\n",
    "# for model_id, model_config in BETTER_MODELS.items():\n",
    "#     print(f\"\\n{'='*100}\")\n",
    "#     print(f\"🔧 Training with {model_config['description']}\")\n",
    "#     print(f\"📦 Model: {model_config['model_name']}\")\n",
    "    \n",
    "#     # Initialize these variables at the beginning of each iteration\n",
    "#     model = None\n",
    "#     tokenizer = None\n",
    "#     trainer = None\n",
    "    \n",
    "#     try:\n",
    "#         # Load model and tokenizer based on type\n",
    "#         if model_config[\"type\"] == \"mbart\":\n",
    "#             print(\"🤖 Loading mBART for multilingual translation...\")\n",
    "#             model = MBartForConditionalGeneration.from_pretrained(model_config[\"model_name\"])\n",
    "#             tokenizer = MBart50TokenizerFast.from_pretrained(model_config[\"tokenizer_name\"])\n",
    "            \n",
    "#             # Set source language for mBART\n",
    "#             tokenizer.src_lang = \"en_XX\"\n",
    "            \n",
    "#         elif model_config[\"type\"] == \"t5\":\n",
    "#             print(\"🤖 Loading mT5 for multilingual translation...\")\n",
    "#             model = T5ForConditionalGeneration.from_pretrained(model_config[\"model_name\"])\n",
    "#             tokenizer = T5TokenizerFast.from_pretrained(model_config[\"tokenizer_name\"])\n",
    "            \n",
    "#         elif model_config[\"type\"] == \"marian\":\n",
    "#             print(\"🤖 Loading Marian OPUS-MT model...\")\n",
    "#             model = AutoModelForSeq2SeqLM.from_pretrained(model_config[\"model_name\"])\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(model_config[\"tokenizer_name\"])\n",
    "            \n",
    "#         else:\n",
    "#             print(f\"❌ Unknown model type: {model_config['type']}\")\n",
    "#             continue\n",
    "            \n",
    "#         print(f\"✅ Model loaded successfully!\")\n",
    "#         print(f\"📊 Vocab size: {len(tokenizer)}\")\n",
    "        \n",
    "#         # Preprocessing function for different model types\n",
    "#         def preprocess_for_better_models(examples):\n",
    "#             \"\"\"Preprocess for pre-trained multilingual models\"\"\"\n",
    "#             sources = []\n",
    "#             targets = []\n",
    "            \n",
    "#             # Handle both single examples and batches\n",
    "#             if not isinstance(examples[\"translation\"], list):\n",
    "#                 # Single example\n",
    "#                 examples = {\n",
    "#                     \"translation\": [examples[\"translation\"]], \n",
    "#                     \"language\": [examples[\"language\"]]\n",
    "#                 }\n",
    "            \n",
    "#             for translation, language in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "#                 # Check if translation is dict or needs parsing\n",
    "#                 if isinstance(translation, dict):\n",
    "#                     source = translation.get(\"en\", \"\")\n",
    "#                     target = translation.get(language, \"\")\n",
    "#                 else:\n",
    "#                     # Handle string format or other formats\n",
    "#                     print(f\"⚠️ Unexpected translation format: {type(translation)}\")\n",
    "#                     continue\n",
    "                    \n",
    "#                 if not source or not target:\n",
    "#                     continue\n",
    "                \n",
    "#                 if model_config[\"type\"] == \"mbart\":\n",
    "#                     # mBART format: no special prefixes needed, handled by tokenizer\n",
    "#                     sources.append(source)\n",
    "#                     targets.append(target)\n",
    "                    \n",
    "#                 elif model_config[\"type\"] == \"t5\":\n",
    "#                     # T5 format: \"translate English to {language}: {text}\"\n",
    "#                     lang_name = {\n",
    "#                         \"yo\": \"Yoruba\", \"ar\": \"Arabic\", \"zh\": \"Chinese\", \n",
    "#                         \"ru\": \"Russian\", \"hi\": \"Hindi\", \"ja\": \"Japanese\",\n",
    "#                         \"sw\": \"Swahili\", \"bn\": \"Bengali\", \"tr\": \"Turkish\"\n",
    "#                     }.get(language, language)\n",
    "#                     sources.append(f\"translate English to {lang_name}: {source}\")\n",
    "#                     targets.append(target)\n",
    "                    \n",
    "#                 else:  # Marian\n",
    "#                     # Marian format: source as-is\n",
    "#                     sources.append(source)\n",
    "#                     targets.append(target)\n",
    "            \n",
    "#             if not sources or not targets:\n",
    "#                 print(\"⚠️ No valid source-target pairs found\")\n",
    "#                 return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "            \n",
    "#             # Dynamic length calculation on sample\n",
    "#             sample_size = min(100, len(sources))\n",
    "#             sample_sources = sources[:sample_size]\n",
    "#             sample_targets = targets[:sample_size]\n",
    "                \n",
    "#             source_lengths = [len(tokenizer.encode(s, add_special_tokens=True)) for s in sample_sources]\n",
    "#             target_lengths = [len(tokenizer.encode(t, add_special_tokens=True)) for t in sample_targets]\n",
    "            \n",
    "#             max_source_length = min(256, int(np.percentile(source_lengths, 95)) + 20)\n",
    "#             max_target_length = min(256, int(np.percentile(target_lengths, 95)) + 20)\n",
    "            \n",
    "#             print(f\"  📏 Max lengths - Source: {max_source_length}, Target: {max_target_length}\")\n",
    "            \n",
    "#             # Tokenize inputs\n",
    "#             model_inputs = tokenizer(\n",
    "#                 sources,\n",
    "#                 max_length=max_source_length,\n",
    "#                 truncation=True,\n",
    "#                 padding=\"max_length\",\n",
    "#                 return_tensors=None\n",
    "#             )\n",
    "            \n",
    "#             # Handle target tokenization based on model type\n",
    "#             if model_config[\"type\"] == \"mbart\":\n",
    "#                 # For mBART, need to handle target language properly\n",
    "#                 # Get target language for first example (assuming batch has same target lang)\n",
    "#                 first_lang = None\n",
    "#                 for translation, language in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "#                     if isinstance(translation, dict):\n",
    "#                         first_lang = language\n",
    "#                         break\n",
    "                \n",
    "#                 if first_lang and first_lang in MBART_LANG_CODES:\n",
    "#                     # Set target language\n",
    "#                     target_lang = MBART_LANG_CODES[first_lang]\n",
    "#                     tokenizer.tgt_lang = target_lang\n",
    "                    \n",
    "#                     # Tokenize with target language context\n",
    "#                     with tokenizer.as_target_tokenizer():\n",
    "#                         labels = tokenizer(\n",
    "#                             targets,\n",
    "#                             max_length=max_target_length,\n",
    "#                             truncation=True,\n",
    "#                             padding=\"max_length\",\n",
    "#                             return_tensors=None\n",
    "#                         )\n",
    "#                 else:\n",
    "#                     # Fallback without target language setting\n",
    "#                     labels = tokenizer(\n",
    "#                         targets,\n",
    "#                         max_length=max_target_length,\n",
    "#                         truncation=True,\n",
    "#                         padding=\"max_length\",\n",
    "#                         return_tensors=None\n",
    "#                     )\n",
    "#             else:\n",
    "#                 # For T5 and Marian\n",
    "#                 labels = tokenizer(\n",
    "#                     targets,\n",
    "#                     max_length=max_target_length,\n",
    "#                     truncation=True,\n",
    "#                     padding=\"max_length\",\n",
    "#                     return_tensors=None\n",
    "#                 )\n",
    "            \n",
    "#             # Replace padding with -100 for loss computation\n",
    "#             labels[\"input_ids\"] = [\n",
    "#                 [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n",
    "#                 for label_seq in labels[\"input_ids\"]\n",
    "#             ]\n",
    "            \n",
    "#             model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#             return model_inputs\n",
    "        \n",
    "#         # Preprocess dataset\n",
    "#         print(\"⚙️  Preprocessing data for better models...\")\n",
    "        \n",
    "#         # Process in smaller batches to avoid memory issues\n",
    "#         processed_dataset = full_dataset.map(\n",
    "#             preprocess_for_better_models,\n",
    "#             batched=True,\n",
    "#             remove_columns=full_dataset[\"train\"].column_names,\n",
    "#             desc=\"Preprocessing\",\n",
    "#             batch_size=100  # Reduced batch size\n",
    "#         )\n",
    "        \n",
    "#         train_dataset = processed_dataset[\"train\"]\n",
    "#         eval_dataset = processed_dataset[\"test\"]\n",
    "        \n",
    "#         # Filter out empty examples\n",
    "#         def filter_empty(example):\n",
    "#             return len(example[\"input_ids\"]) > 0 and len(example[\"labels\"]) > 0\n",
    "        \n",
    "#         train_dataset = train_dataset.filter(filter_empty)\n",
    "#         eval_dataset = eval_dataset.filter(filter_empty)\n",
    "        \n",
    "#         print(f\"✅ Preprocessed: {len(train_dataset)} train, {len(eval_dataset)} eval samples\")\n",
    "        \n",
    "#         if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "#             print(\"❌ No valid samples after preprocessing\")\n",
    "#             continue\n",
    "        \n",
    "#         # Setup training\n",
    "#         output_dir = f\"./MT_models_better/{model_id}\"\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "#         # Calculate steps for evaluation and saving\n",
    "#         steps_per_epoch = len(train_dataset) // (8 * 4)  # batch_size * gradient_accumulation_steps\n",
    "#         eval_steps = max(200, steps_per_epoch // 2)\n",
    "#         save_steps = eval_steps\n",
    "        \n",
    "#         # Optimized training arguments for pre-trained models\n",
    "#         training_args = Seq2SeqTrainingArguments(\n",
    "#             output_dir=output_dir,\n",
    "#             num_train_epochs=1,  # Reduced epochs for testing\n",
    "#             per_device_train_batch_size=4,  # Reduced batch size\n",
    "#             per_device_eval_batch_size=4,\n",
    "#             gradient_accumulation_steps=8,  # Increased to maintain effective batch size\n",
    "#             learning_rate=3e-5,  # Slightly higher LR\n",
    "#             weight_decay=0.01,\n",
    "#             warmup_steps=min(500, steps_per_epoch),\n",
    "#             eval_strategy=\"steps\",\n",
    "#             eval_steps=eval_steps,\n",
    "#             save_strategy=\"steps\", \n",
    "#             save_steps=save_steps,\n",
    "#             save_total_limit=2,\n",
    "#             logging_steps=100,\n",
    "#             report_to=\"none\",\n",
    "#             predict_with_generate=True,\n",
    "#             generation_max_length=128,\n",
    "#             generation_num_beams=2,  # Reduced beams for speed\n",
    "#             fp16=torch.cuda.is_available(),\n",
    "#             load_best_model_at_end=True,\n",
    "#             metric_for_best_model=\"bleu\",\n",
    "#             greater_is_better=True,\n",
    "#             dataloader_num_workers=0,  # Reduced workers\n",
    "#             remove_unused_columns=False,\n",
    "#             label_smoothing_factor=0.1,\n",
    "#             dataloader_pin_memory=False,  # Disable pin memory to save GPU memory\n",
    "#         )\n",
    "        \n",
    "#         # Data collator\n",
    "#         data_collator = DataCollatorForSeq2Seq(\n",
    "#             tokenizer=tokenizer,\n",
    "#             model=model,\n",
    "#             padding=True,\n",
    "#             pad_to_multiple_of=8 if training_args.fp16 else None\n",
    "#         )\n",
    "        \n",
    "#         # Trainer\n",
    "#         trainer = Seq2SeqTrainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=eval_dataset,\n",
    "#             tokenizer=tokenizer,\n",
    "#             data_collator=data_collator,\n",
    "#             compute_metrics=compute_bleu_advanced\n",
    "#         )\n",
    "        \n",
    "#         # Train\n",
    "#         print(\"🏋️  Starting training with better pre-trained model...\")\n",
    "#         trainer.train()\n",
    "        \n",
    "#         # Evaluate\n",
    "#         print(\"📊 Final evaluation...\")\n",
    "#         eval_results = trainer.evaluate()\n",
    "        \n",
    "#         # Save\n",
    "#         print(\"💾 Saving model...\")\n",
    "#         trainer.save_model()\n",
    "#         tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "#         # Log results\n",
    "#         bleu_score = eval_results.get(\"eval_bleu\", 0.0)\n",
    "#         exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "#         avg_pred_len = eval_results.get(\"eval_avg_pred_length\", 0.0)\n",
    "#         avg_label_len = eval_results.get(\"eval_avg_label_length\", 0.0)\n",
    "#         empty_preds = eval_results.get(\"eval_empty_predictions\", 0.0)\n",
    "        \n",
    "#         with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#             writer = csv.writer(csvfile)\n",
    "#             writer.writerow([\n",
    "#                 model_id, model_config[\"model_name\"], model_config[\"type\"],\n",
    "#                 len(train_dataset), len(eval_dataset), len(tokenizer),\n",
    "#                 round(bleu_score, 4), round(exact_match, 4), \n",
    "#                 round(avg_pred_len, 2), round(avg_label_len, 2),\n",
    "#                 round(empty_preds, 4), \"SUCCESS\", model_config[\"description\"]\n",
    "#             ])\n",
    "        \n",
    "#         print(f\"✅ Completed: {model_id}\")\n",
    "#         print(f\"📈 BLEU Score: {bleu_score:.4f}\")\n",
    "#         print(f\"🎯 Exact Match: {exact_match:.4f}\")\n",
    "#         print(f\"📏 Lengths - Pred: {avg_pred_len:.1f}, Label: {avg_label_len:.1f}\")\n",
    "        \n",
    "#         # Quick translation test\n",
    "#         print(\"\\n🧪 Quick translation test:\")\n",
    "#         test_input = \"Hello, how are you today?\"\n",
    "        \n",
    "#         if model_config[\"type\"] == \"t5\":\n",
    "#             test_input = \"translate English to Arabic: \" + test_input\n",
    "#         elif model_config[\"type\"] == \"mbart\":\n",
    "#             tokenizer.src_lang = \"en_XX\"\n",
    "#             tokenizer.tgt_lang = \"ar_AR\"  # Test with Arabic\n",
    "        \n",
    "#         # inputs = tokenizer(test_input, return_tensors=\"pt\", padding=True)\n",
    "#         inputs = tokenizer(test_input, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(\n",
    "#                 **inputs,\n",
    "#                 max_length=50,\n",
    "#                 num_beams=2,\n",
    "#                 early_stopping=True,\n",
    "#                 do_sample=False\n",
    "#             )\n",
    "        \n",
    "#         translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#         print(f\"  Input: {test_input}\")\n",
    "#         print(f\"  Output: {translation}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to train {model_id}: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "        \n",
    "#         with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "#             writer = csv.writer(csvfile)\n",
    "#             writer.writerow([\n",
    "#                 model_id, model_config.get(\"model_name\", \"\"), model_config.get(\"type\", \"\"),\n",
    "#                 0, 0, 0, 0, 0, 0, 0, 0, \"TRAINING_FAILED\", str(e)[:100]\n",
    "#             ])\n",
    "    \n",
    "#     finally:\n",
    "#         # Cleanup - ensure variables exist before deleting\n",
    "#         if 'model' in locals() and model is not None:\n",
    "#             del model\n",
    "#         if 'trainer' in locals() and trainer is not None:\n",
    "#             del trainer\n",
    "#         if 'tokenizer' in locals() and tokenizer is not None:\n",
    "#             del tokenizer\n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "\n",
    "# print(\"\\n🎉 Better model training completed!\")\n",
    "# print(f\"📋 Results: {results_log_file}\")\n",
    "\n",
    "# # Usage examples\n",
    "# print(f\"\\n💡 Usage examples:\")\n",
    "# print(f\"# mBART:\")\n",
    "# print(f\"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\")\n",
    "# print(f\"model = MBartForConditionalGeneration.from_pretrained('./MT_models_better/mbart_small')\")\n",
    "# print(f\"tokenizer = MBart50TokenizerFast.from_pretrained('./MT_models_better/mbart_small')\")\n",
    "\n",
    "# print(f\"\\n# Expected improvements:\")\n",
    "# print(f\"  • BLEU: 0.15-0.35 (instead of 0.0044)\")\n",
    "# print(f\"  • Better multilingual understanding\")\n",
    "# print(f\"  • More natural translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb366498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\user 4\\.conda\\envs\\multilingual_mt\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce2e7d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMBart50Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MBart50Tokenizer\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMBart50Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/mbart-large-50-many-to-many-mmt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\multilingual_mt\\lib\\site-packages\\transformers\\utils\\import_utils.py:1259\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1259\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\multilingual_mt\\lib\\site-packages\\transformers\\utils\\import_utils.py:1247\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1245\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nMBart50Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a87e2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sentencepiece 0.2.0\n",
      "Uninstalling sentencepiece-0.2.0:\n",
      "  Successfully uninstalled sentencepiece-0.2.0\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 991.5/991.5 kB 11.6 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip uninstall -y sentencepiece\n",
    "!python -m  pip install sentencepiece --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745a5e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sentencepiece\n",
    "print(sentencepiece.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79de1615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!which python  # or use %system in Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8be3d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User 4\\.conda\\envs\\multilingual_mt\\python.exe\n",
      "C:\\Users\\User 4\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n",
      "C:\\Users\\User 4\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "C:\\Users\\User 4\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1509419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffe171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading complete multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "🌍 All languages included: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr']\n",
      "🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\n",
      "📊 Training approach: ONE model per tokenizer handling ALL 9 languages\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 1/9\n",
      "🔧 Custom Tokenizer: medium_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 30002\n",
      "🔑 Special tokens - EOS: 30001, PAD: 0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 30002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n",
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 11.6983, 'grad_norm': 12.954483985900879, 'learning_rate': 1.7154811715481172e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 9.2011, 'grad_norm': 21.545574188232422, 'learning_rate': 3.765690376569038e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 8.4085, 'grad_norm': 3.1917359828948975, 'learning_rate': 5.857740585774059e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 7.7051, 'grad_norm': 12.05348014831543, 'learning_rate': 7.949790794979079e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 7.2482, 'grad_norm': 2.7623400688171387, 'learning_rate': 9.9953509995351e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 7.0132, 'grad_norm': 2.659332036972046, 'learning_rate': 9.762900976290097e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 6.7444, 'grad_norm': 2.34305739402771, 'learning_rate': 9.530450953045096e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 6.6418, 'grad_norm': 2.755898952484131, 'learning_rate': 9.298000929800093e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 6.4822, 'grad_norm': 2.4888992309570312, 'learning_rate': 9.065550906555091e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 6.2576, 'grad_norm': 2.545474052429199, 'learning_rate': 8.83310088331009e-05, 'epoch': 1.0460612405129548}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (50.5s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (100.0s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (150.3s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (198.5s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0280, Exact Match=0.0000 (198.5s total)\n",
      "{'eval_loss': 6.165963649749756, 'eval_bleu': 0.027955099066433148, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 11.9037111334002, 'eval_avg_label_length': 18.546, 'eval_empty_predictions': 0.003, 'eval_runtime': 277.8683, 'eval_samples_per_second': 3.599, 'eval_steps_per_second': 0.45, 'epoch': 1.0460612405129548}\n",
      "{'loss': 6.1651, 'grad_norm': 2.3154983520507812, 'learning_rate': 8.600650860065086e-05, 'epoch': 1.1507458780423974}\n",
      "{'loss': 6.1515, 'grad_norm': 2.4828169345855713, 'learning_rate': 8.368200836820084e-05, 'epoch': 1.25543051557184}\n",
      "{'loss': 6.1025, 'grad_norm': 2.5897395610809326, 'learning_rate': 8.135750813575082e-05, 'epoch': 1.3601151531012823}\n",
      "{'loss': 5.9841, 'grad_norm': 2.3163864612579346, 'learning_rate': 7.903300790330079e-05, 'epoch': 1.464799790630725}\n",
      "{'loss': 5.9061, 'grad_norm': 2.564626693725586, 'learning_rate': 7.670850767085078e-05, 'epoch': 1.5694844281601674}\n",
      "{'loss': 5.9342, 'grad_norm': 2.370742082595825, 'learning_rate': 7.438400743840075e-05, 'epoch': 1.6741690656896102}\n",
      "{'loss': 5.8737, 'grad_norm': 2.347428798675537, 'learning_rate': 7.205950720595072e-05, 'epoch': 1.7788537032190526}\n",
      "{'loss': 5.8942, 'grad_norm': 2.2002007961273193, 'learning_rate': 6.973500697350071e-05, 'epoch': 1.8835383407484951}\n",
      "{'loss': 5.8323, 'grad_norm': 2.3826169967651367, 'learning_rate': 6.741050674105067e-05, 'epoch': 1.9882229782779377}\n",
      "{'loss': 5.6207, 'grad_norm': 2.400874376296997, 'learning_rate': 6.508600650860064e-05, 'epoch': 2.0921224810259096}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (55.4s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (111.0s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (169.2s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (227.1s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0788, Exact Match=0.0080 (227.2s total)\n",
      "{'eval_loss': 5.692808628082275, 'eval_bleu': 0.0787695694725977, 'eval_exact_match': 0.008, 'eval_avg_pred_length': 15.246, 'eval_avg_label_length': 18.546, 'eval_empty_predictions': 0.0, 'eval_runtime': 320.8299, 'eval_samples_per_second': 3.117, 'eval_steps_per_second': 0.39, 'epoch': 2.0921224810259096}\n",
      "{'loss': 5.7198, 'grad_norm': 2.652836561203003, 'learning_rate': 6.276150627615063e-05, 'epoch': 2.196807118555352}\n",
      "{'loss': 5.6277, 'grad_norm': 2.390144109725952, 'learning_rate': 6.0437006043700606e-05, 'epoch': 2.3014917560847947}\n",
      "{'loss': 5.9366, 'grad_norm': 4.8746337890625, 'learning_rate': 5.8112505811250586e-05, 'epoch': 2.406176393614237}\n",
      "{'loss': 5.6819, 'grad_norm': 5.2748918533325195, 'learning_rate': 5.578800557880056e-05, 'epoch': 2.51086103114368}\n",
      "{'loss': 5.6982, 'grad_norm': 12.179885864257812, 'learning_rate': 5.346350534635054e-05, 'epoch': 2.615545668673122}\n",
      "{'loss': 5.6098, 'grad_norm': 8.739861488342285, 'learning_rate': 5.113900511390052e-05, 'epoch': 2.7202303062025646}\n",
      "{'loss': 5.5867, 'grad_norm': 2.4796650409698486, 'learning_rate': 4.881450488145049e-05, 'epoch': 2.8249149437320074}\n",
      "{'loss': 5.6287, 'grad_norm': 4.19431209564209, 'learning_rate': 4.649000464900047e-05, 'epoch': 2.92959958126145}\n",
      "{'loss': 5.4545, 'grad_norm': 2.668001413345337, 'learning_rate': 4.416550441655045e-05, 'epoch': 3.0334990840094216}\n",
      "{'loss': 5.4716, 'grad_norm': 3.350954055786133, 'learning_rate': 4.184100418410042e-05, 'epoch': 3.1381837215388644}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (69.1s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (131.5s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (200.1s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (266.0s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1010, Exact Match=0.0070 (266.1s total)\n",
      "{'eval_loss': 5.521420955657959, 'eval_bleu': 0.10100238208734469, 'eval_exact_match': 0.007, 'eval_avg_pred_length': 17.079, 'eval_avg_label_length': 18.546, 'eval_empty_predictions': 0.0, 'eval_runtime': 382.4647, 'eval_samples_per_second': 2.615, 'eval_steps_per_second': 0.327, 'epoch': 3.1381837215388644}\n",
      "{'loss': 5.4599, 'grad_norm': 3.594058036804199, 'learning_rate': 3.9516503951650395e-05, 'epoch': 3.2428683590683067}\n",
      "{'loss': 5.4264, 'grad_norm': 2.6661505699157715, 'learning_rate': 3.7192003719200375e-05, 'epoch': 3.3475529965977495}\n",
      "{'loss': 5.4295, 'grad_norm': 2.906961441040039, 'learning_rate': 3.4867503486750356e-05, 'epoch': 3.452237634127192}\n",
      "{'loss': 5.422, 'grad_norm': 19.86724853515625, 'learning_rate': 3.254300325430032e-05, 'epoch': 3.556922271656634}\n",
      "{'loss': 5.5086, 'grad_norm': 12.84324836730957, 'learning_rate': 3.0218503021850303e-05, 'epoch': 3.661606909186077}\n",
      "{'loss': 5.4414, 'grad_norm': 2.5503551959991455, 'learning_rate': 2.789400278940028e-05, 'epoch': 3.7662915467155194}\n",
      "{'loss': 5.4627, 'grad_norm': 2.5164308547973633, 'learning_rate': 2.556950255695026e-05, 'epoch': 3.870976184244962}\n",
      "{'loss': 5.3521, 'grad_norm': 2.382392168045044, 'learning_rate': 2.3245002324500234e-05, 'epoch': 3.9756608217744045}\n",
      "{'loss': 5.2934, 'grad_norm': 2.4412589073181152, 'learning_rate': 2.092050209205021e-05, 'epoch': 4.079560324522376}\n",
      "{'loss': 5.2938, 'grad_norm': 2.757262706756592, 'learning_rate': 1.8596001859600188e-05, 'epoch': 4.184244962051819}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (73.7s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (148.9s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (231.9s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (298.0s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1167, Exact Match=0.0080 (298.1s total)\n",
      "{'eval_loss': 5.422433376312256, 'eval_bleu': 0.11670801279476145, 'eval_exact_match': 0.008, 'eval_avg_pred_length': 18.364, 'eval_avg_label_length': 18.546, 'eval_empty_predictions': 0.0, 'eval_runtime': 419.696, 'eval_samples_per_second': 2.383, 'eval_steps_per_second': 0.298, 'epoch': 4.184244962051819}\n",
      "{'loss': 5.268, 'grad_norm': 2.646784543991089, 'learning_rate': 1.627150162715016e-05, 'epoch': 4.2889295995812615}\n",
      "{'loss': 5.2679, 'grad_norm': 2.9185240268707275, 'learning_rate': 1.394700139470014e-05, 'epoch': 4.393614237110704}\n",
      "{'loss': 5.2676, 'grad_norm': 2.876589059829712, 'learning_rate': 1.1622501162250117e-05, 'epoch': 4.498298874640146}\n",
      "{'loss': 5.2302, 'grad_norm': 2.4981353282928467, 'learning_rate': 9.298000929800094e-06, 'epoch': 4.6029835121695895}\n",
      "{'loss': 5.184, 'grad_norm': 2.642320394515991, 'learning_rate': 6.97350069735007e-06, 'epoch': 4.707668149699032}\n",
      "{'loss': 5.214, 'grad_norm': 2.5425689220428467, 'learning_rate': 4.649000464900047e-06, 'epoch': 4.812352787228474}\n",
      "{'loss': 5.1994, 'grad_norm': 2.617732286453247, 'learning_rate': 2.3245002324500235e-06, 'epoch': 4.9170374247579165}\n",
      "{'train_runtime': 3078.945, 'train_samples_per_second': 49.637, 'train_steps_per_second': 0.776, 'train_loss': 6.028411239560179, 'epoch': 5.0}\n",
      "📊 Final multilingual evaluation...\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (68.8s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (140.6s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (220.2s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (289.8s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1135, Exact Match=0.0060 (289.8s total)\n",
      "{'eval_loss': 5.381457328796387, 'eval_bleu': 0.11346328528318271, 'eval_exact_match': 0.006, 'eval_avg_pred_length': 17.987, 'eval_avg_label_length': 18.546, 'eval_empty_predictions': 0.0, 'eval_runtime': 424.0456, 'eval_samples_per_second': 2.358, 'eval_steps_per_second': 0.295, 'epoch': 5.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_medium_hf_bpe_hf\n",
      "📈 Overall BLEU Score: 0.1135\n",
      "🎯 Overall Exact Match: 0.0060\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): ح س نا ً ، ما ذا ؟\n",
      "  zh (Chinese): 你 不 知道 吗 ?\n",
      "  hi (Hindi): , तुम क्या कर रहे हो ?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 2/9\n",
      "🔧 Custom Tokenizer: medium_hf_wordpiece_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/hf_wordpiece_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 30002\n",
      "🔑 Special tokens - EOS: 30001, PAD: 0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 30002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with medium_hf_wordpiece_hf: 100%|██████████| 34376/34376 [00:05<00:00, 6544.68 examples/s]\n",
      "Preprocessing with medium_hf_wordpiece_hf: 100%|██████████| 3820/3820 [00:00<00:00, 6431.39 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:34<00:00, 889.34 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 872.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 12.2691, 'grad_norm': 13.021721839904785, 'learning_rate': 1.6317991631799166e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 9.1893, 'grad_norm': 6.743889331817627, 'learning_rate': 3.723849372384937e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 8.4695, 'grad_norm': 84.4217529296875, 'learning_rate': 5.8158995815899583e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 7.9399, 'grad_norm': 3.676532745361328, 'learning_rate': 7.90794979079498e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 7.2826, 'grad_norm': 3.3325424194335938, 'learning_rate': 0.0001, 'epoch': 0.5234231876472127}\n",
      "{'loss': 6.8487, 'grad_norm': 305.4402160644531, 'learning_rate': 9.767549976754998e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 7.3415, 'grad_norm': 2.721489429473877, 'learning_rate': 9.535099953509997e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 6.5594, 'grad_norm': 11.091630935668945, 'learning_rate': 9.302649930264994e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 6.8241, 'grad_norm': 2.958259344100952, 'learning_rate': 9.070199907019991e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 6.2859, 'grad_norm': 3.753361225128174, 'learning_rate': 8.837749883774989e-05, 'epoch': 1.0460612405129548}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (75.1s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (159.3s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (246.9s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (338.3s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0120, Exact Match=0.0000 (338.5s total)\n",
      "{'eval_loss': 6.305033206939697, 'eval_bleu': 0.011961434659277556, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 23.526, 'eval_avg_label_length': 21.64964964964965, 'eval_empty_predictions': 0.0, 'eval_runtime': 496.7242, 'eval_samples_per_second': 2.013, 'eval_steps_per_second': 0.252, 'epoch': 1.0460612405129548}\n",
      "{'loss': 6.5483, 'grad_norm': 2.7100882530212402, 'learning_rate': 8.605299860529986e-05, 'epoch': 1.1507458780423974}\n",
      "{'loss': 6.3633, 'grad_norm': 5.466279983520508, 'learning_rate': 8.372849837284983e-05, 'epoch': 1.25543051557184}\n",
      "{'loss': 6.2072, 'grad_norm': 2.587608814239502, 'learning_rate': 8.140399814039982e-05, 'epoch': 1.3601151531012823}\n",
      "{'loss': 6.0166, 'grad_norm': 2.568824052810669, 'learning_rate': 7.90794979079498e-05, 'epoch': 1.464799790630725}\n",
      "{'loss': 5.8493, 'grad_norm': 2.529658079147339, 'learning_rate': 7.675499767549977e-05, 'epoch': 1.5694844281601674}\n",
      "{'loss': 5.7715, 'grad_norm': 2.5425214767456055, 'learning_rate': 7.443049744304975e-05, 'epoch': 1.6741690656896102}\n",
      "{'loss': 5.7128, 'grad_norm': 3.3379673957824707, 'learning_rate': 7.210599721059973e-05, 'epoch': 1.7788537032190526}\n",
      "{'loss': 5.6902, 'grad_norm': 2.65828800201416, 'learning_rate': 6.97814969781497e-05, 'epoch': 1.8835383407484951}\n",
      "{'loss': 5.5984, 'grad_norm': 2.367584705352783, 'learning_rate': 6.745699674569968e-05, 'epoch': 1.9882229782779377}\n",
      "{'loss': 5.3792, 'grad_norm': 2.6489574909210205, 'learning_rate': 6.513249651324965e-05, 'epoch': 2.0921224810259096}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (74.0s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (152.7s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (231.4s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (307.5s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0349, Exact Match=0.0050 (307.6s total)\n",
      "{'eval_loss': 5.393668174743652, 'eval_bleu': 0.03490923677771297, 'eval_exact_match': 0.005, 'eval_avg_pred_length': 23.213, 'eval_avg_label_length': 21.64964964964965, 'eval_empty_predictions': 0.0, 'eval_runtime': 455.7458, 'eval_samples_per_second': 2.194, 'eval_steps_per_second': 0.274, 'epoch': 2.0921224810259096}\n",
      "{'loss': 5.4673, 'grad_norm': 2.797290563583374, 'learning_rate': 6.280799628079964e-05, 'epoch': 2.196807118555352}\n",
      "{'loss': 5.3351, 'grad_norm': 2.392547130584717, 'learning_rate': 6.048349604834961e-05, 'epoch': 2.3014917560847947}\n",
      "{'loss': 5.2943, 'grad_norm': 2.51045823097229, 'learning_rate': 5.8158995815899583e-05, 'epoch': 2.406176393614237}\n",
      "{'loss': 5.2712, 'grad_norm': 2.329240083694458, 'learning_rate': 5.5834495583449564e-05, 'epoch': 2.51086103114368}\n",
      "{'loss': 5.2065, 'grad_norm': 2.577418088912964, 'learning_rate': 5.3509995350999544e-05, 'epoch': 2.615545668673122}\n",
      "{'loss': 5.1969, 'grad_norm': 2.435398578643799, 'learning_rate': 5.118549511854951e-05, 'epoch': 2.7202303062025646}\n",
      "{'loss': 5.1776, 'grad_norm': 2.399174928665161, 'learning_rate': 4.886099488609949e-05, 'epoch': 2.8249149437320074}\n",
      "{'loss': 5.2048, 'grad_norm': 2.450650215148926, 'learning_rate': 4.6536494653649465e-05, 'epoch': 2.92959958126145}\n",
      "{'loss': 4.9742, 'grad_norm': 2.393385171890259, 'learning_rate': 4.4211994421199445e-05, 'epoch': 3.0334990840094216}\n",
      "{'loss': 4.9597, 'grad_norm': 2.858889102935791, 'learning_rate': 4.188749418874942e-05, 'epoch': 3.1381837215388644}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (77.8s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (160.1s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (245.1s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (327.4s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0787, Exact Match=0.0330 (327.5s total)\n",
      "{'eval_loss': 4.998898506164551, 'eval_bleu': 0.07869030388138004, 'eval_exact_match': 0.033, 'eval_avg_pred_length': 23.405, 'eval_avg_label_length': 21.64964964964965, 'eval_empty_predictions': 0.0, 'eval_runtime': 484.1622, 'eval_samples_per_second': 2.065, 'eval_steps_per_second': 0.258, 'epoch': 3.1381837215388644}\n",
      "{'loss': 4.9407, 'grad_norm': 2.5428452491760254, 'learning_rate': 3.95629939562994e-05, 'epoch': 3.2428683590683067}\n",
      "{'loss': 4.9092, 'grad_norm': 2.5044033527374268, 'learning_rate': 3.723849372384937e-05, 'epoch': 3.3475529965977495}\n",
      "{'loss': 4.908, 'grad_norm': 2.621413230895996, 'learning_rate': 3.4913993491399346e-05, 'epoch': 3.452237634127192}\n",
      "{'loss': 4.8941, 'grad_norm': 2.6386380195617676, 'learning_rate': 3.258949325894933e-05, 'epoch': 3.556922271656634}\n",
      "{'loss': 4.9279, 'grad_norm': 2.7239415645599365, 'learning_rate': 3.0264993026499304e-05, 'epoch': 3.661606909186077}\n",
      "{'loss': 4.8814, 'grad_norm': 30.236459732055664, 'learning_rate': 2.7940492794049284e-05, 'epoch': 3.7662915467155194}\n",
      "{'loss': 4.9, 'grad_norm': 2.3871421813964844, 'learning_rate': 2.5615992561599254e-05, 'epoch': 3.870976184244962}\n",
      "{'loss': 4.8172, 'grad_norm': 2.400102138519287, 'learning_rate': 2.3291492329149235e-05, 'epoch': 3.9756608217744045}\n",
      "{'loss': 4.7473, 'grad_norm': 2.5545899868011475, 'learning_rate': 2.096699209669921e-05, 'epoch': 4.079560324522376}\n",
      "{'loss': 4.7385, 'grad_norm': 2.352705240249634, 'learning_rate': 1.864249186424919e-05, 'epoch': 4.184244962051819}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (74.1s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (155.5s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (234.8s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (309.4s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1016, Exact Match=0.0500 (309.5s total)\n",
      "{'eval_loss': 4.836324214935303, 'eval_bleu': 0.10162257328401596, 'eval_exact_match': 0.05, 'eval_avg_pred_length': 23.132, 'eval_avg_label_length': 21.64964964964965, 'eval_empty_predictions': 0.0, 'eval_runtime': 455.7544, 'eval_samples_per_second': 2.194, 'eval_steps_per_second': 0.274, 'epoch': 4.184244962051819}\n",
      "{'loss': 4.7334, 'grad_norm': 2.580963373184204, 'learning_rate': 1.6317991631799166e-05, 'epoch': 4.2889295995812615}\n",
      "{'loss': 4.7341, 'grad_norm': 2.8072564601898193, 'learning_rate': 1.399349139934914e-05, 'epoch': 4.393614237110704}\n",
      "{'loss': 4.7335, 'grad_norm': 2.665405035018921, 'learning_rate': 1.1668991166899116e-05, 'epoch': 4.498298874640146}\n",
      "{'loss': 4.7098, 'grad_norm': 7.0768842697143555, 'learning_rate': 9.344490934449093e-06, 'epoch': 4.6029835121695895}\n",
      "{'loss': 4.6886, 'grad_norm': 2.832073450088501, 'learning_rate': 7.01999070199907e-06, 'epoch': 4.707668149699032}\n",
      "{'loss': 4.7631, 'grad_norm': 2.8224804401397705, 'learning_rate': 4.695490469549047e-06, 'epoch': 4.812352787228474}\n",
      "{'loss': 4.7037, 'grad_norm': 2.570794105529785, 'learning_rate': 2.370990237099024e-06, 'epoch': 4.9170374247579165}\n",
      "{'train_runtime': 3577.0979, 'train_samples_per_second': 42.725, 'train_steps_per_second': 0.668, 'train_loss': 5.768132810712359, 'epoch': 5.0}\n",
      "📊 Final multilingual evaluation...\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (76.6s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (157.8s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (245.2s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (321.9s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1136, Exact Match=0.0670 (322.0s total)\n",
      "{'eval_loss': 4.791514873504639, 'eval_bleu': 0.11355553179539939, 'eval_exact_match': 0.067, 'eval_avg_pred_length': 23.297, 'eval_avg_label_length': 21.64964964964965, 'eval_empty_predictions': 0.0, 'eval_runtime': 463.8947, 'eval_samples_per_second': 2.156, 'eval_steps_per_second': 0.269, 'epoch': 5.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_medium_hf_wordpiece_hf\n",
      "📈 Overall BLEU Score: 0.1136\n",
      "🎯 Overall Exact Match: 0.0670\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): ح ##سن ##اً ، ما ##ذا ت ##فع ##له ؟\n",
      "  zh (Chinese): 好 ##吧 , h ##ow y ##ou to ##da ##g ##da ##y ?\n",
      "  hi (Hindi): क्या त ##ु ##म् ##हार ##े लिए त ##ु ##म क्या कर रहे हो ?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 3/9\n",
      "🔧 Custom Tokenizer: medium_sp_unigram_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finalmedium/sp_unigram_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 30002\n",
      "🔑 Special tokens - EOS: 2, PAD: 30000\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 30002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with medium_sp_unigram_hf: 100%|██████████| 34376/34376 [00:04<00:00, 7796.38 examples/s]\n",
      "Preprocessing with medium_sp_unigram_hf: 100%|██████████| 3820/3820 [00:00<00:00, 7538.05 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:30<00:00, 1012.19 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 1011.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 11.3399, 'grad_norm': 3.4217100143432617, 'learning_rate': 1.7573221757322174e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 8.7204, 'grad_norm': 4.216411590576172, 'learning_rate': 3.849372384937239e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 8.2838, 'grad_norm': 3.684976577758789, 'learning_rate': 5.94142259414226e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 7.8448, 'grad_norm': 2.833085536956787, 'learning_rate': 8.03347280334728e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 7.2812, 'grad_norm': 3.3034207820892334, 'learning_rate': 9.9860529986053e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 6.8879, 'grad_norm': 3.2613837718963623, 'learning_rate': 9.753602975360298e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 6.5371, 'grad_norm': 2.1150360107421875, 'learning_rate': 9.521152952115295e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 6.4454, 'grad_norm': 3.429581642150879, 'learning_rate': 9.288702928870293e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 6.2547, 'grad_norm': 2.5029494762420654, 'learning_rate': 9.056252905625291e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 5.9825, 'grad_norm': 2.467186450958252, 'learning_rate': 8.823802882380289e-05, 'epoch': 1.0460612405129548}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (81.0s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (156.7s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (235.9s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (305.2s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0081, Exact Match=0.0010 (305.3s total)\n",
      "{'eval_loss': 5.912182807922363, 'eval_bleu': 0.008142985233377415, 'eval_exact_match': 0.001, 'eval_avg_pred_length': 10.136, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 442.8105, 'eval_samples_per_second': 2.258, 'eval_steps_per_second': 0.282, 'epoch': 1.0460612405129548}\n",
      "{'loss': 5.9188, 'grad_norm': 2.1715145111083984, 'learning_rate': 8.591352859135286e-05, 'epoch': 1.1507458780423974}\n",
      "{'loss': 6.1108, 'grad_norm': 2.5119659900665283, 'learning_rate': 8.358902835890285e-05, 'epoch': 1.25543051557184}\n",
      "{'loss': 5.9054, 'grad_norm': 2.765846014022827, 'learning_rate': 8.126452812645282e-05, 'epoch': 1.3601151531012823}\n",
      "{'loss': 5.8756, 'grad_norm': 2.6919655799865723, 'learning_rate': 7.89400278940028e-05, 'epoch': 1.464799790630725}\n",
      "{'loss': 5.7524, 'grad_norm': 2.377687454223633, 'learning_rate': 7.661552766155277e-05, 'epoch': 1.5694844281601674}\n",
      "{'loss': 5.7422, 'grad_norm': 2.307039260864258, 'learning_rate': 7.429102742910274e-05, 'epoch': 1.6741690656896102}\n",
      "{'loss': 5.6887, 'grad_norm': 2.261833667755127, 'learning_rate': 7.196652719665272e-05, 'epoch': 1.7788537032190526}\n",
      "{'loss': 5.666, 'grad_norm': 11.209966659545898, 'learning_rate': 6.96420269642027e-05, 'epoch': 1.8835383407484951}\n",
      "{'loss': 5.6346, 'grad_norm': 2.4871668815612793, 'learning_rate': 6.731752673175268e-05, 'epoch': 1.9882229782779377}\n",
      "{'loss': 5.3987, 'grad_norm': 2.5672848224639893, 'learning_rate': 6.499302649930265e-05, 'epoch': 2.0921224810259096}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (64.6s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (122.1s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (181.8s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (239.7s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0554, Exact Match=0.0340 (239.7s total)\n",
      "{'eval_loss': 5.477763652801514, 'eval_bleu': 0.05540235994726534, 'eval_exact_match': 0.034, 'eval_avg_pred_length': 6.722, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 351.2962, 'eval_samples_per_second': 2.847, 'eval_steps_per_second': 0.356, 'epoch': 2.0921224810259096}\n",
      "{'loss': 5.4825, 'grad_norm': 2.786635160446167, 'learning_rate': 6.266852626685264e-05, 'epoch': 2.196807118555352}\n",
      "{'loss': 5.3793, 'grad_norm': 2.6585195064544678, 'learning_rate': 6.0344026034402604e-05, 'epoch': 2.3014917560847947}\n",
      "{'loss': 5.3394, 'grad_norm': 2.6221089363098145, 'learning_rate': 5.801952580195258e-05, 'epoch': 2.406176393614237}\n",
      "{'loss': 5.3288, 'grad_norm': 2.460247039794922, 'learning_rate': 5.569502556950256e-05, 'epoch': 2.51086103114368}\n",
      "{'loss': 5.2848, 'grad_norm': 2.633023738861084, 'learning_rate': 5.337052533705254e-05, 'epoch': 2.615545668673122}\n",
      "{'loss': 5.3071, 'grad_norm': 2.687786340713501, 'learning_rate': 5.104602510460251e-05, 'epoch': 2.7202303062025646}\n",
      "{'loss': 5.2956, 'grad_norm': 2.484440803527832, 'learning_rate': 4.872152487215249e-05, 'epoch': 2.8249149437320074}\n",
      "{'loss': 5.2816, 'grad_norm': 2.73563814163208, 'learning_rate': 4.6397024639702466e-05, 'epoch': 2.92959958126145}\n",
      "{'loss': 5.0996, 'grad_norm': 2.646819591522217, 'learning_rate': 4.407252440725244e-05, 'epoch': 3.0334990840094216}\n",
      "{'loss': 5.0875, 'grad_norm': 2.8045105934143066, 'learning_rate': 4.174802417480242e-05, 'epoch': 3.1381837215388644}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (65.2s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (128.6s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (192.4s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (258.4s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0777, Exact Match=0.0670 (258.4s total)\n",
      "{'eval_loss': 5.23481559753418, 'eval_bleu': 0.07771445468322341, 'eval_exact_match': 0.067, 'eval_avg_pred_length': 7.297, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 365.5068, 'eval_samples_per_second': 2.736, 'eval_steps_per_second': 0.342, 'epoch': 3.1381837215388644}\n",
      "{'loss': 5.0699, 'grad_norm': 2.606218099594116, 'learning_rate': 3.94235239423524e-05, 'epoch': 3.2428683590683067}\n",
      "{'loss': 5.0252, 'grad_norm': 2.6987757682800293, 'learning_rate': 3.7099023709902373e-05, 'epoch': 3.3475529965977495}\n",
      "{'loss': 5.0381, 'grad_norm': 2.7992312908172607, 'learning_rate': 3.477452347745235e-05, 'epoch': 3.452237634127192}\n",
      "{'loss': 5.0242, 'grad_norm': 2.818303108215332, 'learning_rate': 3.245002324500233e-05, 'epoch': 3.556922271656634}\n",
      "{'loss': 5.0669, 'grad_norm': 2.7573068141937256, 'learning_rate': 3.0125523012552304e-05, 'epoch': 3.661606909186077}\n",
      "{'loss': 5.0278, 'grad_norm': 2.557452917098999, 'learning_rate': 2.780102278010228e-05, 'epoch': 3.7662915467155194}\n",
      "{'loss': 5.0512, 'grad_norm': 2.6105458736419678, 'learning_rate': 2.5476522547652255e-05, 'epoch': 3.870976184244962}\n",
      "{'loss': 4.9674, 'grad_norm': 2.4662609100341797, 'learning_rate': 2.3152022315202232e-05, 'epoch': 3.9756608217744045}\n",
      "{'loss': 4.9123, 'grad_norm': 2.482431173324585, 'learning_rate': 2.082752208275221e-05, 'epoch': 4.079560324522376}\n",
      "{'loss': 4.8911, 'grad_norm': 2.657850980758667, 'learning_rate': 1.8503021850302186e-05, 'epoch': 4.184244962051819}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (65.1s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (126.1s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (193.2s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (251.2s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0919, Exact Match=0.0790 (251.3s total)\n",
      "{'eval_loss': 5.116468906402588, 'eval_bleu': 0.09189691304690817, 'eval_exact_match': 0.079, 'eval_avg_pred_length': 7.285, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 357.6898, 'eval_samples_per_second': 2.796, 'eval_steps_per_second': 0.349, 'epoch': 4.184244962051819}\n",
      "{'loss': 4.8575, 'grad_norm': 2.8115651607513428, 'learning_rate': 1.6178521617852163e-05, 'epoch': 4.2889295995812615}\n",
      "{'loss': 4.8582, 'grad_norm': 2.6618597507476807, 'learning_rate': 1.3854021385402138e-05, 'epoch': 4.393614237110704}\n",
      "{'loss': 4.8501, 'grad_norm': 2.7140378952026367, 'learning_rate': 1.1529521152952117e-05, 'epoch': 4.498298874640146}\n",
      "{'loss': 4.8487, 'grad_norm': 2.5081875324249268, 'learning_rate': 9.205020920502094e-06, 'epoch': 4.6029835121695895}\n",
      "{'loss': 4.8008, 'grad_norm': 2.4816811084747314, 'learning_rate': 6.88052068805207e-06, 'epoch': 4.707668149699032}\n",
      "{'loss': 4.8276, 'grad_norm': 2.739205837249756, 'learning_rate': 4.556020455602046e-06, 'epoch': 4.812352787228474}\n",
      "{'loss': 4.8202, 'grad_norm': 2.522071361541748, 'learning_rate': 2.2315202231520224e-06, 'epoch': 4.9170374247579165}\n",
      "{'train_runtime': 3183.6003, 'train_samples_per_second': 48.005, 'train_steps_per_second': 0.751, 'train_loss': 5.731290611362856, 'epoch': 5.0}\n",
      "📊 Final multilingual evaluation...\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (69.4s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (133.9s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (201.9s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (263.5s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0966, Exact Match=0.0860 (263.5s total)\n",
      "{'eval_loss': 5.074953079223633, 'eval_bleu': 0.09664645000476077, 'eval_exact_match': 0.086, 'eval_avg_pred_length': 7.577, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 377.2154, 'eval_samples_per_second': 2.651, 'eval_steps_per_second': 0.331, 'epoch': 5.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_medium_sp_unigram_hf\n",
      "📈 Overall BLEU Score: 0.0966\n",
      "🎯 Overall Exact Match: 0.0860\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): حسناً، هل تفعل ذلك؟\n",
      "  zh (Chinese): 好吧, 你好吗?\n",
      "  hi (Hindi): उसने कहा, \"मैं तुम्हें क्या कर रहे हो?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 4/9\n",
      "🔧 Custom Tokenizer: large_hf_bpe_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finallarge/hf_bpe_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 50002\n",
      "🔑 Special tokens - EOS: 50001, PAD: 0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 50002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with large_hf_bpe_hf: 100%|██████████| 34376/34376 [00:04<00:00, 7873.34 examples/s]\n",
      "Preprocessing with large_hf_bpe_hf: 100%|██████████| 3820/3820 [00:00<00:00, 7625.90 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:29<00:00, 1038.61 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 1033.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 12.3333, 'grad_norm': 39.86501693725586, 'learning_rate': 1.6317991631799166e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 9.5082, 'grad_norm': 4.491843223571777, 'learning_rate': 3.723849372384937e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 8.5616, 'grad_norm': 2.9539096355438232, 'learning_rate': 5.8158995815899583e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 7.9838, 'grad_norm': 3.2830698490142822, 'learning_rate': 7.90794979079498e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 7.5525, 'grad_norm': 3.104830741882324, 'learning_rate': 0.0001, 'epoch': 0.5234231876472127}\n",
      "{'loss': 7.3278, 'grad_norm': 3.121720552444458, 'learning_rate': 9.767549976754998e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 7.1855, 'grad_norm': 4.011468410491943, 'learning_rate': 9.535099953509997e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 7.1486, 'grad_norm': 43.45774841308594, 'learning_rate': 9.302649930264994e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 7.0581, 'grad_norm': 3.0987708568573, 'learning_rate': 9.070199907019991e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 6.8282, 'grad_norm': 4.003382682800293, 'learning_rate': 8.837749883774989e-05, 'epoch': 1.0460612405129548}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (95.8s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (190.6s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (289.3s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (381.6s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0271, Exact Match=0.0010 (381.7s total)\n",
      "{'eval_loss': 6.65108060836792, 'eval_bleu': 0.027127270070219915, 'eval_exact_match': 0.001, 'eval_avg_pred_length': 21.698, 'eval_avg_label_length': 16.973, 'eval_empty_predictions': 0.0, 'eval_runtime': 515.6182, 'eval_samples_per_second': 1.939, 'eval_steps_per_second': 0.242, 'epoch': 1.0460612405129548}\n",
      "{'loss': 6.596, 'grad_norm': 3.339005947113037, 'learning_rate': 8.605299860529986e-05, 'epoch': 1.1507458780423974}\n",
      "{'loss': 6.5981, 'grad_norm': 5.110861778259277, 'learning_rate': 8.372849837284983e-05, 'epoch': 1.25543051557184}\n",
      "{'loss': 6.5449, 'grad_norm': 3.8237645626068115, 'learning_rate': 8.140399814039982e-05, 'epoch': 1.3601151531012823}\n",
      "{'loss': 6.4128, 'grad_norm': 2.5747122764587402, 'learning_rate': 7.90794979079498e-05, 'epoch': 1.464799790630725}\n",
      "{'loss': 6.3012, 'grad_norm': 2.6670632362365723, 'learning_rate': 7.675499767549977e-05, 'epoch': 1.5694844281601674}\n",
      "{'loss': 6.3492, 'grad_norm': 2.6480846405029297, 'learning_rate': 7.443049744304975e-05, 'epoch': 1.6741690656896102}\n",
      "{'loss': 6.3198, 'grad_norm': 10.612813949584961, 'learning_rate': 7.210599721059973e-05, 'epoch': 1.7788537032190526}\n",
      "{'loss': 6.3211, 'grad_norm': 2.3381478786468506, 'learning_rate': 6.97814969781497e-05, 'epoch': 1.8835383407484951}\n",
      "{'loss': 6.2482, 'grad_norm': 2.8226559162139893, 'learning_rate': 6.745699674569968e-05, 'epoch': 1.9882229782779377}\n",
      "{'loss': 6.0006, 'grad_norm': 2.878108501434326, 'learning_rate': 6.513249651324965e-05, 'epoch': 2.0921224810259096}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (84.9s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (161.5s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (244.4s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (324.7s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0426, Exact Match=0.0050 (324.8s total)\n",
      "{'eval_loss': 6.085799694061279, 'eval_bleu': 0.042574497472195924, 'eval_exact_match': 0.005, 'eval_avg_pred_length': 18.358, 'eval_avg_label_length': 16.973, 'eval_empty_predictions': 0.0, 'eval_runtime': 430.7057, 'eval_samples_per_second': 2.322, 'eval_steps_per_second': 0.29, 'epoch': 2.0921224810259096}\n",
      "{'loss': 6.1191, 'grad_norm': 19.246824264526367, 'learning_rate': 6.280799628079964e-05, 'epoch': 2.196807118555352}\n",
      "{'loss': 6.004, 'grad_norm': 3.191696882247925, 'learning_rate': 6.048349604834961e-05, 'epoch': 2.3014917560847947}\n",
      "{'loss': 5.9935, 'grad_norm': 4.718059539794922, 'learning_rate': 5.8158995815899583e-05, 'epoch': 2.406176393614237}\n",
      "{'loss': 6.0096, 'grad_norm': 3.1847145557403564, 'learning_rate': 5.5834495583449564e-05, 'epoch': 2.51086103114368}\n",
      "{'loss': 5.9346, 'grad_norm': 2.5157651901245117, 'learning_rate': 5.3509995350999544e-05, 'epoch': 2.615545668673122}\n",
      "{'loss': 5.9475, 'grad_norm': 2.1983046531677246, 'learning_rate': 5.118549511854951e-05, 'epoch': 2.7202303062025646}\n",
      "{'loss': 5.9342, 'grad_norm': 2.582315683364868, 'learning_rate': 4.886099488609949e-05, 'epoch': 2.8249149437320074}\n",
      "{'loss': 5.9574, 'grad_norm': 53.67562484741211, 'learning_rate': 4.6536494653649465e-05, 'epoch': 2.92959958126145}\n",
      "{'loss': 5.7083, 'grad_norm': 4.638682842254639, 'learning_rate': 4.4211994421199445e-05, 'epoch': 3.0334990840094216}\n",
      "{'loss': 5.732, 'grad_norm': 2.8616952896118164, 'learning_rate': 4.188749418874942e-05, 'epoch': 3.1381837215388644}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (95.9s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (184.0s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (280.5s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (369.5s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0821, Exact Match=0.0360 (369.6s total)\n",
      "{'eval_loss': 5.851266384124756, 'eval_bleu': 0.08212927964887112, 'eval_exact_match': 0.036, 'eval_avg_pred_length': 20.462, 'eval_avg_label_length': 16.973, 'eval_empty_predictions': 0.0, 'eval_runtime': 496.8606, 'eval_samples_per_second': 2.013, 'eval_steps_per_second': 0.252, 'epoch': 3.1381837215388644}\n",
      "{'loss': 5.722, 'grad_norm': 2.5984809398651123, 'learning_rate': 3.95629939562994e-05, 'epoch': 3.2428683590683067}\n",
      "{'loss': 5.7039, 'grad_norm': 2.58516263961792, 'learning_rate': 3.723849372384937e-05, 'epoch': 3.3475529965977495}\n",
      "{'loss': 5.6972, 'grad_norm': 2.572784662246704, 'learning_rate': 3.4913993491399346e-05, 'epoch': 3.452237634127192}\n",
      "{'loss': 5.6941, 'grad_norm': 2.7924463748931885, 'learning_rate': 3.258949325894933e-05, 'epoch': 3.556922271656634}\n",
      "{'loss': 5.7562, 'grad_norm': 2.9483280181884766, 'learning_rate': 3.0264993026499304e-05, 'epoch': 3.661606909186077}\n",
      "{'loss': 5.6785, 'grad_norm': 2.6069376468658447, 'learning_rate': 2.7940492794049284e-05, 'epoch': 3.7662915467155194}\n",
      "{'loss': 5.7061, 'grad_norm': 2.4358651638031006, 'learning_rate': 2.5615992561599254e-05, 'epoch': 3.870976184244962}\n",
      "{'loss': 5.6073, 'grad_norm': 2.262213706970215, 'learning_rate': 2.3291492329149235e-05, 'epoch': 3.9756608217744045}\n",
      "{'loss': 5.5343, 'grad_norm': 2.3782896995544434, 'learning_rate': 2.096699209669921e-05, 'epoch': 4.079560324522376}\n",
      "{'loss': 5.5553, 'grad_norm': 2.4746923446655273, 'learning_rate': 1.864249186424919e-05, 'epoch': 4.184244962051819}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (100.6s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (187.6s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (291.0s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (383.3s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1054, Exact Match=0.0630 (383.4s total)\n",
      "{'eval_loss': 5.731821060180664, 'eval_bleu': 0.10543047048572256, 'eval_exact_match': 0.063, 'eval_avg_pred_length': 22.112, 'eval_avg_label_length': 16.973, 'eval_empty_predictions': 0.0, 'eval_runtime': 515.4826, 'eval_samples_per_second': 1.94, 'eval_steps_per_second': 0.242, 'epoch': 4.184244962051819}\n",
      "{'loss': 5.529, 'grad_norm': 2.5997509956359863, 'learning_rate': 1.6317991631799166e-05, 'epoch': 4.2889295995812615}\n",
      "{'loss': 5.523, 'grad_norm': 2.674621820449829, 'learning_rate': 1.399349139934914e-05, 'epoch': 4.393614237110704}\n",
      "{'loss': 5.5488, 'grad_norm': 2.818511962890625, 'learning_rate': 1.1668991166899116e-05, 'epoch': 4.498298874640146}\n",
      "{'loss': 5.4928, 'grad_norm': 2.540236711502075, 'learning_rate': 9.344490934449093e-06, 'epoch': 4.6029835121695895}\n",
      "{'loss': 5.4494, 'grad_norm': 2.5827744007110596, 'learning_rate': 7.01999070199907e-06, 'epoch': 4.707668149699032}\n",
      "{'loss': 5.5046, 'grad_norm': 2.5150535106658936, 'learning_rate': 4.695490469549047e-06, 'epoch': 4.812352787228474}\n",
      "{'loss': 5.4731, 'grad_norm': 2.5497312545776367, 'learning_rate': 2.370990237099024e-06, 'epoch': 4.9170374247579165}\n",
      "{'train_runtime': 3711.3804, 'train_samples_per_second': 41.179, 'train_steps_per_second': 0.644, 'train_loss': 6.361609429395348, 'epoch': 5.0}\n",
      "📊 Final multilingual evaluation...\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (100.7s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (194.8s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (293.4s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (387.7s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1039, Exact Match=0.0550 (387.8s total)\n",
      "{'eval_loss': 5.692331314086914, 'eval_bleu': 0.10391631561052277, 'eval_exact_match': 0.055, 'eval_avg_pred_length': 22.353, 'eval_avg_label_length': 16.973, 'eval_empty_predictions': 0.0, 'eval_runtime': 523.1351, 'eval_samples_per_second': 1.912, 'eval_steps_per_second': 0.239, 'epoch': 5.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_large_hf_bpe_hf\n",
      "📈 Overall BLEU Score: 0.1039\n",
      "🎯 Overall Exact Match: 0.0550\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): حس نا ً ، أ ين نا ً ؟\n",
      "  zh (Chinese): - 你 不 知道 ?\n",
      "  hi (Hindi): आप , क्या तुम क्या कर रहे हो ?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 5/9\n",
      "🔧 Custom Tokenizer: large_hf_wordpiece_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finallarge/hf_wordpiece_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 50002\n",
      "🔑 Special tokens - EOS: 50001, PAD: 0\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 50002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with large_hf_wordpiece_hf: 100%|██████████| 34376/34376 [00:04<00:00, 7985.20 examples/s]\n",
      "Preprocessing with large_hf_wordpiece_hf: 100%|██████████| 3820/3820 [00:00<00:00, 7898.79 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:29<00:00, 1042.53 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 1041.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 12.4722, 'grad_norm': 8.382189750671387, 'learning_rate': 1.6736401673640167e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 9.6524, 'grad_norm': 8.992897987365723, 'learning_rate': 3.765690376569038e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 9.0752, 'grad_norm': 4.165016174316406, 'learning_rate': 5.857740585774059e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 8.3755, 'grad_norm': 12.15662956237793, 'learning_rate': 7.949790794979079e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 8.0045, 'grad_norm': 2.9229161739349365, 'learning_rate': 9.9953509995351e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 7.7123, 'grad_norm': 24.226835250854492, 'learning_rate': 9.762900976290097e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 7.4933, 'grad_norm': 3.233417510986328, 'learning_rate': 9.530450953045096e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 7.4505, 'grad_norm': 7.1789164543151855, 'learning_rate': 9.298000929800093e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 7.1726, 'grad_norm': 2.9940457344055176, 'learning_rate': 9.065550906555091e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 6.8637, 'grad_norm': 2.7126708030700684, 'learning_rate': 8.83310088331009e-05, 'epoch': 1.0460612405129548}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (77.1s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (154.1s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (227.5s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (304.4s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0300, Exact Match=0.0000 (304.5s total)\n",
      "{'eval_loss': 6.767971515655518, 'eval_bleu': 0.030028439985277003, 'eval_exact_match': 0.0, 'eval_avg_pred_length': 13.994, 'eval_avg_label_length': 17.95895895895896, 'eval_empty_predictions': 0.0, 'eval_runtime': 400.2734, 'eval_samples_per_second': 2.498, 'eval_steps_per_second': 0.312, 'epoch': 1.0460612405129548}\n",
      "{'loss': 6.8254, 'grad_norm': 9.334978103637695, 'learning_rate': 8.600650860065086e-05, 'epoch': 1.1507458780423974}\n",
      "{'loss': 6.7307, 'grad_norm': 2.693002462387085, 'learning_rate': 8.368200836820084e-05, 'epoch': 1.25543051557184}\n",
      "{'loss': 6.6123, 'grad_norm': 18.2917537689209, 'learning_rate': 8.135750813575082e-05, 'epoch': 1.3601151531012823}\n",
      "{'loss': 6.4596, 'grad_norm': 2.568268299102783, 'learning_rate': 7.903300790330079e-05, 'epoch': 1.464799790630725}\n",
      "{'loss': 6.3553, 'grad_norm': 2.4839766025543213, 'learning_rate': 7.670850767085078e-05, 'epoch': 1.5694844281601674}\n",
      "{'loss': 6.3407, 'grad_norm': 2.5611913204193115, 'learning_rate': 7.438400743840075e-05, 'epoch': 1.6741690656896102}\n",
      "{'loss': 6.2631, 'grad_norm': 2.3657007217407227, 'learning_rate': 7.205950720595072e-05, 'epoch': 1.7788537032190526}\n",
      "{'loss': 6.2744, 'grad_norm': 2.4749464988708496, 'learning_rate': 6.973500697350071e-05, 'epoch': 1.8835383407484951}\n",
      "{'loss': 6.1846, 'grad_norm': 2.4074325561523438, 'learning_rate': 6.741050674105067e-05, 'epoch': 1.9882229782779377}\n",
      "{'loss': 5.9344, 'grad_norm': 2.6413960456848145, 'learning_rate': 6.508600650860064e-05, 'epoch': 2.0921224810259096}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (81.1s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (162.9s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (245.7s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (322.4s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0651, Exact Match=0.0210 (322.5s total)\n",
      "{'eval_loss': 6.034422397613525, 'eval_bleu': 0.06505347563754749, 'eval_exact_match': 0.021, 'eval_avg_pred_length': 16.484422110552764, 'eval_avg_label_length': 17.95895895895896, 'eval_empty_predictions': 0.005, 'eval_runtime': 427.7469, 'eval_samples_per_second': 2.338, 'eval_steps_per_second': 0.292, 'epoch': 2.0921224810259096}\n",
      "{'loss': 6.0457, 'grad_norm': 2.56921124458313, 'learning_rate': 6.276150627615063e-05, 'epoch': 2.196807118555352}\n",
      "{'loss': 5.9113, 'grad_norm': 2.326075792312622, 'learning_rate': 6.0437006043700606e-05, 'epoch': 2.3014917560847947}\n",
      "{'loss': 5.8889, 'grad_norm': 2.5639872550964355, 'learning_rate': 5.8112505811250586e-05, 'epoch': 2.406176393614237}\n",
      "{'loss': 5.9049, 'grad_norm': 2.504946231842041, 'learning_rate': 5.578800557880056e-05, 'epoch': 2.51086103114368}\n",
      "{'loss': 5.8418, 'grad_norm': 2.548954486846924, 'learning_rate': 5.346350534635054e-05, 'epoch': 2.615545668673122}\n",
      "{'loss': 5.8398, 'grad_norm': 2.398975372314453, 'learning_rate': 5.113900511390052e-05, 'epoch': 2.7202303062025646}\n",
      "{'loss': 5.8363, 'grad_norm': 2.331861972808838, 'learning_rate': 4.881450488145049e-05, 'epoch': 2.8249149437320074}\n",
      "{'loss': 5.8542, 'grad_norm': 2.7937159538269043, 'learning_rate': 4.649000464900047e-05, 'epoch': 2.92959958126145}\n",
      "{'loss': 5.6015, 'grad_norm': 2.482257127761841, 'learning_rate': 4.416550441655045e-05, 'epoch': 3.0334990840094216}\n",
      "{'loss': 5.6192, 'grad_norm': 2.8739817142486572, 'learning_rate': 4.184100418410042e-05, 'epoch': 3.1381837215388644}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (96.8s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (190.9s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (285.7s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (370.3s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1076, Exact Match=0.0560 (370.3s total)\n",
      "{'eval_loss': 5.737259864807129, 'eval_bleu': 0.10763567400318202, 'eval_exact_match': 0.056, 'eval_avg_pred_length': 17.876008064516128, 'eval_avg_label_length': 17.95895895895896, 'eval_empty_predictions': 0.008, 'eval_runtime': 492.8109, 'eval_samples_per_second': 2.029, 'eval_steps_per_second': 0.254, 'epoch': 3.1381837215388644}\n",
      "{'loss': 5.5963, 'grad_norm': 2.571502208709717, 'learning_rate': 3.9516503951650395e-05, 'epoch': 3.2428683590683067}\n",
      "{'loss': 5.5599, 'grad_norm': 2.6804018020629883, 'learning_rate': 3.7192003719200375e-05, 'epoch': 3.3475529965977495}\n",
      "{'loss': 5.5753, 'grad_norm': 2.6374948024749756, 'learning_rate': 3.4867503486750356e-05, 'epoch': 3.452237634127192}\n",
      "{'loss': 5.5519, 'grad_norm': 2.6701135635375977, 'learning_rate': 3.254300325430032e-05, 'epoch': 3.556922271656634}\n",
      "{'loss': 5.6062, 'grad_norm': 2.700047016143799, 'learning_rate': 3.0218503021850303e-05, 'epoch': 3.661606909186077}\n",
      "{'loss': 5.5553, 'grad_norm': 2.5952205657958984, 'learning_rate': 2.789400278940028e-05, 'epoch': 3.7662915467155194}\n",
      "{'loss': 5.5883, 'grad_norm': 2.522141456604004, 'learning_rate': 2.556950255695026e-05, 'epoch': 3.870976184244962}\n",
      "{'loss': 5.4704, 'grad_norm': 2.35361909866333, 'learning_rate': 2.3245002324500234e-05, 'epoch': 3.9756608217744045}\n",
      "{'loss': 5.4051, 'grad_norm': 2.315110445022583, 'learning_rate': 2.092050209205021e-05, 'epoch': 4.079560324522376}\n",
      "{'loss': 5.4099, 'grad_norm': 2.4995477199554443, 'learning_rate': 1.8596001859600188e-05, 'epoch': 4.184244962051819}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (93.5s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (191.4s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (288.7s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (377.4s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1204, Exact Match=0.0740 (377.4s total)\n",
      "{'eval_loss': 5.5953192710876465, 'eval_bleu': 0.12043236576239112, 'eval_exact_match': 0.074, 'eval_avg_pred_length': 20.074371859296484, 'eval_avg_label_length': 17.95895895895896, 'eval_empty_predictions': 0.005, 'eval_runtime': 503.0678, 'eval_samples_per_second': 1.988, 'eval_steps_per_second': 0.248, 'epoch': 4.184244962051819}\n",
      "{'loss': 5.386, 'grad_norm': 2.6478331089019775, 'learning_rate': 1.627150162715016e-05, 'epoch': 4.2889295995812615}\n",
      "{'loss': 5.3819, 'grad_norm': 2.729702949523926, 'learning_rate': 1.394700139470014e-05, 'epoch': 4.393614237110704}\n",
      "{'loss': 5.4093, 'grad_norm': 2.8534085750579834, 'learning_rate': 1.1622501162250117e-05, 'epoch': 4.498298874640146}\n",
      "{'loss': 5.364, 'grad_norm': 2.570964813232422, 'learning_rate': 9.298000929800094e-06, 'epoch': 4.6029835121695895}\n",
      "{'loss': 5.2991, 'grad_norm': 2.560394763946533, 'learning_rate': 6.97350069735007e-06, 'epoch': 4.707668149699032}\n",
      "{'loss': 5.3582, 'grad_norm': 2.6217164993286133, 'learning_rate': 4.649000464900047e-06, 'epoch': 4.812352787228474}\n",
      "{'loss': 5.3237, 'grad_norm': 2.66408371925354, 'learning_rate': 2.3245002324500235e-06, 'epoch': 4.9170374247579165}\n",
      "{'train_runtime': 3581.3903, 'train_samples_per_second': 42.673, 'train_steps_per_second': 0.667, 'train_loss': 6.353629727543148, 'epoch': 5.0}\n",
      "📊 Final multilingual evaluation...\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (102.0s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (197.0s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (300.1s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (385.3s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1218, Exact Match=0.0780 (385.4s total)\n",
      "{'eval_loss': 5.548454284667969, 'eval_bleu': 0.12176561155335353, 'eval_exact_match': 0.078, 'eval_avg_pred_length': 19.944779116465863, 'eval_avg_label_length': 17.95895895895896, 'eval_empty_predictions': 0.004, 'eval_runtime': 513.7511, 'eval_samples_per_second': 1.946, 'eval_steps_per_second': 0.243, 'epoch': 5.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_large_hf_wordpiece_hf\n",
      "📈 Overall BLEU Score: 0.1218\n",
      "🎯 Overall Exact Match: 0.0780\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): هل ، ما ##ذا تت ##حدث ؟\n",
      "  zh (Chinese): - 你 ##不 ##知道 你 ##知道\n",
      "  hi (Hindi): आप , आप क्या कर रहे हैं ?\n",
      "\n",
      "====================================================================================================\n",
      "🚀 Training Model 6/9\n",
      "🔧 Custom Tokenizer: large_sp_unigram_hf\n",
      "📦 Compatible Base Model: facebook/bart-large\n",
      "🌍 Target Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'] (ALL SIMULTANEOUSLY)\n",
      "🔧 Loading custom tokenizer: vocab_final/vocab_finallarge/sp_unigram_hf\n",
      "✅ Custom tokenizer loaded successfully!\n",
      "📊 Custom vocab size: 50002\n",
      "🔑 Special tokens - EOS: 2, PAD: 50000\n",
      "🤖 Loading compatible base model: facebook/bart-large\n",
      "🔄 Resizing model embeddings to match custom tokenizer...\n",
      "   Vocab size: 50265 → 50002\n",
      "✅ Model configured with custom tokenizer!\n",
      "📊 Using complete multilingual dataset:\n",
      "   • Train samples: 34376\n",
      "   • Test samples: 3820\n",
      "   • Languages: 9 (['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr'])\n",
      "⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing with large_sp_unigram_hf: 100%|██████████| 34376/34376 [00:04<00:00, 7965.07 examples/s]\n",
      "Preprocessing with large_sp_unigram_hf: 100%|██████████| 3820/3820 [00:00<00:00, 7660.81 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:29<00:00, 1032.83 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:03<00:00, 1024.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed multilingual dataset:\n",
      "   • Train samples: 30566\n",
      "   • Eval samples: 3386\n",
      "   • Reduced eval samples to: 1000 (to avoid memory issues)\n",
      "🏋️  Starting multilingual training with compatible model-tokenizer pair...\n",
      "{'loss': 11.5044, 'grad_norm': 3.63551664352417, 'learning_rate': 1.7573221757322174e-05, 'epoch': 0.10468463752944256}\n",
      "{'loss': 8.9993, 'grad_norm': 3.296987295150757, 'learning_rate': 3.849372384937239e-05, 'epoch': 0.2093692750588851}\n",
      "{'loss': 8.2906, 'grad_norm': 2.983065605163574, 'learning_rate': 5.94142259414226e-05, 'epoch': 0.31405391258832765}\n",
      "{'loss': 7.8263, 'grad_norm': 3.1163668632507324, 'learning_rate': 8.03347280334728e-05, 'epoch': 0.4187385501177702}\n",
      "{'loss': 7.4074, 'grad_norm': 2.5006611347198486, 'learning_rate': 9.9860529986053e-05, 'epoch': 0.5234231876472127}\n",
      "{'loss': 7.3326, 'grad_norm': 3.549058437347412, 'learning_rate': 9.753602975360298e-05, 'epoch': 0.6281078251766553}\n",
      "{'loss': 7.029, 'grad_norm': 2.484065532684326, 'learning_rate': 9.521152952115295e-05, 'epoch': 0.7327924627060979}\n",
      "{'loss': 6.8779, 'grad_norm': 2.5776960849761963, 'learning_rate': 9.288702928870293e-05, 'epoch': 0.8374771002355405}\n",
      "{'loss': 6.6682, 'grad_norm': 2.538095474243164, 'learning_rate': 9.056252905625291e-05, 'epoch': 0.942161737764983}\n",
      "{'loss': 6.3831, 'grad_norm': 2.3342795372009277, 'learning_rate': 8.823802882380289e-05, 'epoch': 1.0460612405129548}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (77.8s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (154.4s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (226.3s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (301.8s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0170, Exact Match=0.0080 (301.8s total)\n",
      "{'eval_loss': 6.341037750244141, 'eval_bleu': 0.017017811032019093, 'eval_exact_match': 0.008, 'eval_avg_pred_length': 6.400801603206413, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.002, 'eval_runtime': 394.3164, 'eval_samples_per_second': 2.536, 'eval_steps_per_second': 0.317, 'epoch': 1.0460612405129548}\n",
      "{'loss': 6.2992, 'grad_norm': 2.4720823764801025, 'learning_rate': 8.591352859135286e-05, 'epoch': 1.1507458780423974}\n",
      "{'loss': 6.2619, 'grad_norm': 2.3305470943450928, 'learning_rate': 8.358902835890285e-05, 'epoch': 1.25543051557184}\n",
      "{'loss': 6.1944, 'grad_norm': 2.6558926105499268, 'learning_rate': 8.126452812645282e-05, 'epoch': 1.3601151531012823}\n",
      "{'loss': 6.0781, 'grad_norm': 2.4655449390411377, 'learning_rate': 7.89400278940028e-05, 'epoch': 1.464799790630725}\n",
      "{'loss': 5.9742, 'grad_norm': 2.4382400512695312, 'learning_rate': 7.661552766155277e-05, 'epoch': 1.5694844281601674}\n",
      "{'loss': 5.9983, 'grad_norm': 2.389491081237793, 'learning_rate': 7.429102742910274e-05, 'epoch': 1.6741690656896102}\n",
      "{'loss': 5.9513, 'grad_norm': 2.5110838413238525, 'learning_rate': 7.196652719665272e-05, 'epoch': 1.7788537032190526}\n",
      "{'loss': 5.9299, 'grad_norm': 2.89821720123291, 'learning_rate': 6.96420269642027e-05, 'epoch': 1.8835383407484951}\n",
      "{'loss': 5.8949, 'grad_norm': 2.6355788707733154, 'learning_rate': 6.731752673175268e-05, 'epoch': 1.9882229782779377}\n",
      "{'loss': 5.6499, 'grad_norm': 2.595773935317993, 'learning_rate': 6.499302649930265e-05, 'epoch': 2.0921224810259096}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (86.6s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (167.8s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (254.8s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (330.9s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0661, Exact Match=0.0520 (330.9s total)\n",
      "{'eval_loss': 5.774507999420166, 'eval_bleu': 0.06607140810272223, 'eval_exact_match': 0.052, 'eval_avg_pred_length': 7.524, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 434.5566, 'eval_samples_per_second': 2.301, 'eval_steps_per_second': 0.288, 'epoch': 2.0921224810259096}\n",
      "{'loss': 5.7224, 'grad_norm': 2.582242012023926, 'learning_rate': 6.266852626685264e-05, 'epoch': 2.196807118555352}\n",
      "{'loss': 5.6399, 'grad_norm': 2.6451919078826904, 'learning_rate': 6.0344026034402604e-05, 'epoch': 2.3014917560847947}\n",
      "{'loss': 5.6022, 'grad_norm': 2.428671360015869, 'learning_rate': 5.801952580195258e-05, 'epoch': 2.406176393614237}\n",
      "{'loss': 5.5929, 'grad_norm': 2.5760645866394043, 'learning_rate': 5.569502556950256e-05, 'epoch': 2.51086103114368}\n",
      "{'loss': 5.5583, 'grad_norm': 2.584742307662964, 'learning_rate': 5.337052533705254e-05, 'epoch': 2.615545668673122}\n",
      "{'loss': 5.5748, 'grad_norm': 3.5967209339141846, 'learning_rate': 5.104602510460251e-05, 'epoch': 2.7202303062025646}\n",
      "{'loss': 5.5786, 'grad_norm': 2.574061632156372, 'learning_rate': 4.872152487215249e-05, 'epoch': 2.8249149437320074}\n",
      "{'loss': 5.5609, 'grad_norm': 2.5405075550079346, 'learning_rate': 4.6397024639702466e-05, 'epoch': 2.92959958126145}\n",
      "{'loss': 5.3785, 'grad_norm': 2.6438241004943848, 'learning_rate': 4.407252440725244e-05, 'epoch': 3.0334990840094216}\n",
      "{'loss': 5.3576, 'grad_norm': 3.0788254737854004, 'learning_rate': 4.174802417480242e-05, 'epoch': 3.1381837215388644}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (80.1s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (169.2s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (252.7s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (330.5s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.0841, Exact Match=0.0720 (330.6s total)\n",
      "{'eval_loss': 5.574815273284912, 'eval_bleu': 0.08410927232295322, 'eval_exact_match': 0.072, 'eval_avg_pred_length': 6.757, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 432.5068, 'eval_samples_per_second': 2.312, 'eval_steps_per_second': 0.289, 'epoch': 3.1381837215388644}\n",
      "{'loss': 5.3525, 'grad_norm': 2.632021427154541, 'learning_rate': 3.94235239423524e-05, 'epoch': 3.2428683590683067}\n",
      "{'loss': 5.3082, 'grad_norm': 2.8431923389434814, 'learning_rate': 3.7099023709902373e-05, 'epoch': 3.3475529965977495}\n",
      "{'loss': 5.3131, 'grad_norm': 2.759091854095459, 'learning_rate': 3.477452347745235e-05, 'epoch': 3.452237634127192}\n",
      "{'loss': 5.3085, 'grad_norm': 2.7943809032440186, 'learning_rate': 3.245002324500233e-05, 'epoch': 3.556922271656634}\n",
      "{'loss': 5.3433, 'grad_norm': 2.918717384338379, 'learning_rate': 3.0125523012552304e-05, 'epoch': 3.661606909186077}\n",
      "{'loss': 5.3083, 'grad_norm': 2.3798043727874756, 'learning_rate': 2.780102278010228e-05, 'epoch': 3.7662915467155194}\n",
      "{'loss': 5.3428, 'grad_norm': 2.550670862197876, 'learning_rate': 2.5476522547652255e-05, 'epoch': 3.870976184244962}\n",
      "{'loss': 5.2512, 'grad_norm': 2.56591534614563, 'learning_rate': 2.3152022315202232e-05, 'epoch': 3.9756608217744045}\n",
      "{'loss': 5.1871, 'grad_norm': 2.553853988647461, 'learning_rate': 2.082752208275221e-05, 'epoch': 4.079560324522376}\n",
      "{'loss': 5.1617, 'grad_norm': 2.6176366806030273, 'learning_rate': 1.8503021850302186e-05, 'epoch': 4.184244962051819}\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (93.6s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (182.9s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (267.2s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (343.6s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1009, Exact Match=0.0920 (343.7s total)\n",
      "{'eval_loss': 5.475937843322754, 'eval_bleu': 0.1009169053587867, 'eval_exact_match': 0.092, 'eval_avg_pred_length': 7.004, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 450.5558, 'eval_samples_per_second': 2.219, 'eval_steps_per_second': 0.277, 'epoch': 4.184244962051819}\n",
      "{'loss': 5.1452, 'grad_norm': 3.075958728790283, 'learning_rate': 1.6178521617852163e-05, 'epoch': 4.2889295995812615}\n",
      "{'loss': 5.1441, 'grad_norm': 2.75349760055542, 'learning_rate': 1.3854021385402138e-05, 'epoch': 4.393614237110704}\n",
      "{'loss': 5.1307, 'grad_norm': 2.7164366245269775, 'learning_rate': 1.1529521152952117e-05, 'epoch': 4.498298874640146}\n",
      "{'loss': 5.12, 'grad_norm': 2.6111021041870117, 'learning_rate': 9.205020920502094e-06, 'epoch': 4.6029835121695895}\n",
      "{'loss': 5.0749, 'grad_norm': 2.8526298999786377, 'learning_rate': 6.88052068805207e-06, 'epoch': 4.707668149699032}\n",
      "{'loss': 5.105, 'grad_norm': 2.7222695350646973, 'learning_rate': 4.556020455602046e-06, 'epoch': 4.812352787228474}\n",
      "{'loss': 5.1128, 'grad_norm': 2.62093448638916, 'learning_rate': 2.2315202231520224e-06, 'epoch': 4.9170374247579165}\n",
      "{'train_runtime': 3482.7931, 'train_samples_per_second': 43.881, 'train_steps_per_second': 0.686, 'train_loss': 6.002481672853605, 'epoch': 5.0}\n",
      "📊 Final multilingual evaluation...\n",
      "🔄 Starting evaluation of 1000 samples...\n",
      "📊 Evaluation progress: 250/1000 samples (90.9s elapsed)\n",
      "📊 Evaluation progress: 500/1000 samples (176.2s elapsed)\n",
      "📊 Evaluation progress: 750/1000 samples (269.1s elapsed)\n",
      "📊 Evaluation progress: 1000/1000 samples (351.5s elapsed)\n",
      "🧮 Computing BLEU scores...\n",
      "✅ Evaluation complete: BLEU=0.1039, Exact Match=0.0960 (351.5s total)\n",
      "{'eval_loss': 5.428654193878174, 'eval_bleu': 0.10388467436107324, 'eval_exact_match': 0.096, 'eval_avg_pred_length': 7.129, 'eval_avg_label_length': 7.369, 'eval_empty_predictions': 0.0, 'eval_runtime': 459.163, 'eval_samples_per_second': 2.178, 'eval_steps_per_second': 0.272, 'epoch': 5.0}\n",
      "💾 Saving multilingual model and custom tokenizer...\n",
      "✅ Completed: multilingual_large_sp_unigram_hf\n",
      "📈 Overall BLEU Score: 0.1039\n",
      "🎯 Overall Exact Match: 0.0960\n",
      "\n",
      "🧪 Quick multilingual translation tests:\n",
      "  ar (Arabic): اً، أنت تفعلي؟\n",
      "  zh (Chinese): 是的 你知道?\n",
      "  hi (Hindi): ा, तुम्हें क्या कर रहे हो?\n",
      "\n",
      "🎉 Multilingual training with compatible model-tokenizer pairs completed!\n",
      "📋 Results saved to: ./MT_models_multilingual_custom_tokenizers\\multilingual_custom_tokenizer_results.csv\n",
      "🔢 Total models trained: 9 (3 sizes × 3 types)\n",
      "🌍 Each model handles all 9 languages simultaneously\n",
      "\n",
      "📊 Expected improvements:\n",
      "• BPE tokenizers → BART models (proper compatibility)\n",
      "• WordPiece tokenizers → mT5 models (better multilingual support)\n",
      "• Unigram tokenizers → mT5 models (native compatibility)\n",
      "• Increased sequence length (256 vs 128)\n",
      "• Better hyperparameters and training setup\n"
     ]
    }
   ],
   "source": [
    "##working ... but slow (running final code)\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    logging as hf_logging,\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    BertTokenizerFast,\n",
    "    GPT2TokenizerFast,\n",
    "    BartForConditionalGeneration\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# FIXED: Model-Tokenizer Compatibility Mapping\n",
    "MODEL_TOKENIZER_CONFIGS = {\n",
    "    \"hf_bpe_hf\": {\n",
    "        \"base_model\": \"facebook/bart-large\",  # BART uses BPE\n",
    "        \"model_class\": BartForConditionalGeneration,\n",
    "        \"description\": \"BART with BPE tokenization\"\n",
    "    },\n",
    "    \"hf_wordpiece_hf\": {\n",
    "        \"base_model\": \"facebook/bart-large\",  # BART uses BPE\n",
    "        \"model_class\": BartForConditionalGeneration,\n",
    "        \"description\": \"BART with BPE tokenization\"\n",
    "    },\n",
    "    \"sp_unigram_hf\": {\n",
    "        \"base_model\": \"facebook/bart-large\",  # BART uses BPE\n",
    "        \"model_class\": BartForConditionalGeneration,\n",
    "        \"description\": \"BART with BPE tokenization\"\n",
    "    }\n",
    "    # \"hf_wordpiece_hf\": {\n",
    "    #     \"base_model\": \"google/mt5-base\",  # mT5 can work with WordPiece\n",
    "    #     \"model_class\": T5ForConditionalGeneration,\n",
    "    #     \"description\": \"mT5 with WordPiece tokenization\"\n",
    "    # },\n",
    "    # \"sp_unigram_hf\": {\n",
    "    #     \"base_model\": \"google/mt5-base\",  # mT5 uses SentencePiece Unigram\n",
    "    #     \"model_class\": T5ForConditionalGeneration,\n",
    "    #     \"description\": \"mT5 with SentencePiece Unigram\"\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Custom tokenizer settings\n",
    "# tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
    "tokenizer_sizes = [ \"medium\", \"large\"]\n",
    "tokenizer_types = [\"hf_bpe_hf\", \"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
    "# tokenizer_types = [\"hf_wordpiece_hf\", \"sp_unigram_hf\"]\n",
    "\n",
    "# Languages for evaluation\n",
    "languages = {\n",
    "    \"yo\": \"Yoruba\",\n",
    "    \"ar\": \"Arabic\", \n",
    "    \"zh\": \"Chinese\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"hi\": \"Hindi\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"swa\": \"Swahili\",\n",
    "    \"bn\": \"Bengali\",\n",
    "    \"tr\": \"Turkish\"\n",
    "}\n",
    "SRC_LANG = \"en\"\n",
    "\n",
    "# Load complete multilingual dataset\n",
    "dataset_path = \"balanced_mt_dataset\"\n",
    "print(\"📦 Loading complete multilingual dataset...\")\n",
    "full_dataset = load_from_disk(dataset_path)\n",
    "print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "print(f\"🌍 All languages included: {list(languages.keys())}\")\n",
    "\n",
    "# Enhanced BLEU computation with progress tracking\n",
    "def compute_multilingual_bleu(eval_pred):\n",
    "    \"\"\"Multilingual BLEU computation with progress tracking\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if len(predictions.shape) == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    total_samples = len(predictions)\n",
    "    print(f\"🔄 Starting evaluation of {total_samples} samples...\")\n",
    "    \n",
    "    # Process in chunks with progress updates\n",
    "    chunk_size = 50\n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        end_idx = min(i + chunk_size, total_samples)\n",
    "        chunk_preds = predictions[i:end_idx]\n",
    "        chunk_labels = labels[i:end_idx]\n",
    "        \n",
    "        try:\n",
    "            for pred, label in zip(chunk_preds, chunk_labels):\n",
    "                # CRITICAL FIX: Handle negative token IDs properly\n",
    "                # Filter out negative IDs and pad tokens before decoding\n",
    "                pred_clean = [token for token in pred if token >= 0 and token < len(tokenizer)]\n",
    "                label_clean = [token for token in label if token != -100 and token >= 0 and token < len(tokenizer)]\n",
    "                \n",
    "                try:\n",
    "                    decoded_pred = tokenizer.decode(pred_clean, skip_special_tokens=True).strip() if pred_clean else \"\"\n",
    "                    decoded_label = tokenizer.decode(label_clean, skip_special_tokens=True).strip() if label_clean else \"\"\n",
    "                except Exception as e:\n",
    "                    # Fallback for any decode errors\n",
    "                    decoded_pred = \"\"\n",
    "                    decoded_label = \"\"\n",
    "                \n",
    "                decoded_preds.append(decoded_pred)\n",
    "                decoded_labels.append(decoded_label)\n",
    "            \n",
    "            # Progress update every chunk\n",
    "            if (i + chunk_size) % (chunk_size * 5) == 0 or end_idx == total_samples:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"📊 Evaluation progress: {end_idx}/{total_samples} samples ({elapsed:.1f}s elapsed)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Batch decode failed for chunk {i}-{end_idx}: {str(e)}\")\n",
    "            # Add empty strings for failed batch\n",
    "            for _ in range(end_idx - i):\n",
    "                decoded_preds.append(\"\")\n",
    "                decoded_labels.append(\"\")\n",
    "    \n",
    "    # Compute BLEU with smoothing\n",
    "    print(\"🧮 Computing BLEU scores...\")\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for idx, (pred, label) in enumerate(zip(decoded_preds, decoded_labels)):\n",
    "        if idx % 1000 == 0 and idx > 0:\n",
    "            print(f\"   BLEU calculation: {idx}/{len(decoded_preds)} samples\")\n",
    "            \n",
    "        if not pred.strip() or not label.strip():\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "        \n",
    "        # Check exact match\n",
    "        if pred.lower().strip() == label.lower().strip():\n",
    "            exact_matches += 1\n",
    "        \n",
    "        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            bleu = sentence_bleu(\n",
    "                [label_tokens], \n",
    "                pred_tokens,\n",
    "                smoothing_function=smoothing,\n",
    "                weights=(0.25, 0.25, 0.25, 0.25)\n",
    "            )\n",
    "            bleu_scores.append(bleu)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match_ratio = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"✅ Evaluation complete: BLEU={avg_bleu:.4f}, Exact Match={exact_match_ratio:.4f} ({total_time:.1f}s total)\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"exact_match\": exact_match_ratio,\n",
    "        \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "        \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "        \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "    }\n",
    "\n",
    "def setup_custom_tokenizer_for_model(tokenizer_path, model_type):\n",
    "    \"\"\"Setup custom tokenizer with proper special tokens for specific model\"\"\"\n",
    "    # Load custom tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "    \n",
    "    # Model-specific special token configuration\n",
    "    if model_type == \"bart\":\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "    elif model_type == \"t5\":\n",
    "        special_tokens = {\n",
    "            'pad_token': '<pad>',\n",
    "            'eos_token': '</s>',\n",
    "            'unk_token': '<unk>',\n",
    "            'bos_token': '<pad>',  # T5 doesn't use BOS\n",
    "            'sep_token': '</s>',\n",
    "            'mask_token': '<extra_id_0>'\n",
    "        }\n",
    "    else:  # Default fallback\n",
    "        special_tokens = {\n",
    "            'bos_token': '<s>',\n",
    "            'eos_token': '</s>',\n",
    "            'sep_token': '</s>',\n",
    "            'pad_token': '<pad>',\n",
    "            'unk_token': '<unk>',\n",
    "            'mask_token': '<mask>'\n",
    "        }\n",
    "    \n",
    "    # Add missing special tokens\n",
    "    tokens_to_add = {}\n",
    "    for token_name, token_value in special_tokens.items():\n",
    "        if getattr(tokenizer, token_name, None) is None:\n",
    "            tokens_to_add[token_name] = token_value\n",
    "    \n",
    "    if tokens_to_add:\n",
    "        tokenizer.add_special_tokens(tokens_to_add)\n",
    "    \n",
    "    # Ensure we have the required tokens\n",
    "    assert tokenizer.eos_token is not None, \"EOS token is required\"\n",
    "    assert tokenizer.pad_token is not None, \"PAD token is required\"\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def configure_model_for_custom_tokenizer(model, tokenizer, model_type):\n",
    "    \"\"\"Configure model for custom tokenizer based on model type\"\"\"\n",
    "    # Resize embeddings\n",
    "    print(\"🔄 Resizing model embeddings to match custom tokenizer...\")\n",
    "    old_vocab_size = model.config.vocab_size\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   Vocab size: {old_vocab_size} → {len(tokenizer)}\")\n",
    "    \n",
    "    # Model-specific configuration\n",
    "    if model_type == \"bart\":\n",
    "        model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.sep_token_id = tokenizer.eos_token_id\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    elif model_type == \"t5\":\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.decoder_start_token_id = tokenizer.pad_token_id  # T5 uses pad_token_id as decoder start\n",
    "        model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Update generation config if it exists\n",
    "    if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "        if model_type == \"bart\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "            model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "        elif model_type == \"t5\":\n",
    "            model.generation_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup logging\n",
    "log_dir = \"./MT_models_multilingual_custom_tokenizers\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "results_log_file = os.path.join(log_dir, \"multilingual_custom_tokenizer_results.csv\")\n",
    "\n",
    "if not os.path.exists(results_log_file):\n",
    "    with open(results_log_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Model_ID\", \"Base_Model\", \"Tokenizer_Size\", \"Tokenizer_Type\", \n",
    "                        \"Total_Languages\", \"Total_Train_Samples\", \"Total_Test_Samples\", \n",
    "                        \"Custom_Vocab_Size\", \"Overall_BLEU\", \"Overall_Exact_Match\", \n",
    "                        \"Avg_Pred_Length\", \"Avg_Label_Length\", \"Empty_Predictions\", \n",
    "                        \"Training_Status\", \"Notes\"])\n",
    "\n",
    "print(\"🚀 Starting multilingual training with COMPATIBLE model-tokenizer pairs...\")\n",
    "print(f\"📊 Training approach: ONE model per tokenizer handling ALL {len(languages)} languages\")\n",
    "\n",
    "# Main training loop: 3 sizes × 3 types = 9 models total\n",
    "for size in tokenizer_sizes:\n",
    "    for tok_type in tokenizer_types:\n",
    "        # Get compatible model configuration\n",
    "        model_config = MODEL_TOKENIZER_CONFIGS[tok_type]\n",
    "        \n",
    "        # Path to custom tokenizer\n",
    "        tokenizer_path = f\"vocab_final/vocab_final{size}/{tok_type}\"\n",
    "        model_id = f\"multilingual_{size}_{tok_type}\"\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"🚀 Training Model {tokenizer_sizes.index(size)*3 + tokenizer_types.index(tok_type) + 1}/9\")\n",
    "        print(f\"🔧 Custom Tokenizer: {size}_{tok_type}\")\n",
    "        print(f\"📦 Compatible Base Model: {model_config['base_model']}\")\n",
    "        print(f\"🌍 Target Languages: {list(languages.keys())} (ALL SIMULTANEOUSLY)\")\n",
    "        \n",
    "        # Initialize variables\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        trainer = None\n",
    "        \n",
    "        try:\n",
    "            # Load and setup custom tokenizer\n",
    "            print(f\"🔧 Loading custom tokenizer: {tokenizer_path}\")\n",
    "            model_type = \"bart\" if \"bart\" in model_config['base_model'] else \"t5\"\n",
    "            tokenizer = setup_custom_tokenizer_for_model(tokenizer_path, model_type)\n",
    "            \n",
    "            print(f\"✅ Custom tokenizer loaded successfully!\")\n",
    "            print(f\"📊 Custom vocab size: {len(tokenizer)}\")\n",
    "            print(f\"🔑 Special tokens - EOS: {tokenizer.eos_token_id}, PAD: {tokenizer.pad_token_id}\")\n",
    "            \n",
    "            # Load compatible base model\n",
    "            print(f\"🤖 Loading compatible base model: {model_config['base_model']}\")\n",
    "            model = model_config['model_class'].from_pretrained(model_config['base_model'])\n",
    "            \n",
    "            # Configure model for custom tokenizer\n",
    "            model = configure_model_for_custom_tokenizer(model, tokenizer, model_type)\n",
    "            \n",
    "            print(f\"✅ Model configured with custom tokenizer!\")\n",
    "            \n",
    "            # Use COMPLETE multilingual dataset (all languages together)\n",
    "            print(f\"📊 Using complete multilingual dataset:\")\n",
    "            print(f\"   • Train samples: {len(full_dataset['train'])}\")\n",
    "            print(f\"   • Test samples: {len(full_dataset['test'])}\")\n",
    "            print(f\"   • Languages: {len(languages)} ({list(languages.keys())})\")\n",
    "            \n",
    "            # IMPROVED: Preprocessing function for multilingual data\n",
    "            def preprocess_multilingual_improved(examples):\n",
    "                \"\"\"Improved preprocessing for multilingual data with model-specific formatting\"\"\"\n",
    "                sources = []\n",
    "                targets = []\n",
    "                \n",
    "                # Handle both single examples and batches\n",
    "                if not isinstance(examples[\"translation\"], list):\n",
    "                    examples = {\n",
    "                        \"translation\": [examples[\"translation\"]], \n",
    "                        \"language\": [examples[\"language\"]]\n",
    "                    }\n",
    "                \n",
    "                for translation, lang in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "                    if isinstance(translation, dict) and lang in languages:\n",
    "                        source = translation.get(SRC_LANG, \"\")\n",
    "                        target = translation.get(lang, \"\")\n",
    "                        \n",
    "                        if source.strip() and target.strip():  # Ensure non-empty\n",
    "                            # Model-specific formatting\n",
    "                            if model_type == \"t5\":\n",
    "                                # T5 style: \"translate English to German: Hello\"\n",
    "                                source_formatted = f\"translate English to {languages[lang]}: {source}\"\n",
    "                            else:  # BART style\n",
    "                                # BART style with language token\n",
    "                                source_formatted = f\"{source} </s> {lang}_XX\"  # Add language code\n",
    "                            \n",
    "                            sources.append(source_formatted)\n",
    "                            targets.append(target)\n",
    "                \n",
    "                if not sources or not targets:\n",
    "                    return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "                \n",
    "                # Tokenize with custom tokenizer\n",
    "                max_length = 256  # INCREASED for better performance\n",
    "                \n",
    "                # Input tokenization - REMOVE token_type_ids\n",
    "                model_inputs = tokenizer(\n",
    "                    sources,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=None,\n",
    "                    return_token_type_ids=False  # CRITICAL FIX\n",
    "                )\n",
    "                # Ensure token_type_ids is removed\n",
    "                model_inputs.pop(\"token_type_ids\", None)\n",
    "                \n",
    "                # Target tokenization  \n",
    "                with tokenizer.as_target_tokenizer():\n",
    "                    labels = tokenizer(\n",
    "                        targets,\n",
    "                        max_length=max_length,\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\",\n",
    "                        return_tensors=None,\n",
    "                        return_token_type_ids=False  # CRITICAL FIX\n",
    "                    )\n",
    "                # Ensure token_type_ids is removed\n",
    "                labels.pop(\"token_type_ids\", None)\n",
    "                \n",
    "                # IMPROVED: Label processing with proper EOS handling\n",
    "                processed_labels = []\n",
    "                for label_seq in labels[\"input_ids\"]:\n",
    "                    # Find actual end of sequence (before padding)\n",
    "                    try:\n",
    "                        pad_start = label_seq.index(tokenizer.pad_token_id)\n",
    "                        actual_tokens = label_seq[:pad_start]\n",
    "                    except ValueError:\n",
    "                        actual_tokens = label_seq\n",
    "                    \n",
    "                    # Ensure EOS token at end\n",
    "                    if actual_tokens and actual_tokens[-1] != tokenizer.eos_token_id:\n",
    "                        actual_tokens.append(tokenizer.eos_token_id)\n",
    "                    elif not actual_tokens:\n",
    "                        actual_tokens = [tokenizer.eos_token_id]\n",
    "                    \n",
    "                    # Create final sequence with -100 for padding\n",
    "                    final_labels = actual_tokens + [-100] * (max_length - len(actual_tokens))\n",
    "                    final_labels = final_labels[:max_length]  # Ensure correct length\n",
    "                    \n",
    "                    processed_labels.append(final_labels)\n",
    "                \n",
    "                model_inputs[\"labels\"] = processed_labels\n",
    "                return model_inputs\n",
    "            \n",
    "            # Preprocess complete multilingual dataset\n",
    "            print(\"⚙️  Preprocessing complete multilingual dataset with custom tokenizer...\")\n",
    "            processed_dataset = full_dataset.map(\n",
    "                preprocess_multilingual_improved,\n",
    "                batched=True,\n",
    "                remove_columns=full_dataset[\"train\"].column_names,\n",
    "                desc=f\"Preprocessing with {size}_{tok_type}\",\n",
    "                batch_size=50,  # Smaller batch for stability\n",
    "                num_proc=1  # Single process to avoid issues\n",
    "            )\n",
    "            \n",
    "            train_dataset = processed_dataset[\"train\"]\n",
    "            eval_dataset = processed_dataset[\"test\"]\n",
    "            \n",
    "            # Filter out empty examples\n",
    "            def filter_valid_examples(example):\n",
    "                return (\n",
    "                    len(example[\"input_ids\"]) > 0 and \n",
    "                    len(example[\"labels\"]) > 0 and\n",
    "                    any(label != -100 for label in example[\"labels\"]) and\n",
    "                    sum(1 for token in example[\"input_ids\"] if token != tokenizer.pad_token_id) > 0\n",
    "                )\n",
    "            \n",
    "            train_dataset = train_dataset.filter(filter_valid_examples)\n",
    "            eval_dataset = eval_dataset.filter(filter_valid_examples)\n",
    "            \n",
    "            # Use smaller eval dataset to avoid memory issues\n",
    "            print(f\"✅ Preprocessed multilingual dataset:\")\n",
    "            print(f\"   • Train samples: {len(train_dataset)}\")\n",
    "            print(f\"   • Eval samples: {len(eval_dataset)}\")\n",
    "            \n",
    "            # REDUCE eval dataset size to avoid hanging\n",
    "            if len(eval_dataset) > 1000:\n",
    "                eval_dataset = eval_dataset.select(range(1000))\n",
    "                print(f\"   • Reduced eval samples to: {len(eval_dataset)} (to avoid memory issues)\")\n",
    "            \n",
    "            if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "                print(\"❌ No valid samples after preprocessing\")\n",
    "                continue\n",
    "            \n",
    "            # Setup training directory\n",
    "            output_dir = f\"./MT_models_multilingual_custom_tokenizers/{model_id}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # IMPROVED: Training arguments with simpler evaluation\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                num_train_epochs=5,  # More epochs for custom tokenizers\n",
    "                per_device_train_batch_size=8,  # Smaller batch size\n",
    "                per_device_eval_batch_size=8,  # REDUCED eval batch size\n",
    "                gradient_accumulation_steps=8,  # Higher accumulation\n",
    "                learning_rate=1e-4,  # Lower learning rate for stability\n",
    "                weight_decay=0.01,\n",
    "                warmup_ratio=0.1,  # Warmup as ratio\n",
    "                eval_strategy=\"steps\",  # Change to steps-based evaluation\n",
    "                eval_steps=500,  # Evaluate every 500 steps instead of epoch end\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=2,\n",
    "                logging_steps=50,\n",
    "                report_to=\"none\",\n",
    "                predict_with_generate=True,\n",
    "                generation_max_length=128,  # REDUCED generation length\n",
    "                generation_num_beams=2,  # REDUCED beams\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                load_best_model_at_end=False,  # DISABLED to avoid issues\n",
    "                dataloader_num_workers=0,\n",
    "                remove_unused_columns=False,  # CRITICAL: Keep all columns\n",
    "                ignore_data_skip=True,\n",
    "                label_smoothing_factor=0.1,  # Label smoothing\n",
    "                max_grad_norm=1.0,  # Gradient clipping\n",
    "                dataloader_pin_memory=False,  # Disable pin memory\n",
    "                skip_memory_metrics=True,  # Skip memory tracking\n",
    "                \n",
    "                \n",
    "            )\n",
    "            \n",
    "            # Data collator with explicit token_type_ids handling\n",
    "            data_collator = DataCollatorForSeq2Seq(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "            \n",
    "            # CRITICAL FIX: Custom data collator that removes token_type_ids\n",
    "            class CustomDataCollator(DataCollatorForSeq2Seq):\n",
    "                def __call__(self, features):\n",
    "                    batch = super().__call__(features)\n",
    "                    # Remove token_type_ids if present\n",
    "                    batch.pop(\"token_type_ids\", None)\n",
    "                    return batch\n",
    "            \n",
    "            data_collator = CustomDataCollator(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "            \n",
    "            # Trainer\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_multilingual_bleu\n",
    "            )\n",
    "            \n",
    "            # Train multilingual model\n",
    "            print(\"🏋️  Starting multilingual training with compatible model-tokenizer pair...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            print(\"📊 Final multilingual evaluation...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Save model and custom tokenizer\n",
    "            print(\"💾 Saving multilingual model and custom tokenizer...\")\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "            # Log results\n",
    "            overall_bleu = eval_results.get(\"eval_bleu\", 0.0)\n",
    "            overall_exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "            avg_pred_len = eval_results.get(\"eval_avg_pred_length\", 0.0)\n",
    "            avg_label_len = eval_results.get(\"eval_avg_label_length\", 0.0)\n",
    "            empty_preds = eval_results.get(\"eval_empty_predictions\", 0.0)\n",
    "            \n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config[\"base_model\"], size, tok_type,\n",
    "                    len(languages), len(train_dataset), len(eval_dataset), len(tokenizer),\n",
    "                    round(overall_bleu, 4), round(overall_exact_match, 4), \n",
    "                    round(avg_pred_len, 2), round(avg_label_len, 2),\n",
    "                    round(empty_preds, 4), \"SUCCESS\", \n",
    "                    f\"Compatible {model_config['description']} with {size}_{tok_type}\"\n",
    "                ])\n",
    "            \n",
    "            print(f\"✅ Completed: {model_id}\")\n",
    "            print(f\"📈 Overall BLEU Score: {overall_bleu:.4f}\")\n",
    "            print(f\"🎯 Overall Exact Match: {overall_exact_match:.4f}\")\n",
    "            \n",
    "            # Quick translation tests for different languages\n",
    "            print(f\"\\n🧪 Quick multilingual translation tests:\")\n",
    "            test_input = \"Hello, how are you today?\"\n",
    "            \n",
    "            for test_lang in [\"ar\", \"zh\", \"hi\"]:  # Test 3 languages\n",
    "                if model_type == \"t5\":\n",
    "                    formatted_input = f\"translate English to {languages[test_lang]}: {test_input}\"\n",
    "                else:  # BART\n",
    "                    formatted_input = f\"{test_input} </s> {test_lang}_XX\"\n",
    "                    \n",
    "                inputs = tokenizer(formatted_input, return_tensors=\"pt\", padding=True, max_length=256, truncation=True)\n",
    "                \n",
    "                # Move to device\n",
    "                device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=128,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        forced_eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"  {test_lang} ({languages[test_lang]}): {translation}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to train {model_id}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            with open(results_log_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    model_id, model_config.get(\"base_model\", \"\"), size, tok_type,\n",
    "                    len(languages), 0, 0, 0, 0, 0, 0, 0, 0, \"TRAINING_FAILED\", str(e)[:100]\n",
    "                ])\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if model is not None:\n",
    "                del model\n",
    "            if trainer is not None:\n",
    "                del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "print(\"\\n🎉 Multilingual training with compatible model-tokenizer pairs completed!\")\n",
    "print(f\"📋 Results saved to: {results_log_file}\")\n",
    "print(f\"🔢 Total models trained: 9 (3 sizes × 3 types)\")\n",
    "print(f\"🌍 Each model handles all {len(languages)} languages simultaneously\")\n",
    "print(\"\\n📊 Expected improvements:\")\n",
    "print(\"• BPE tokenizers → BART models (proper compatibility)\")\n",
    "print(\"• WordPiece tokenizers → mT5 models (better multilingual support)\")\n",
    "print(\"• Unigram tokenizers → mT5 models (native compatibility)\")\n",
    "print(\"• Increased sequence length (256 vs 128)\")\n",
    "print(\"• Better hyperparameters and training setup\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c0c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Loading complete multilingual dataset...\n",
      "✅ Dataset loaded: 34376 train, 3820 test\n",
      "🌍 Languages: ['yo', 'ar', 'zh', 'ru', 'hi', 'ja', 'swa', 'bn', 'tr']\n",
      "🚀 Starting MT5-ONLY multilingual training...\n",
      "📊 Will train 3 sizes × 2 types = 6 MT5 models\n",
      "\n",
      "================================================================================\n",
      "🚀 Training MT5 Model 1/6\n",
      "🔧 Custom Tokenizer: small_hf_wordpiece_hf\n",
      "🌍 Target Languages: ALL 9 languages\n",
      "🔧 Loading mT5-compatible tokenizer: vocab_final/vocab_finalsmall/hf_wordpiece_hf\n",
      "   ✅ Added 2 special tokens\n",
      "   📊 Vocab size: 23634\n",
      "   🔑 EOS: 23632, PAD: 0\n",
      "🤖 Loading google/mt5-base...\n",
      "🔄 Configuring mT5 model...\n",
      "   Resized embeddings: 250112 → 23634\n",
      "   ✅ mT5 model configured successfully!\n",
      "⚙️  Preprocessing multilingual dataset for MT5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MT5 preprocessing small_hf_wordpiece_hf: 100%|██████████| 34376/34376 [00:16<00:00, 2136.99 examples/s]\n",
      "MT5 preprocessing small_hf_wordpiece_hf: 100%|██████████| 3820/3820 [00:01<00:00, 2127.55 examples/s]\n",
      "Filter: 100%|██████████| 30566/30566 [00:03<00:00, 8754.06 examples/s]\n",
      "Filter: 100%|██████████| 3386/3386 [00:00<00:00, 8423.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Reduced eval to 500 samples\n",
      "   ✅ Train: 30566, Eval: 500\n",
      "🏋️  Starting MT5 training...\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.2093692750588851}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.4187385501177702}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 417\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏋️  Starting MT5 training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 417\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Final evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:2237\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:2578\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2571\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2572\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2574\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2575\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2576\u001b[0m )\n\u001b[0;32m   2577\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2578\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2581\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2582\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2583\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2584\u001b[0m ):\n\u001b[0;32m   2585\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2586\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\transformers\\trainer.py:3840\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3838\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3842\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\accelerate\\accelerator.py:2574\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2573\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User 4\\.conda\\envs\\cuda_mt_env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Only mt5\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,  # CRITICAL: Use MT5, not T5\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# MT5-ONLY Model Configuration\n",
    "MT5_TOKENIZER_CONFIGS = {\n",
    "    \"hf_wordpiece_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",\n",
    "        \"description\": \"mT5 with WordPiece tokenization\"\n",
    "    },\n",
    "    \"sp_unigram_hf\": {\n",
    "        \"base_model\": \"google/mt5-base\",\n",
    "        \"description\": \"mT5 with SentencePiece Unigram\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Settings\n",
    "tokenizer_sizes = [\"small\", \"medium\", \"large\"]\n",
    "mt5_tokenizer_types = [\"hf_wordpiece_hf\", \"sp_unigram_hf\"]  # Only MT5 compatible\n",
    "\n",
    "# Languages\n",
    "languages = {\n",
    "    \"yo\": \"Yoruba\", \"ar\": \"Arabic\", \"zh\": \"Chinese\", \"ru\": \"Russian\",\n",
    "    \"hi\": \"Hindi\", \"ja\": \"Japanese\", \"swa\": \"Swahili\", \"bn\": \"Bengali\", \"tr\": \"Turkish\"\n",
    "}\n",
    "SRC_LANG = \"en\"\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"balanced_mt_dataset\"\n",
    "print(\"📦 Loading complete multilingual dataset...\")\n",
    "full_dataset = load_from_disk(dataset_path)\n",
    "print(f\"✅ Dataset loaded: {len(full_dataset['train'])} train, {len(full_dataset['test'])} test\")\n",
    "print(f\"🌍 Languages: {list(languages.keys())}\")\n",
    "\n",
    "def setup_mt5_tokenizer(tokenizer_path):\n",
    "    \"\"\"Setup custom tokenizer specifically for mT5\"\"\"\n",
    "    print(f\"🔧 Loading mT5-compatible tokenizer: {tokenizer_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "    \n",
    "    # mT5-specific special tokens\n",
    "    mt5_special_tokens = {\n",
    "        'pad_token': '<pad>',\n",
    "        'eos_token': '</s>',\n",
    "        'unk_token': '<unk>',\n",
    "        'bos_token': '<pad>',  # mT5 uses <pad> as decoder start\n",
    "        'sep_token': '</s>',\n",
    "    }\n",
    "    \n",
    "    # Add missing special tokens\n",
    "    tokens_to_add = {}\n",
    "    for token_name, token_value in mt5_special_tokens.items():\n",
    "        if getattr(tokenizer, token_name, None) is None:\n",
    "            tokens_to_add[token_name] = token_value\n",
    "    \n",
    "    if tokens_to_add:\n",
    "        num_added = tokenizer.add_special_tokens(tokens_to_add)\n",
    "        print(f\"   ✅ Added {num_added} special tokens\")\n",
    "    \n",
    "    print(f\"   📊 Vocab size: {len(tokenizer)}\")\n",
    "    print(f\"   🔑 EOS: {tokenizer.eos_token_id}, PAD: {tokenizer.pad_token_id}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def configure_mt5_model(model, tokenizer):\n",
    "    \"\"\"Configure mT5 model for custom tokenizer\"\"\"\n",
    "    print(\"🔄 Configuring mT5 model...\")\n",
    "    \n",
    "    # Resize embeddings\n",
    "    old_vocab_size = model.config.vocab_size\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"   Resized embeddings: {old_vocab_size} → {len(tokenizer)}\")\n",
    "    \n",
    "    # Configure model parameters\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "    model.config.forced_eos_token_id = tokenizer.eos_token_id\n",
    "    model.config.vocab_size = len(tokenizer)\n",
    "    \n",
    "    # Initialize new embeddings properly\n",
    "    with torch.no_grad():\n",
    "        if old_vocab_size < len(tokenizer):\n",
    "            # Input embeddings\n",
    "            embed_layer = model.get_input_embeddings()\n",
    "            new_embeddings = embed_layer.weight[old_vocab_size:].data\n",
    "            new_embeddings.normal_(mean=0.0, std=0.02)\n",
    "            \n",
    "            # Output embeddings (lm_head)\n",
    "            if hasattr(model, 'lm_head') and hasattr(model.lm_head, 'weight'):\n",
    "                new_out_embeddings = model.lm_head.weight[old_vocab_size:].data\n",
    "                new_out_embeddings.normal_(mean=0.0, std=0.02)\n",
    "    \n",
    "    # Update generation config\n",
    "    if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "        model.generation_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    print(\"   ✅ mT5 model configured successfully!\")\n",
    "    return model\n",
    "\n",
    "def preprocess_mt5_data(examples, tokenizer):\n",
    "    \"\"\"Preprocessing specifically optimized for mT5\"\"\"\n",
    "    sources = []\n",
    "    targets = []\n",
    "    \n",
    "    # Handle batching\n",
    "    if not isinstance(examples[\"translation\"], list):\n",
    "        examples = {\n",
    "            \"translation\": [examples[\"translation\"]], \n",
    "            \"language\": [examples[\"language\"]]\n",
    "        }\n",
    "    \n",
    "    for translation, lang in zip(examples[\"translation\"], examples[\"language\"]):\n",
    "        if isinstance(translation, dict) and lang in languages:\n",
    "            source = translation.get(SRC_LANG, \"\")\n",
    "            target = translation.get(lang, \"\")\n",
    "            \n",
    "            if source.strip() and target.strip():\n",
    "                # mT5 task format\n",
    "                source_formatted = f\"translate English to {languages[lang]}: {source}\"\n",
    "                sources.append(source_formatted)\n",
    "                targets.append(target)\n",
    "    \n",
    "    if not sources:\n",
    "        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    \n",
    "    # Tokenization settings\n",
    "    max_length = 128\n",
    "    \n",
    "    # Tokenize sources\n",
    "    model_inputs = tokenizer(\n",
    "        sources,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\", \n",
    "            return_tensors=None,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "    \n",
    "    # Process labels (convert pad tokens to -100)\n",
    "    processed_labels = []\n",
    "    for label_seq in labels[\"input_ids\"]:\n",
    "        processed_seq = [\n",
    "            -100 if token == tokenizer.pad_token_id else token \n",
    "            for token in label_seq\n",
    "        ]\n",
    "        processed_labels.append(processed_seq)\n",
    "    \n",
    "    model_inputs[\"labels\"] = processed_labels\n",
    "    return model_inputs\n",
    "\n",
    "class MT5DataCollator(DataCollatorForSeq2Seq):\n",
    "    \"\"\"Custom data collator for mT5\"\"\"\n",
    "    def __call__(self, features):\n",
    "        # Clean features\n",
    "        cleaned_features = []\n",
    "        for feature in features:\n",
    "            cleaned_feature = {k: v for k, v in feature.items() if k != \"token_type_ids\"}\n",
    "            cleaned_features.append(cleaned_feature)\n",
    "        \n",
    "        batch = super().__call__(cleaned_features)\n",
    "        batch.pop(\"token_type_ids\", None)\n",
    "        \n",
    "        # Fix labels\n",
    "        if \"labels\" in batch:\n",
    "            labels = batch[\"labels\"]\n",
    "            labels = torch.where(\n",
    "                labels == self.tokenizer.pad_token_id,\n",
    "                torch.tensor(-100, dtype=labels.dtype, device=labels.device),\n",
    "                labels\n",
    "            )\n",
    "            batch[\"labels\"] = labels\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def compute_bleu_metrics(eval_pred):\n",
    "    \"\"\"Compute BLEU scores with progress tracking\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    if len(predictions.shape) == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    total_samples = len(predictions)\n",
    "    print(f\"🔄 Evaluating {total_samples} samples...\")\n",
    "    \n",
    "    for i, (pred, label) in enumerate(zip(predictions, labels)):\n",
    "        if i % 250 == 0 and i > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"   Progress: {i}/{total_samples} ({elapsed:.1f}s)\")\n",
    "        \n",
    "        # Clean tokens\n",
    "        pred_clean = [token for token in pred if token >= 0 and token < len(tokenizer)]\n",
    "        label_clean = [token for token in label if token != -100 and token >= 0 and token < len(tokenizer)]\n",
    "        \n",
    "        try:\n",
    "            decoded_pred = tokenizer.decode(pred_clean, skip_special_tokens=True).strip() if pred_clean else \"\"\n",
    "            decoded_label = tokenizer.decode(label_clean, skip_special_tokens=True).strip() if label_clean else \"\"\n",
    "        except:\n",
    "            decoded_pred = \"\"\n",
    "            decoded_label = \"\"\n",
    "        \n",
    "        decoded_preds.append(decoded_pred)\n",
    "        decoded_labels.append(decoded_label)\n",
    "    \n",
    "    # Compute BLEU\n",
    "    print(\"🧮 Computing BLEU scores...\")\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        if not pred.strip() or not label.strip():\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        if pred.lower().strip() == label.lower().strip():\n",
    "            exact_matches += 1\n",
    "        \n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "        \n",
    "        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            bleu = sentence_bleu([label_tokens], pred_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(bleu)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "    exact_match = exact_matches / len(decoded_preds) if decoded_preds else 0.0\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"✅ Evaluation complete: BLEU={avg_bleu:.4f}, EM={exact_match:.4f} ({total_time:.1f}s)\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"avg_pred_length\": np.mean([len(p.split()) for p in decoded_preds if p.strip()]) if decoded_preds else 0.0,\n",
    "        \"avg_label_length\": np.mean([len(l.split()) for l in decoded_labels if l.strip()]) if decoded_labels else 0.0,\n",
    "        \"empty_predictions\": sum(1 for p in decoded_preds if not p.strip()) / len(decoded_preds) if decoded_preds else 0.0\n",
    "    }\n",
    "\n",
    "# Setup logging\n",
    "log_dir = \"./MT5_models_only\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "results_file = os.path.join(log_dir, \"mt5_results.csv\")\n",
    "\n",
    "if not os.path.exists(results_file):\n",
    "    with open(results_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Model_ID\", \"Tokenizer_Size\", \"Tokenizer_Type\", \"Custom_Vocab_Size\", \n",
    "                        \"Train_Samples\", \"Eval_Samples\", \"BLEU_Score\", \"Exact_Match\", \n",
    "                        \"Training_Status\", \"Notes\"])\n",
    "\n",
    "print(\"🚀 Starting MT5-ONLY multilingual training...\")\n",
    "print(f\"📊 Will train {len(tokenizer_sizes)} sizes × {len(mt5_tokenizer_types)} types = {len(tokenizer_sizes) * len(mt5_tokenizer_types)} MT5 models\")\n",
    "\n",
    "# Main MT5 training loop\n",
    "model_count = 0\n",
    "for size in tokenizer_sizes:\n",
    "    for tok_type in mt5_tokenizer_types:\n",
    "        model_count += 1\n",
    "        tokenizer_path = f\"vocab_final/vocab_final{size}/{tok_type}\"\n",
    "        model_id = f\"mt5_{size}_{tok_type}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🚀 Training MT5 Model {model_count}/{len(tokenizer_sizes) * len(mt5_tokenizer_types)}\")\n",
    "        print(f\"🔧 Custom Tokenizer: {size}_{tok_type}\")\n",
    "        print(f\"🌍 Target Languages: ALL {len(languages)} languages\")\n",
    "        \n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        trainer = None\n",
    "        \n",
    "        try:\n",
    "            # Setup MT5 tokenizer\n",
    "            tokenizer = setup_mt5_tokenizer(tokenizer_path)\n",
    "            \n",
    "            # Load MT5 model\n",
    "            print(\"🤖 Loading google/mt5-base...\")\n",
    "            model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-base\")\n",
    "            \n",
    "            # Configure model\n",
    "            model = configure_mt5_model(model, tokenizer)\n",
    "            \n",
    "            # Preprocess dataset\n",
    "            print(\"⚙️  Preprocessing multilingual dataset for MT5...\")\n",
    "            processed_dataset = full_dataset.map(\n",
    "                lambda x: preprocess_mt5_data(x, tokenizer),\n",
    "                batched=True,\n",
    "                remove_columns=full_dataset[\"train\"].column_names,\n",
    "                desc=f\"MT5 preprocessing {size}_{tok_type}\",\n",
    "                batch_size=32,\n",
    "                num_proc=1\n",
    "            )\n",
    "            \n",
    "            # Filter valid examples\n",
    "            def is_valid(example):\n",
    "                return (\n",
    "                    len(example[\"input_ids\"]) > 0 and \n",
    "                    len(example[\"labels\"]) > 0 and\n",
    "                    any(label != -100 for label in example[\"labels\"])\n",
    "                )\n",
    "            \n",
    "            train_dataset = processed_dataset[\"train\"].filter(is_valid)\n",
    "            eval_dataset = processed_dataset[\"test\"].filter(is_valid)\n",
    "            \n",
    "            # Limit eval dataset size\n",
    "            if len(eval_dataset) > 500:\n",
    "                eval_dataset = eval_dataset.select(range(500))\n",
    "                print(f\"   Reduced eval to {len(eval_dataset)} samples\")\n",
    "            \n",
    "            print(f\"   ✅ Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n",
    "            \n",
    "            if len(train_dataset) == 0:\n",
    "                print(\"❌ No valid training samples!\")\n",
    "                continue\n",
    "            \n",
    "            # Setup training\n",
    "            output_dir = f\"./MT5_models_only/{model_id}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # MT5-optimized training arguments\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                num_train_epochs=1,\n",
    "                per_device_train_batch_size=4,\n",
    "                per_device_eval_batch_size=4,\n",
    "                gradient_accumulation_steps=16,\n",
    "                learning_rate=3e-5,  # Lower for MT5\n",
    "                weight_decay=0.01,\n",
    "                warmup_steps=100,\n",
    "                eval_strategy=\"steps\",\n",
    "                eval_steps=1000,\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=1,\n",
    "                logging_steps=100,\n",
    "                report_to=\"none\",\n",
    "                predict_with_generate=True,\n",
    "                generation_max_length=64,\n",
    "                generation_num_beams=2,\n",
    "                fp16=False,  # Disabled for stability\n",
    "                load_best_model_at_end=False,\n",
    "                dataloader_num_workers=0,\n",
    "                remove_unused_columns=False,\n",
    "                ignore_data_skip=True,\n",
    "                max_grad_norm=0.5,\n",
    "                dataloader_pin_memory=False,\n",
    "                skip_memory_metrics=True,\n",
    "                optim=\"adamw_torch\",\n",
    "                lr_scheduler_type=\"cosine\"\n",
    "            )\n",
    "            \n",
    "            # Data collator\n",
    "            data_collator = MT5DataCollator(\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "                label_pad_token_id=-100\n",
    "            )\n",
    "            \n",
    "            # Trainer\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_bleu_metrics\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            print(\"🏋️  Starting MT5 training...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            print(\"📊 Final evaluation...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Save\n",
    "            print(\"💾 Saving MT5 model and tokenizer...\")\n",
    "            trainer.save_model()\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            \n",
    "            # Log results\n",
    "            bleu_score = eval_results.get(\"eval_bleu\", 0.0)\n",
    "            exact_match = eval_results.get(\"eval_exact_match\", 0.0)\n",
    "            \n",
    "            with open(results_file, 'a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    model_id, size, tok_type, len(tokenizer),\n",
    "                    len(train_dataset), len(eval_dataset), \n",
    "                    round(bleu_score, 4), round(exact_match, 4),\n",
    "                    \"SUCCESS\", f\"MT5 with {size}_{tok_type}\"\n",
    "                ])\n",
    "            \n",
    "            print(f\"✅ Completed: {model_id}\")\n",
    "            print(f\"📈 BLEU: {bleu_score:.4f}, Exact Match: {exact_match:.4f}\")\n",
    "            \n",
    "            # Quick test\n",
    "            print(\"\\n🧪 Quick translation test:\")\n",
    "            test_input = \"translate English to Hindi: Hello, how are you?\"\n",
    "            inputs = tokenizer(test_input, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "            \n",
    "            device = next(model.parameters()).device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=64,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    forced_eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"   Translation: {translation}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed: {model_id} - {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            with open(results_file, 'a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    model_id, size, tok_type, 0, 0, 0, 0, 0,\n",
    "                    \"FAILED\", str(e)[:100]\n",
    "                ])\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if model is not None:\n",
    "                del model\n",
    "            if trainer is not None:\n",
    "                del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "print(f\"\\n🎉 MT5-only training completed!\")\n",
    "print(f\"📋 Results saved to: {results_file}\")\n",
    "print(f\"🔢 Total MT5 models: {len(tokenizer_sizes) * len(mt5_tokenizer_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2bdfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_mt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
